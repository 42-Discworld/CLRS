{
    "docs": [
        {
            "location": "/Chap22/22.1/",
            "text": "22.1-1\n\n\n\n\nGiven an adjacency-list representation of a directed graph, how long does it take to compute the $out\\text-degree$ of every vertex? How long does it take to compute the $in\\text-degrees$?\n\n\n\n\nSince it seems as though the list for the neighbors of each vertex $v$ is just an undecorated list, to find the length of each would take time $O(out\\text-degree(v))$. So, the total cost will be\n\n\n$$\\sum_{v \\in V}O(out\\text-degree(v)) = O(|E| + |V|).$$\n\n\nNote that the $|V|$ showing up in the asymptotics is necessary, because it still takes a constant amount of time to know that a list is empty. This time could be reduced to $O(|V|)$ if for each list in the adjacency list representation, we just also stored its length.\n\n\nTo compute the in degree of each vertex, we will have to scan through all of the adjacency lists and keep counters for how many times each vertex has appeared. As in the previous case, the time to scan through all of the adjacency lists takes time $O(|E| + |V|)$.\n\n\n22.1-2\n\n\n\n\nGive an adjacency-list representation for a complete binary tree on 7 vertices. Give an equivalent adjacency-matrix representation. Assume that vertices are numbered from 1 to 7 as in a binary heap.\n\n\n\n\n\n\n\n\nAdjacency-list representation\n\n\n\\begin{align}\n1 &: 2 \\rightarrow 3 \\\\\n2 &: 1 \\rightarrow 4 \\rightarrow 5 \\\\\n3 &: 1 \\rightarrow 6 \\rightarrow 7 \\\\\n4 &: 2 \\\\\n5 &: 2 \\\\\n6 &: 3 \\\\\n7 &: 3.\n\\end{align}\n\n\n\n\n\n\nAdjacency-matrix representation\n\n\n$$\n\\begin{pmatrix}\n0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\end{pmatrix}.\n$$\n\n\n\n\n\n\n22.1-3\n\n\n\n\nThe \ntranspose\n of a directed graph $G = (V, E)$ is the graph $G^\\text T = (V, E^\\text T)$, where $E^\\text T = {(v, u) \\in V \\times V: (u, v) \\in E}$. Thus, $G^\\text T$ is $G$ with all its edges reversed. Describe efficient algorithms for computing $G^\\text T$ from $G$, for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms.\n\n\n\n\n\n\nAdjacency-list representation:\n  For the adjacency list representation, we will maintain an initially empty adjacency list representation of the transpose. Then, we scan through every list in the original graph. If we are in the list corresponding to vertex $v$ and see $u$ as an entry in the list, then we add an entry of $v$ to the list in the transpose graph corresponding to vertex $u$. Since this only requires a scan through all of the lists, it only takes time $O(|E| + |V|)$.\n\n\nAdjacency-matrix representation:\n to compute the graph transpose, we just take the matrix transpose. This means looking along every entry above the diagonal, and swapping it with the entry that occurs below the diagonal. This takes time $O(|V|^2)$.\n\n\n\n\n22.1-4\n\n\n\n\nGiven an adjacency-list representation of a multigraph $G = (V, E)$, describe an $O(V + E)$-time algorithm to compute the adjacency-list representation of the ''equivalent'' undirected graph $G' = (V, E')$, where $E'$ consists of the edges in $E$ with all multiple edges between two vertices replaced by a single edge and with all self-loops removed.\n\n\n\n\nCreate an array $A$ of size $|V|$. For a list in the adjacency list corresponding to vertex $v$, examine items on the list one by one. If any item is equal to $v$, remove it. If vertex $u$ appears on the list, examine $A[u]$. If it's not equal to $v$, set it equal to $v$. If it's equal to $v$, remove $u$ from the list. Since we have constant time lookup in the array, the total runtime is $O(V + E)$.\n\n\n22.1-5\n\n\n\n\nThe \nsquare\n of a directed graph $G = (V, E)$ is the graph $G^2 = (V, E^2)$ such that $(u, v) \\in E$ if and only $G$ contains a path with at most two edges between $u$ and $v$. Describe efficient algorithms for computing $G^2$ from $G$ for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms.\n\n\n\n\n\n\nAdjacency-list representation:\n To compute $G^2$ from the adjacency-list representation $Adj$ of $G$, we perform the following for each $Adj[u]$:\n\n\n\n\n    \nfor\n \neach\n \nvertex\n \nv\n \nin\n \nAdj\n[\nu\n]\n\n        \nfor\n \neach\n \nvertex\n \nw\n \nin\n \nAdj\n[\nv\n]\n\n            \nedge\n(\nu\n,\n \nw\n)\n \n\u2208\n \nE\n^\n2\n\n            \ninsert\n \nw\n \nin\n \nAdj2\n(\nu\n)\n\n\n\n\n\nwhere $Adj2$ is the adjacency-list representation of $G^2$. After we have computed $Adj2$, we have to remove any duplicate edges from the lists (there may be more than one two-edge path in $G$ between any two vertices). For every edge in $Adj$ we scan at most $|V|$ vertices, we compute $Adj2$ in time $O(VE)$. Removing duplicate edges is done in $O(V + E)$ as shown in exercise 22.1-4. Thus the total running time is\n\n\n$$O(VE) + O(V + E) = O(VE).$$\n\n\n\n\nAdjacency-matrix representation:\n Let $A$ denote the adjacency-matrix representation of $G$. The adjacency-matrix representation of $G^2$ is the square of $A$. Computing $A^2$ can be done in time $O(V^3)$ (and even faster, theoretically; Strassen's algorithm for example will compute $A^2$ in $O(V^{\\lg 7})$).\n\n\n\n\n22.1-6\n\n\n\n\nMost graph algorithms that take an adjacency-matrix representation as input require time $\\Omega(V^2)$, but there are some exceptions. Show how to determine whether a directed graph $G$ contains a \nuniversal sink\n $-$ a vertex with $in\\text-degree$ $|V| - 1$ and $out\\text-degree$ $0$ $-$ in time $O(V)$, given an adjacency matrix for $G$.\n\n\n\n\nWe start by observing that if $a_{ij} = 1$, so that $(i, j) \\in E$, then vertex $i$ cannot be a universal sink, for it has an outgoing edge. Thus, if row $i$ contains a $1$, then vertex $i$ cannot be a universal sink. This observation also means that if there is a $\\text{self-loop}(i, i)$, then vertex $i$ is not a universal sink. Now suppose that $a_{ij} = 0$, so that $(i, j) \\notin E$, and also that $i \\ne j$. Then vertex $j$ cannot be a universal sink, for either its $in\\text-degree$ must be strictly less than $|V| - 1$ or it has a self-loop. Thus if column $j$ contains a $0$ in any position other than the diagonal entry $(j, j)$, then vertex $j$ cannot be a universal sink.\n\n\nUsing the above observations, the following procedure returns $\\text{TRUE}$ if vertex $k$ is a universal sink, and $\\text{FALSE}$ otherwise. It takes as input a $|V| \\times |V|$ adjacency matrix $A = (a_{ij})$.\n\n\nIS\n-\nSINK\n(\nA\n,\n \nk\n)\n\n    \nlet\n \nA\n \nbe\n \n|\nV\n|\n \n\u00d7\n \n|\nV\n|\n\n    \nfor\n \nj\n \n=\n \n1\n \nto\n \n|\nV\n|\n      \n// Check for a 1 in row k\n\n        \nif\n \na\n[\nk\n][\nj\n]\n \n==\n \n1\n\n            \nreturn\n \nFALSE\n\n    \nfor\n \ni\n \n=\n \n1\n \nto\n \n|\nV\n|\n      \n// Check for an off-diagonal 0 in column k\n\n        \nif\n \na\n[\ni\n][\nk\n]\n \n==\n \n0\n \nand\n \ni\n \n!=\n \nk\n\n            \nreturn\n \nFALSE\n\n    \nreturn\n \nTRUE\n\n\n\n\n\nBecause this procedure runs in $O(V)$ time, we may call it only $O(1)$ times in order to achieve our $O(V)$-time bound for determining whether directed graph $G$ contains a universal sink.\n\n\nObserve also that a directed graph can have at most one universal sink. This property holds because if vertex $j$ is a universal sink, then we would have $(i, j) \\in E$ for all $i \\ne j$ and so no other vertex $i$ could be a universal sink.\n\n\nThe following procedure takes an adjacency matrix $A$ as input and returns either a message that there is no universal sink or a message containing the identity of the universal sink. It works by eliminating all but one vertex as a potential universal sink and then checking the remaining candidate vertex by a single call to $\\text{IS-SINK}$.\n\n\nUNIVERSAL\n-\nSINK\n(\nA\n)\n\n    \nlet\n \nA\n \nbe\n \n|\nV\n|\n \n\u00d7\n \n|\nV\n|\n\n    \ni\n \n=\n \nj\n \n=\n \n1\n\n    \nwhile\n \ni\n \n\u2264\n \n|\nV\n|\n \nand\n \nj\n \n\u2264\n \n|\nV\n|\n\n        \nif\n \na\n[\ni\n][\nj\n]\n \n==\n \n1\n\n            \ni\n \n=\n \ni\n \n+\n \n1\n\n        \nelse\n \nj\n \n=\n \nj\n \n+\n \n1\n\n    \ns\n \n=\n \n0\n\n    \nif\n \ni\n \n>\n \n|\nV\n|\n\n        \nreturn\n \n\"there is no universal sink\"\n\n    \nelse\n \nif\n \nIS\n-\nSINK\n(\nA\n,\n \ni\n)\n \n=\n \nFALSE\n\n        \nreturn\n \n\"there is no universal sink\"\n\n    \nelse\n \nreturn\n \ni\n \n\"is a universal sink\"\n\n\n\n\n\n$\\text{UNIVERSAL-SINK}$ walks through the adjacency matrix, starting at the upper left corner and always moving either right or down by one position, depending on whether the current entry $a_{ij}$ it is examining is $0$ or $1$. It stops once either $i$ or $j$ exceeds $|V|$.\n\n\nTo understand why $\\text{UNIVERSAL-SINK}$ works, we need to show that after the \nwhile\n loop terminates, the only vertex that might be a universal sink is vertex $i$. The call to $\\text{IS-SINK}$ then determines whether vertex $i$ is indeed a universal sink.\n\n\nLet us fix $i$ and $j$ to be values of these variables at the termination of the \nwhile\n loop. We claim that every vertex $k$ such that $1 \\le k < i$ cannot be a universal sink. That is because the way that $i$ achieved its final value at loop termination was by finding a $1$ in each row $k$ for which $1 \\le k < i$. As we observed above, any vertex $k$ whose row contains a $1$ cannot be a universal sink.\n\n\nIf $i > |V|$ at loop termination, then we have eliminated all vertices from consid- eration, and so there is no universal sink. If, on the other hand, $i \\le |V|$ at loop termination, we need to show that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink. If $i \\le |V|$ at loop termination, then the \nwhile\n loop terminated because $j > |V|$. That means that we found a $0$ in every column. Recall our earlier observation that if column $k$ contains a $0$ in an off-diagonal position, then vertex $k$ cannot be a universal sink. Since we found a $0$ in every column, we found a $0$ in every column $k$ such that $i < k \\le |V|$. Moreover, we never examined any matrix entries in rows greater than $i$, and so we never examined the diagonal entry in any column $k$ such that $i < k \\le |V|$. Therefore, all the $0$s that we found in columns $k$ such that $i < k \\le |V|$ were off-diagonal. We conclude that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink.\n\n\nThus, we have shown that every vertex less than $i$ and every vertex greater than $i$ cannot be a universal sink. The only remaining possibility is that vertex $i$ might be a universal sink, and the call to $\\text{IS-SINK}$ checks whether it is.\n\n\nTo see that $\\text{UNIVERSAL-SINK}$ runs in $O(V)$ time, observe that either $i$ or $j$ is incremented in each iteration of the \nwhile\n loop. Thus, the \nwhile\n loop makes at most $2|V| - 1$ iterations. Each iteration takes $O(1)$ time, for a total \nwhile\n loop time of $O(V)$ and, combined with the $O(V)$-time call to $\\text{IS-SINK}$, we get a total running time of $O(V)$.\n\n\n22.1-7\n\n\n\n\nThe \nincidence matrix\n of a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $B = (b_{ij})$ such that\n$$\nb_{ij} =\n\\begin{cases}\n-1 & \\text{if edge $j$ leaves vertex $i$}, \\\\\n 1 & \\text{if edge $j$ enters vertex $i$}, \\\\\n 0 & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nDescribe what the entries of the matrix product $BB^\\text T$ represent, where $B^\\text T$ is the transpose of $B$.\n\n\n\n\n$$BB^\\text T(i, j) = \\sum\\limits_{e\\in E}b_{ie} b_{ej}^\\text T = \\sum\\limits_{e\\in E}b_{ie}b_{je}.$$\n\n\n\n\nIf $i = j$, then $b_{ie} b_{je} = 1$ (it is $1 \\cdot 1$ or $(-1) \\cdot (-1)$) whenever $e$ enters or leaves vertex $i$, and $0$ otherwise.\n\n\nIf $i \\ne j$, then $b_{ie} b_{je} = -1$ when $e = (i, j)$ or $e = (j, i)$, and $0$ otherwise.\n\n\n\n\nThus,\n\n\n$$\nBB^\\text T(i, j) =\n\\begin{cases}\n\\text{degree of $i$ = in-degree + out-degree}   & \\text{if $i = j$}, \\\\\n\\text{$-$(# of edges connecting $i$ and $j$)}  & \\text{if $i \\ne j$}.\n\\end{cases}\n$$\n\n\n22.1-8\n\n\n\n\nSuppose that instead of a linked list, each array entry $Adj[u]$ is a hash table containing the vertices $v$ for which $(u, v) \\in E$. If all edge lookups are equally likely, what is the expected time to determine whether an edge is in the graph? What disadvantages does this scheme have? Suggest an alternate data structure for each edge list that solves these problems. Does your alternative have disadvantages compared to the hash table?\n\n\n\n\nThe expected loopup time is $O(1)$, but in the worst case it could take $O(|V|)$. If we first sorted vertices in each adjacency list then we could perform a binary search so that the worst case lookup time is $O(\\lg |V|)$, but this has the disadvantage of having a much worse expected lookup time.",
            "title": "22.1 Representations of graphs"
        },
        {
            "location": "/Chap22/22.1/#221-1",
            "text": "Given an adjacency-list representation of a directed graph, how long does it take to compute the $out\\text-degree$ of every vertex? How long does it take to compute the $in\\text-degrees$?   Since it seems as though the list for the neighbors of each vertex $v$ is just an undecorated list, to find the length of each would take time $O(out\\text-degree(v))$. So, the total cost will be  $$\\sum_{v \\in V}O(out\\text-degree(v)) = O(|E| + |V|).$$  Note that the $|V|$ showing up in the asymptotics is necessary, because it still takes a constant amount of time to know that a list is empty. This time could be reduced to $O(|V|)$ if for each list in the adjacency list representation, we just also stored its length.  To compute the in degree of each vertex, we will have to scan through all of the adjacency lists and keep counters for how many times each vertex has appeared. As in the previous case, the time to scan through all of the adjacency lists takes time $O(|E| + |V|)$.",
            "title": "22.1-1"
        },
        {
            "location": "/Chap22/22.1/#221-2",
            "text": "Give an adjacency-list representation for a complete binary tree on 7 vertices. Give an equivalent adjacency-matrix representation. Assume that vertices are numbered from 1 to 7 as in a binary heap.     Adjacency-list representation  \\begin{align}\n1 &: 2 \\rightarrow 3 \\\\\n2 &: 1 \\rightarrow 4 \\rightarrow 5 \\\\\n3 &: 1 \\rightarrow 6 \\rightarrow 7 \\\\\n4 &: 2 \\\\\n5 &: 2 \\\\\n6 &: 3 \\\\\n7 &: 3.\n\\end{align}    Adjacency-matrix representation  $$\n\\begin{pmatrix}\n0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n\\end{pmatrix}.\n$$",
            "title": "22.1-2"
        },
        {
            "location": "/Chap22/22.1/#221-3",
            "text": "The  transpose  of a directed graph $G = (V, E)$ is the graph $G^\\text T = (V, E^\\text T)$, where $E^\\text T = {(v, u) \\in V \\times V: (u, v) \\in E}$. Thus, $G^\\text T$ is $G$ with all its edges reversed. Describe efficient algorithms for computing $G^\\text T$ from $G$, for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms.    Adjacency-list representation:   For the adjacency list representation, we will maintain an initially empty adjacency list representation of the transpose. Then, we scan through every list in the original graph. If we are in the list corresponding to vertex $v$ and see $u$ as an entry in the list, then we add an entry of $v$ to the list in the transpose graph corresponding to vertex $u$. Since this only requires a scan through all of the lists, it only takes time $O(|E| + |V|)$.  Adjacency-matrix representation:  to compute the graph transpose, we just take the matrix transpose. This means looking along every entry above the diagonal, and swapping it with the entry that occurs below the diagonal. This takes time $O(|V|^2)$.",
            "title": "22.1-3"
        },
        {
            "location": "/Chap22/22.1/#221-4",
            "text": "Given an adjacency-list representation of a multigraph $G = (V, E)$, describe an $O(V + E)$-time algorithm to compute the adjacency-list representation of the ''equivalent'' undirected graph $G' = (V, E')$, where $E'$ consists of the edges in $E$ with all multiple edges between two vertices replaced by a single edge and with all self-loops removed.   Create an array $A$ of size $|V|$. For a list in the adjacency list corresponding to vertex $v$, examine items on the list one by one. If any item is equal to $v$, remove it. If vertex $u$ appears on the list, examine $A[u]$. If it's not equal to $v$, set it equal to $v$. If it's equal to $v$, remove $u$ from the list. Since we have constant time lookup in the array, the total runtime is $O(V + E)$.",
            "title": "22.1-4"
        },
        {
            "location": "/Chap22/22.1/#221-5",
            "text": "The  square  of a directed graph $G = (V, E)$ is the graph $G^2 = (V, E^2)$ such that $(u, v) \\in E$ if and only $G$ contains a path with at most two edges between $u$ and $v$. Describe efficient algorithms for computing $G^2$ from $G$ for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms.    Adjacency-list representation:  To compute $G^2$ from the adjacency-list representation $Adj$ of $G$, we perform the following for each $Adj[u]$:        for   each   vertex   v   in   Adj [ u ] \n         for   each   vertex   w   in   Adj [ v ] \n             edge ( u ,   w )   \u2208   E ^ 2 \n             insert   w   in   Adj2 ( u )   where $Adj2$ is the adjacency-list representation of $G^2$. After we have computed $Adj2$, we have to remove any duplicate edges from the lists (there may be more than one two-edge path in $G$ between any two vertices). For every edge in $Adj$ we scan at most $|V|$ vertices, we compute $Adj2$ in time $O(VE)$. Removing duplicate edges is done in $O(V + E)$ as shown in exercise 22.1-4. Thus the total running time is  $$O(VE) + O(V + E) = O(VE).$$   Adjacency-matrix representation:  Let $A$ denote the adjacency-matrix representation of $G$. The adjacency-matrix representation of $G^2$ is the square of $A$. Computing $A^2$ can be done in time $O(V^3)$ (and even faster, theoretically; Strassen's algorithm for example will compute $A^2$ in $O(V^{\\lg 7})$).",
            "title": "22.1-5"
        },
        {
            "location": "/Chap22/22.1/#221-6",
            "text": "Most graph algorithms that take an adjacency-matrix representation as input require time $\\Omega(V^2)$, but there are some exceptions. Show how to determine whether a directed graph $G$ contains a  universal sink  $-$ a vertex with $in\\text-degree$ $|V| - 1$ and $out\\text-degree$ $0$ $-$ in time $O(V)$, given an adjacency matrix for $G$.   We start by observing that if $a_{ij} = 1$, so that $(i, j) \\in E$, then vertex $i$ cannot be a universal sink, for it has an outgoing edge. Thus, if row $i$ contains a $1$, then vertex $i$ cannot be a universal sink. This observation also means that if there is a $\\text{self-loop}(i, i)$, then vertex $i$ is not a universal sink. Now suppose that $a_{ij} = 0$, so that $(i, j) \\notin E$, and also that $i \\ne j$. Then vertex $j$ cannot be a universal sink, for either its $in\\text-degree$ must be strictly less than $|V| - 1$ or it has a self-loop. Thus if column $j$ contains a $0$ in any position other than the diagonal entry $(j, j)$, then vertex $j$ cannot be a universal sink.  Using the above observations, the following procedure returns $\\text{TRUE}$ if vertex $k$ is a universal sink, and $\\text{FALSE}$ otherwise. It takes as input a $|V| \\times |V|$ adjacency matrix $A = (a_{ij})$.  IS - SINK ( A ,   k ) \n     let   A   be   | V |   \u00d7   | V | \n     for   j   =   1   to   | V |        // Check for a 1 in row k \n         if   a [ k ][ j ]   ==   1 \n             return   FALSE \n     for   i   =   1   to   | V |        // Check for an off-diagonal 0 in column k \n         if   a [ i ][ k ]   ==   0   and   i   !=   k \n             return   FALSE \n     return   TRUE   Because this procedure runs in $O(V)$ time, we may call it only $O(1)$ times in order to achieve our $O(V)$-time bound for determining whether directed graph $G$ contains a universal sink.  Observe also that a directed graph can have at most one universal sink. This property holds because if vertex $j$ is a universal sink, then we would have $(i, j) \\in E$ for all $i \\ne j$ and so no other vertex $i$ could be a universal sink.  The following procedure takes an adjacency matrix $A$ as input and returns either a message that there is no universal sink or a message containing the identity of the universal sink. It works by eliminating all but one vertex as a potential universal sink and then checking the remaining candidate vertex by a single call to $\\text{IS-SINK}$.  UNIVERSAL - SINK ( A ) \n     let   A   be   | V |   \u00d7   | V | \n     i   =   j   =   1 \n     while   i   \u2264   | V |   and   j   \u2264   | V | \n         if   a [ i ][ j ]   ==   1 \n             i   =   i   +   1 \n         else   j   =   j   +   1 \n     s   =   0 \n     if   i   >   | V | \n         return   \"there is no universal sink\" \n     else   if   IS - SINK ( A ,   i )   =   FALSE \n         return   \"there is no universal sink\" \n     else   return   i   \"is a universal sink\"   $\\text{UNIVERSAL-SINK}$ walks through the adjacency matrix, starting at the upper left corner and always moving either right or down by one position, depending on whether the current entry $a_{ij}$ it is examining is $0$ or $1$. It stops once either $i$ or $j$ exceeds $|V|$.  To understand why $\\text{UNIVERSAL-SINK}$ works, we need to show that after the  while  loop terminates, the only vertex that might be a universal sink is vertex $i$. The call to $\\text{IS-SINK}$ then determines whether vertex $i$ is indeed a universal sink.  Let us fix $i$ and $j$ to be values of these variables at the termination of the  while  loop. We claim that every vertex $k$ such that $1 \\le k < i$ cannot be a universal sink. That is because the way that $i$ achieved its final value at loop termination was by finding a $1$ in each row $k$ for which $1 \\le k < i$. As we observed above, any vertex $k$ whose row contains a $1$ cannot be a universal sink.  If $i > |V|$ at loop termination, then we have eliminated all vertices from consid- eration, and so there is no universal sink. If, on the other hand, $i \\le |V|$ at loop termination, we need to show that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink. If $i \\le |V|$ at loop termination, then the  while  loop terminated because $j > |V|$. That means that we found a $0$ in every column. Recall our earlier observation that if column $k$ contains a $0$ in an off-diagonal position, then vertex $k$ cannot be a universal sink. Since we found a $0$ in every column, we found a $0$ in every column $k$ such that $i < k \\le |V|$. Moreover, we never examined any matrix entries in rows greater than $i$, and so we never examined the diagonal entry in any column $k$ such that $i < k \\le |V|$. Therefore, all the $0$s that we found in columns $k$ such that $i < k \\le |V|$ were off-diagonal. We conclude that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink.  Thus, we have shown that every vertex less than $i$ and every vertex greater than $i$ cannot be a universal sink. The only remaining possibility is that vertex $i$ might be a universal sink, and the call to $\\text{IS-SINK}$ checks whether it is.  To see that $\\text{UNIVERSAL-SINK}$ runs in $O(V)$ time, observe that either $i$ or $j$ is incremented in each iteration of the  while  loop. Thus, the  while  loop makes at most $2|V| - 1$ iterations. Each iteration takes $O(1)$ time, for a total  while  loop time of $O(V)$ and, combined with the $O(V)$-time call to $\\text{IS-SINK}$, we get a total running time of $O(V)$.",
            "title": "22.1-6"
        },
        {
            "location": "/Chap22/22.1/#221-7",
            "text": "The  incidence matrix  of a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $B = (b_{ij})$ such that\n$$\nb_{ij} =\n\\begin{cases}\n-1 & \\text{if edge $j$ leaves vertex $i$}, \\\\\n 1 & \\text{if edge $j$ enters vertex $i$}, \\\\\n 0 & \\text{otherwise}.\n\\end{cases}\n$$  Describe what the entries of the matrix product $BB^\\text T$ represent, where $B^\\text T$ is the transpose of $B$.   $$BB^\\text T(i, j) = \\sum\\limits_{e\\in E}b_{ie} b_{ej}^\\text T = \\sum\\limits_{e\\in E}b_{ie}b_{je}.$$   If $i = j$, then $b_{ie} b_{je} = 1$ (it is $1 \\cdot 1$ or $(-1) \\cdot (-1)$) whenever $e$ enters or leaves vertex $i$, and $0$ otherwise.  If $i \\ne j$, then $b_{ie} b_{je} = -1$ when $e = (i, j)$ or $e = (j, i)$, and $0$ otherwise.   Thus,  $$\nBB^\\text T(i, j) =\n\\begin{cases}\n\\text{degree of $i$ = in-degree + out-degree}   & \\text{if $i = j$}, \\\\\n\\text{$-$(# of edges connecting $i$ and $j$)}  & \\text{if $i \\ne j$}.\n\\end{cases}\n$$",
            "title": "22.1-7"
        },
        {
            "location": "/Chap22/22.1/#221-8",
            "text": "Suppose that instead of a linked list, each array entry $Adj[u]$ is a hash table containing the vertices $v$ for which $(u, v) \\in E$. If all edge lookups are equally likely, what is the expected time to determine whether an edge is in the graph? What disadvantages does this scheme have? Suggest an alternate data structure for each edge list that solves these problems. Does your alternative have disadvantages compared to the hash table?   The expected loopup time is $O(1)$, but in the worst case it could take $O(|V|)$. If we first sorted vertices in each adjacency list then we could perform a binary search so that the worst case lookup time is $O(\\lg |V|)$, but this has the disadvantage of having a much worse expected lookup time.",
            "title": "22.1-8"
        },
        {
            "location": "/Chap22/22.2/",
            "text": "22.2-1\n\n\n\n\nShow the $d$ and $\\pi$ values that result from running breadth-first search on the directed graph of Figure 22.2(a), using vertex $3$ as the source.\n\n\n\n\n\\begin{array}{c|cccccc}\n\\text{vertex} & 1 & 2 & 3 & 4 & 5 & 6 \\\\\n\\hline\nd & \\infty & 3 & 0 & 2 & 1 & 1 \\\\\n\\pi & \\text{NIL} & 4 & \\text{NIL} & 5 & 3 & 3\n\\end{array}\n\n\n22.2-2\n\n\n\n\nShow the $d$ and $\\pi$ values that result from running breadth-first search on the undirected graph of Figure 22.3, using vertex $u$ as the source.\n\n\n\n\n\\begin{array}{c|cccccc}\n\\text{vertex} & r & s & t & u & v & w & x & y \\\\\n\\hline\nd & 4 & 3 & 1 & 0 & 5 & 2 & 1 & 1 \\\\\n\\pi & s & w & u & \\text{NIL} & r & t & u & u\n\\end{array}\n\n\n22.2-3\n\n\n\n\nShow that using a single bit to store each vertex color suffices by arguing that the $\\text{BFS}$ procedure would produce the same result if lines 5 and 14 were removed.\n\n\n\n\n$\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change.\n\n\nThe $\\text{BFS}$ procedure cares only whether a vertex is white or not. $A$ vertex $v$ must become non-white at the same time that $v.d$ is assigned a finite value so that we do not attempt to assign to $v.d$ again, and so we need to change vertex colors in lines 5 and 14. Once we have changed a vertex's color to non-white, we do not need to change it again.\n\n\n22.2-4\n\n\n\n\nWhat is the running time of $\\text{BFS}$ if we represent its input graph by an adjacency matrix and modify the algorithm to handle this form of input?\n\n\n\n\nThe correctness proof for the $\\text{BFS}$ algorithm shows that $d[u] = \\delta(s, u)$, and the algorithm doesn't assume that the adjacency lists are in any particular order.\n\n\nIn Figure 22.3, if t precedes $x$ in $Adj[w]$, we can get the breadth-first tree shown in the figure. But if $x$ precedes $t$ in $Adj[w]$ and $u$ precedes $y$ in $Adj[x]$, we can get edge $(x, u)$ in the breadth-first tree.\n\n\n22.2-5\n\n\n\n\nArgue that in a breadth-first search, the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list. Using Figure 22.3 as an example, show that the breadth-first tree computed by $\\text{BFS}$ can depend on the ordering within adjacency lists.\n\n\n\n\nThe correctness proof for the $\\text{BFS}$ algorithm shows that $\\delta.d = \\delta(s, u)$, and the algorithm doesn't assume that the adjacency lists are in any particular order.\n\n\nIn Figure 22.3, if $t$ precedes $x$ in $Adj[w]$\u008d, we can get the breadth-first tree shown in the figure. But if $x$ precedes $t$ in $Adj[w]$\u008d and $u$ precedes $y$ in $Adj[x]$\u008d, we can get edge $(x, u)$ in the breadth-first tree.\n\n\n22.2-6\n\n\n\n\nGive an example of a directed graph $G = (V, E)$, a source vertex $s \\in V$, and a set of tree edges $E_\\pi \\subseteq E$ such that for each vertex $v \\in V$, the unique simple path in the graph $(V, E_\\pi)$ from $s$ to $v$ is a shortest path in $G$, yet the set of edges $E_\\pi$ cannot be produced by running $\\text{BFS}$ on $G$, no matter how the vertices are ordered in each adjacency list.\n\n\n\n\nThe edges in $E_\\pi$ are shaded in the following graph:\n\n\n\n\nTo see that $E_\\pi$ cannot be a breadth-first tree, let's suppose that $Adj[s]$\u008d contains $u$ before $v$. $\\text{BFS}$ adds edges $(s, u)$ and $(s, v)$ to the breadth-first tree. Since $u$ is enqueued before $v$, $\\text{BFS}$ then adds edges $(u, w)$ and $(u, x)$. (The order of $w$ and $x$ in $Adj[u]$\u008d doesn't matter.) Symmetrically, if $Adj[s]$\u008d contains $v$ before $u$, then $\\text{BFS}$ adds edges $(s, v)$ and $(s, u)$ to the breadth-first tree, $v$ is enqueued before $u$, and $\\text{BFS}$ adds edges $(v, w)$ and $(v, x)$. (Again, the order of $w$ and $x$ in $Adj[v]$ doesn't matter.) $\\text{BFS}$ will never put both edges $(u, w)$ and $(v, x)$ into the breadth-first tree. In fact, it will also never put both edges $(u, x)$ and $(v, w)$ into the breadth-first tree.\n\n\n22.2-7\n\n\n\n\nThere are two types of professional wrestlers: ''babyfaces'' (''good guys'') and ''heels'' (''bad guys''). Between any pair of professional wrestlers, there may or may not be a rivalry. Suppose we have $n$ professional wrestlers and we have a list of $r$ pairs of wrestlers for which there are rivalries. Give an $O(n + r)$-time algorithm that determines whether it is possible to designate some of the wrestlers as babyfaces and the remainder as heels such that each rivalry is between a babyface and a heel. If it is possible to perform such a designation, your algorithm should produce it.\n\n\n\n\nCreate a graph $G$ where each vertex represents a wrestler and each edge represents a rivalry. The graph will contain $n$ vertices and $r$ edges.\n\n\nPerform as many $\\text{BFS}$'s as needed to visit all vertices. Assign all wrestlers whose distance is even to be babyfaces and all wrestlers whose distance is odd to be heels. Then check each edge to verify that it goes between a babyface and a heel. This solution would take $O(n + r)$ time for the $\\text{BFS}$, $O(n)$ time to designate each wrestler as a babyface or heel, and $O(r)$ time to check edges, which is $O(n + r)$ time overall.\n\n\n22.2-8 $\\star$\n\n\n\n\nThe \ndiameter\n of a tree $T = (V, E)$ is defined as $\\max_{u,v \\in V} \\delta(u, v)$, that is, the largest of all shortest-path distances in the tree. Give an efficient algorithm to compute the diameter of a tree, and analyze the running time of your algorithm.\n\n\n\n\nSuppose that a and b are the endpoints of the path in the tree which achieve the diameter, and without loss of generality assume that $a$ and $b$ are the unique pair which do so. Let $s$ be any vertex in $T$. We claim that the result of a single $\\text{BFS}$ will return either $a$ or $b$ (or both) as the vertex whose distance from $s$ is greatest.\n\n\nTo see this, suppose to the contrary that some other vertex $x$ is shown to be furthest from $s$. (Note that $x$ cannot be on the path from $a$ to $b$, otherwise we could extend). Then we have\n\n\n$$d(s, a) < d(s, x)$$\n\n\nand\n\n\n$$d(s, b) < d(s, x).$$\n\n\nLet $c$ denote the vertex on the path from $a$ to $b$ which minimizes $d(s, c)$. Since the graph is in fact a tree, we must have\n\n\n$$d(s, a) = d(s, c) + d(c, a)$$\n\n\nand\n\n\n$$d(s, b) = d(s, c) + d(c, b).$$\n\n\n(If there were another path, we could form a cycle). Using the triangle inequality and inequalities and equalities mentioned above we must have\n\n\n\\begin{align}\nd(a, b) + 2d(s, c) & = d(s, c) + d(c, b) + d(s, c) + d(c, a) \\\\\n                   & < d(s, x) + d(s, c) + d(c, b).\n\\end{align}\n\n\nI claim that $d(x, b) = d(s, c) + d(s, b)$. If not, then by the triangle inequality we must have a strict less-than. In other words, there is some path from $x$ to $b$ which does not go through $c$. This gives the contradiction, because it implies there is a cycle formed by concatenating these paths. Then we have\n\n\n$$d(a, b) < d(a, b) + 2d(s, c) < d(x, b).$$\n\n\nSince it is assumed that $d(a, b)$ is maximal among all pairs, we have a contradiction. Therefore, since trees have $|V| - 1$ edges, we can run $\\text{BFS}$ a single time in $O(V)$ to obtain one of the vertices which is the endpoint of the longest simple path contained in the graph. Running $\\text{BFS}$ again will show us where the other one is, so we can solve the diameter problem for trees in $O(V)$.\n\n\n22.2-9\n\n\n\n\nLet $G = (V, E)$ be a connected, undirected graph. Give an $O(V + E)$-time algorithm to compute a path in $G$ that traverses each edge in $E$ exactly once in each direction. Describe how you can find your way out of a maze if you are given a large supply of pennies.\n\n\n\n\nFirst, the algorithm computes a minimum spanning tree of the graph. Note that this can be done using the procedures of Chapter 23. It can also be done by performing a breadth first search, and restricting to the edges between $v$ and $v.\\pi$ for every $v$. To aide in not double counting edges, fix any ordering $\\le$ on the vertices before hand. Then, we will construct the sequence of steps by calling $\\text{MAKE-PATH}(s)$, where $s$ was the root used for the $\\text{BFS}$.\n\n\nMAKE\n-\nPATH\n(\nu\n)\n\n    \nfor\n \nv\n \nadjacent\n \nto\n \nu\n \nin\n \nthe\n \noriginal\n \ngraph\n,\n \nbut\n \nnot\n \nin\n \nthe\n \ntree\n \nsuch\n \nthat\n \nu\n \n\u2264\n \nv\n\n        \ngo\n \nto\n \nv\n \nand\n \nback\n \nto\n \nu\n\n    \nfor\n \nv\n \nadjacent\n \nto\n \nu\n \nin\n \nthe\n \ntree\n,\n \nbut\n \nnot\n \nequal\n \nto\n \nu\n.\nPI\n\n        \ngo\n \nto\n \nv\n\n        \nperform\n \nthe\n \npath\n \nproscribed\n \nby\n \nMAKE\n-\nPATH\n(\nv\n)\n\n    \ngo\n \nto\n \nu\n.\nPI",
            "title": "22.2 Breadth-first search"
        },
        {
            "location": "/Chap22/22.2/#222-1",
            "text": "Show the $d$ and $\\pi$ values that result from running breadth-first search on the directed graph of Figure 22.2(a), using vertex $3$ as the source.   \\begin{array}{c|cccccc}\n\\text{vertex} & 1 & 2 & 3 & 4 & 5 & 6 \\\\\n\\hline\nd & \\infty & 3 & 0 & 2 & 1 & 1 \\\\\n\\pi & \\text{NIL} & 4 & \\text{NIL} & 5 & 3 & 3\n\\end{array}",
            "title": "22.2-1"
        },
        {
            "location": "/Chap22/22.2/#222-2",
            "text": "Show the $d$ and $\\pi$ values that result from running breadth-first search on the undirected graph of Figure 22.3, using vertex $u$ as the source.   \\begin{array}{c|cccccc}\n\\text{vertex} & r & s & t & u & v & w & x & y \\\\\n\\hline\nd & 4 & 3 & 1 & 0 & 5 & 2 & 1 & 1 \\\\\n\\pi & s & w & u & \\text{NIL} & r & t & u & u\n\\end{array}",
            "title": "22.2-2"
        },
        {
            "location": "/Chap22/22.2/#222-3",
            "text": "Show that using a single bit to store each vertex color suffices by arguing that the $\\text{BFS}$ procedure would produce the same result if lines 5 and 14 were removed.   $\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change.  The $\\text{BFS}$ procedure cares only whether a vertex is white or not. $A$ vertex $v$ must become non-white at the same time that $v.d$ is assigned a finite value so that we do not attempt to assign to $v.d$ again, and so we need to change vertex colors in lines 5 and 14. Once we have changed a vertex's color to non-white, we do not need to change it again.",
            "title": "22.2-3"
        },
        {
            "location": "/Chap22/22.2/#222-4",
            "text": "What is the running time of $\\text{BFS}$ if we represent its input graph by an adjacency matrix and modify the algorithm to handle this form of input?   The correctness proof for the $\\text{BFS}$ algorithm shows that $d[u] = \\delta(s, u)$, and the algorithm doesn't assume that the adjacency lists are in any particular order.  In Figure 22.3, if t precedes $x$ in $Adj[w]$, we can get the breadth-first tree shown in the figure. But if $x$ precedes $t$ in $Adj[w]$ and $u$ precedes $y$ in $Adj[x]$, we can get edge $(x, u)$ in the breadth-first tree.",
            "title": "22.2-4"
        },
        {
            "location": "/Chap22/22.2/#222-5",
            "text": "Argue that in a breadth-first search, the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list. Using Figure 22.3 as an example, show that the breadth-first tree computed by $\\text{BFS}$ can depend on the ordering within adjacency lists.   The correctness proof for the $\\text{BFS}$ algorithm shows that $\\delta.d = \\delta(s, u)$, and the algorithm doesn't assume that the adjacency lists are in any particular order.  In Figure 22.3, if $t$ precedes $x$ in $Adj[w]$\u008d, we can get the breadth-first tree shown in the figure. But if $x$ precedes $t$ in $Adj[w]$\u008d and $u$ precedes $y$ in $Adj[x]$\u008d, we can get edge $(x, u)$ in the breadth-first tree.",
            "title": "22.2-5"
        },
        {
            "location": "/Chap22/22.2/#222-6",
            "text": "Give an example of a directed graph $G = (V, E)$, a source vertex $s \\in V$, and a set of tree edges $E_\\pi \\subseteq E$ such that for each vertex $v \\in V$, the unique simple path in the graph $(V, E_\\pi)$ from $s$ to $v$ is a shortest path in $G$, yet the set of edges $E_\\pi$ cannot be produced by running $\\text{BFS}$ on $G$, no matter how the vertices are ordered in each adjacency list.   The edges in $E_\\pi$ are shaded in the following graph:   To see that $E_\\pi$ cannot be a breadth-first tree, let's suppose that $Adj[s]$\u008d contains $u$ before $v$. $\\text{BFS}$ adds edges $(s, u)$ and $(s, v)$ to the breadth-first tree. Since $u$ is enqueued before $v$, $\\text{BFS}$ then adds edges $(u, w)$ and $(u, x)$. (The order of $w$ and $x$ in $Adj[u]$\u008d doesn't matter.) Symmetrically, if $Adj[s]$\u008d contains $v$ before $u$, then $\\text{BFS}$ adds edges $(s, v)$ and $(s, u)$ to the breadth-first tree, $v$ is enqueued before $u$, and $\\text{BFS}$ adds edges $(v, w)$ and $(v, x)$. (Again, the order of $w$ and $x$ in $Adj[v]$ doesn't matter.) $\\text{BFS}$ will never put both edges $(u, w)$ and $(v, x)$ into the breadth-first tree. In fact, it will also never put both edges $(u, x)$ and $(v, w)$ into the breadth-first tree.",
            "title": "22.2-6"
        },
        {
            "location": "/Chap22/22.2/#222-7",
            "text": "There are two types of professional wrestlers: ''babyfaces'' (''good guys'') and ''heels'' (''bad guys''). Between any pair of professional wrestlers, there may or may not be a rivalry. Suppose we have $n$ professional wrestlers and we have a list of $r$ pairs of wrestlers for which there are rivalries. Give an $O(n + r)$-time algorithm that determines whether it is possible to designate some of the wrestlers as babyfaces and the remainder as heels such that each rivalry is between a babyface and a heel. If it is possible to perform such a designation, your algorithm should produce it.   Create a graph $G$ where each vertex represents a wrestler and each edge represents a rivalry. The graph will contain $n$ vertices and $r$ edges.  Perform as many $\\text{BFS}$'s as needed to visit all vertices. Assign all wrestlers whose distance is even to be babyfaces and all wrestlers whose distance is odd to be heels. Then check each edge to verify that it goes between a babyface and a heel. This solution would take $O(n + r)$ time for the $\\text{BFS}$, $O(n)$ time to designate each wrestler as a babyface or heel, and $O(r)$ time to check edges, which is $O(n + r)$ time overall.",
            "title": "22.2-7"
        },
        {
            "location": "/Chap22/22.2/#222-8-star",
            "text": "The  diameter  of a tree $T = (V, E)$ is defined as $\\max_{u,v \\in V} \\delta(u, v)$, that is, the largest of all shortest-path distances in the tree. Give an efficient algorithm to compute the diameter of a tree, and analyze the running time of your algorithm.   Suppose that a and b are the endpoints of the path in the tree which achieve the diameter, and without loss of generality assume that $a$ and $b$ are the unique pair which do so. Let $s$ be any vertex in $T$. We claim that the result of a single $\\text{BFS}$ will return either $a$ or $b$ (or both) as the vertex whose distance from $s$ is greatest.  To see this, suppose to the contrary that some other vertex $x$ is shown to be furthest from $s$. (Note that $x$ cannot be on the path from $a$ to $b$, otherwise we could extend). Then we have  $$d(s, a) < d(s, x)$$  and  $$d(s, b) < d(s, x).$$  Let $c$ denote the vertex on the path from $a$ to $b$ which minimizes $d(s, c)$. Since the graph is in fact a tree, we must have  $$d(s, a) = d(s, c) + d(c, a)$$  and  $$d(s, b) = d(s, c) + d(c, b).$$  (If there were another path, we could form a cycle). Using the triangle inequality and inequalities and equalities mentioned above we must have  \\begin{align}\nd(a, b) + 2d(s, c) & = d(s, c) + d(c, b) + d(s, c) + d(c, a) \\\\\n                   & < d(s, x) + d(s, c) + d(c, b).\n\\end{align}  I claim that $d(x, b) = d(s, c) + d(s, b)$. If not, then by the triangle inequality we must have a strict less-than. In other words, there is some path from $x$ to $b$ which does not go through $c$. This gives the contradiction, because it implies there is a cycle formed by concatenating these paths. Then we have  $$d(a, b) < d(a, b) + 2d(s, c) < d(x, b).$$  Since it is assumed that $d(a, b)$ is maximal among all pairs, we have a contradiction. Therefore, since trees have $|V| - 1$ edges, we can run $\\text{BFS}$ a single time in $O(V)$ to obtain one of the vertices which is the endpoint of the longest simple path contained in the graph. Running $\\text{BFS}$ again will show us where the other one is, so we can solve the diameter problem for trees in $O(V)$.",
            "title": "22.2-8 $\\star$"
        },
        {
            "location": "/Chap22/22.2/#222-9",
            "text": "Let $G = (V, E)$ be a connected, undirected graph. Give an $O(V + E)$-time algorithm to compute a path in $G$ that traverses each edge in $E$ exactly once in each direction. Describe how you can find your way out of a maze if you are given a large supply of pennies.   First, the algorithm computes a minimum spanning tree of the graph. Note that this can be done using the procedures of Chapter 23. It can also be done by performing a breadth first search, and restricting to the edges between $v$ and $v.\\pi$ for every $v$. To aide in not double counting edges, fix any ordering $\\le$ on the vertices before hand. Then, we will construct the sequence of steps by calling $\\text{MAKE-PATH}(s)$, where $s$ was the root used for the $\\text{BFS}$.  MAKE - PATH ( u ) \n     for   v   adjacent   to   u   in   the   original   graph ,   but   not   in   the   tree   such   that   u   \u2264   v \n         go   to   v   and   back   to   u \n     for   v   adjacent   to   u   in   the   tree ,   but   not   equal   to   u . PI \n         go   to   v \n         perform   the   path   proscribed   by   MAKE - PATH ( v ) \n     go   to   u . PI",
            "title": "22.2-9"
        },
        {
            "location": "/Chap22/22.3/",
            "text": "22.3-1\n\n\n\n\nMake a 3-by-3 chart with row and column labels $\\text{WHITE}$, $\\text{GRAY}$, and $\\text{BLACK}$. In each cell $(i, j)$, indicate whether, at any point during a depth-first search of a directed graph, there can be an edge from a vertex of color $i$ to a vertex of color $j$. For each possible edge, indicate what edge types it can be. Make a second such chart for depth-first search of an undirected graph.\n\n\n\n\n\n\n\n\nDirected:\n\n\n\\begin{array}{c|ccc}\nfrom\\backslash to & \\text{BLACK}                & \\text{GRAY}                & \\text{WHITE} \\\\\n\\hline\n\\text{BLACK}      & \\text{Allkinds}             & \\text{Back, Cross}         & \\text{Back, Cross} \\\\\n\\text{GRAY}       & \\text{Tree, Forward, Cross} & \\text{Tree, Forward, Back} & \\text{Back, Cross} \\\\\n\\text{WHITE}      & \\text{Cross, Tree, Forward} & \\text{Cross, Back}         & \\text{Allkinds}\n\\end{array}\n\n\n\n\n\n\nUndirected:\n\n\n\\begin{array}{c|ccc}\nfrom\\backslash to & \\text{BLACK}    & \\text{GRAY}                & \\text{WHITE} \\\\\n\\hline\n\\text{BLACK}      & \\text{Allkinds} & \\text{Allkinds}            & \\text{Allkinds} \\\\\n\\text{GRAY}       & -               & \\text{Tree, Forward, Back} & \\text{Allkinds} \\\\\n\\text{WHITE}      & -               & -                          & \\text{Allkinds}\n\\end{array}\n\n\n\n\n\n\n22.3-2\n\n\n\n\nShow how depth-first search works on the graph of Figure 22.6. Assume that the \nfor\n loop of lines 5\u20137 of the $\\text{DFS}$ procedure considers the vertices in alphabetical order, and assume that each adjacency list is ordered alphabetically. Show the discovery and finishing times for each vertex, and show the classification of each edge.\n\n\n\n\nThe following table gives the discovery time and finish time for each vetex in the graph.\n\n\n\\begin{array}{ccc}\n\\text{Vertex} & \\text{Discovered} & \\text{Finished} \\\\\n\\hline\nq &  1 & 16 \\\\\nr & 17 & 20 \\\\\ns &  2 &  7 \\\\\nt &  8 & 15 \\\\\nu & 18 & 19 \\\\\nv &  3 &  6 \\\\\nw &  4 &  5 \\\\\nx &  9 & 12 \\\\\ny & 13 & 14 \\\\\nz & 10 & 11\n\\end{array}\n\n\n\n\nTree edges:\n $(q, s)$, $(s, v)$, $(v, w)$, $(q, t)$, $(t, x)$, $(x, z)$, $(t, y)$, $(r, u)$.\n\n\nBack edges:\n $(w, s)$, $(z, x)$, $(y, q)$. \n\n\nForward edges:\n $(q, w)$.\n\n\nCross edges:\n $(r, y)$, $(u, y)$.\n\n\n\n\n22.3-3\n\n\n\n\nShow the parenthesis structure of the depth-first search of Figure 22.4.\n\n\n\n\nAs pointed out in figure 22.5, the parentheses structure of the $\\text{DFS}$ of figure 22.4 is $(((())()))(()())$.\n\n\n22.3-4\n\n\n\n\nShow that using a single bit to store each vertex color suffices by arguing that the $\\text{DFS}$ procedure would produce the same result if line 3 of $\\text{DFS-VISIT}$ was removed.\n\n\n\n\n$\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change.\n\n\nThe $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures care only whether a vertex is white or not. By coloring vertex $u$ gray when it is first visited, in line 3 of $\\text{DFS-VISIT}$, we ensure that $u$ will not be visited again. Once we have changed a vertex's color to non-white, we do not need to change it again.\n\n\n22.3-5\n\n\n\n\nShow that edge $(u, v)$ is\n\n\na.\n a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$,\n\n\nb.\n a back edge if and only if $v.d \\le u.d < u.f \\le v.f$, and\n\n\nc.\n a cross edge if and only if $v.d < v.f < u.d < u.f$.\n\n\n\n\na.\n Edge $(u, v)$ is a tree edge or forward edge if and only if $v$ is a descendant of $u$ in the depth-first forest. (If $(u, v)$ is a back edge, then $u$ is a descendant of $v$, and if $(u, v)$ is a cross edge, then neither of $u$ or $v$ is a descendant of the other.) By Corollary 22.8, therefore, $(u, v)$ is a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$.\n\n\nb.\n First, suppose that $(u, v)$ is a back edge. A self-loop is by definition a back edge. If $(u, v)$ is a self-loop, then clearly $v.d = u.d < u.f = v.f$. If $(u, v)$ is not a self-loop, then $u$ is a descendant of $v$ in the depth-first forest, and by Corollary 22.8, $v.d < u.d < u.f < v.f$.\n\n\nNow, suppose that $v.d \\le u.d < u.f \\le v.f$. If $u$ and $v$ are the same vertex, then $v.d = u.d < u.f = v.f$, and $(u, v)$ is a self-loop and hence a back edge. If $u$ and $v$ are distinct, then $v.d < u.d < u.f < v.f$. By the parenthesis theorem, interval $[u.d, u.f]$\u008d is contained entirely within the interval $[v.d, v.f]$\u008d, and $u$ is a descendant of $v$ in a depth-first tree. Thus, $(u, v)$ is a back edge.\n\n\nc.\n First, suppose that $(u, v)$ is a cross edge. Since neither $u$ nor $v$ is an ancestor of the other, the parenthesis theorem says that the intervals $[u.d, u.f]$ and $[v.d, v.f]$ are entirely disjoint. Thus, we must have either $u.d < u.f < v.d < v.f$ or $v.d < v.f < u.d < u.f$. We claim that we cannot have $u.d < v.d$ if $(u, v)$ is a cross edge. Why? If $u.d < v.d$, then $v$ is white at time $u.d$. By the white-path theorem, $v$ is a descendant of $u$, which contradicts $(u, v)$ being a cross edge. Thus, we must have $v.d < v.f < u.d < u.f$.\n\n\nNow suppose that $v.d < v.f < u.d < u.f$. By the parenthesis theorem, neither $u$ nor $v$ is a descendant of the other, which means that $(u, v)$ must be a cross edge.\n\n\n22.3-6\n\n\n\n\nShow that in an undirected graph, classifying an edge $(u, v)$ as a tree edge or a back edge according to whether $(u, v)$ or $(v, u)$ is encountered first during the depth-first search is equivalent to classifying it according to the ordering of the four types in the classification scheme.\n\n\n\n\nBy Theorem 22.10, every edge of an undirected graph is either a tree edge or a back edge. First suppose that $v$ is first discovered by exploring edge $(u, v)$. Then by definition, $(u, v)$ is a tree edge. Moreover, $(u, v)$ must have been discovered before $(v, u)$ because once $(v, u)$ is explored, $v$ is necessarily discovered. Now suppose that $v$ isn't first discovered by $(u, v)$. Then it must be discovered by $(r, v)$ for some $r\\ne u$. If $u$ hasn't yet been discovered then if $(u, v)$ is explored first, it must be a back edge since $v$ is an ancestor of $u$. If $u$ has been discovered then $u$ is an ancestor of $v$, so $(v, u)$ is a back edge.\n\n\n22.3-7\n\n\n\n\nRewrite the procedure $\\text{DFS}$, using a stack to eliminate recursion.\n\n\n\n\nSee the algorithm $\\text{DFS-STACK}(G)$. Note that by a similar justification to 22.2-3, we may remove line 8 from the original $\\text{DFS-VISIT}$ algorithm without changing the final result of the program, that is just working with the colors white and gray.\n\n\n22.3-8\n\n\n\n\nGive a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, and if $u.d < v.d$ in a depth-first search of $G$, then $v$ is a descendant of $u$ in the depth-first forest produced.\n\n\n\n\nLet us consider the example graph depth-first search below.\n\n\n\\begin{array}{c|cc}\n  & d & f \\\\\n\\hline\nw & 1 & 6 \\\\\nu & 2 & 3 \\\\\nv & 4 & 5\n\\end{array}\n\n\n\n\nClearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced. We can see that $u.d < v.d$ in the depth-first search but $v$ is not a descendant of $u$ in the forest.\n\n\n22.3-9\n\n\n\n\nGive a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, then any depth-first search must result in $v.d \\le u.f$.\n\n\n\n\nLet us consider the example graph depth-first search below.\n\n\n\\begin{array}{c|cc}\n  & d & f \\\\\n\\hline\nw & 1 & 6 \\\\\nu & 2 & 3 \\\\\nv & 4 & 5\n\\end{array}\n\n\n\n\nClearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced by search. However, $v.d > u.f$ and the conjecture is false.\n\n\n22.3-10\n\n\n\n\nModify the pseudocode for depth-first search so that it prints out every edge in the directed graph $G$, together with its type. Show what modifications, if any, you need to make if $G$ is undirected.\n\n\n\n\nWe need only update $\\text{DFS-VISIT}$. If $G$ is undirected we don't need to make any modifications. We simply note that lines 11 through 16 will never be executed.\n\n\nDFS\n-\nVISIT\n-\nPRINT\n(\nG\n,\n \nu\n)\n\n  \ntime\n \n=\n \ntime\n \n+\n \n1\n\n  \nu\n.\nd\n \n=\n \ntime\n\n  \nu\n.\ncolor\n \n=\n \nGRAY\n\n  \nfor\n \neach\n \nv\n \nin\n \nG\n.\nAdj\n[\nu\n]\n\n      \nif\n \nv\n.\ncolor\n \n==\n \nwhite\n\n          \nprint\n \n\"(u,v) is a Tree edge.\"\n\n          \nv\n.\nPI\n \n=\n \nu\n\n          \nDFS\n-\nVISIT\n-\nPRINT\n(\nG\n,\n \nv\n)\n\n      \nelse\n \nif\n \nv\n.\ncolor\n \n==\n \ngray\n\n          \nprint\n \n\"(u, v) is a Back edge.\"\n\n      \nelse\n\n          \nif\n \nv\n.\nd\n \n>\n \nu\n.\nd\n\n              \nprint\n \n\"(u, v) is a Forward edge.\"\n\n          \nelse\n \nprint\n \n\"(u, v) is a Cross edge.\"\n\n\n\n\n\n22.3-11\n\n\n\n\nExplain how a vertex $u$ of a directed graph can end up in a depth-first tree containing only $u$, even though $u$ has both incoming and outgoing edges in $G$.\n\n\n\n\nLet us consider the example graph and depth-first search below.\n\n\n\\begin{array}{c|cc}\n  & d & f \\\\\n\\hline\nw & 1 & 2 \\\\\nu & 3 & 4 \\\\\nv & 5 & 6\n\\end{array}\n\n\n\n\nCleary $u$ has both incoming and outgoing edges in $G$ but a depth-first search of $G$ produced a depth-first forest where $u$ is in a tree by itself.\n\n\n22.3-12\n\n\n\n\nShow that we can use a depth-first search of an undirected graph $G$ to identify the connected components of $G$, and that the depth-first forest contains as many trees as $G$ has connected components. More precisely, show how to modify depth-first search so that it assigns to each vertex $v$ an integer label $v.cc$ between $1$ and $k$, where $k$ is the number of connected components of $G$, such that $u.cc = v.cc$ if and only if $u$ and $v$ are in the same connected component.\n\n\n\n\nThe following pseudocode modifies the $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures to assign values to the $cc$ attributes of vertices.\n\n\nDFS\n(\nG\n)\n\n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nu\n.\ncolor\n \n=\n \nWHITE\n\n        \nu\n.\nPI\n \n=\n \nNIL\n\n    \ntime\n \n=\n \n0\n\n    \ncounter\n \n=\n \n0\n\n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nif\n \nu\n.\ncolor\n \n==\n \nWHITE\n\n            \ncounter\n \n=\n \ncounter\n \n+\n \n1\n\n            \nDFS\n-\nVISIT\n(\nG\n,\n \nu\n,\n \ncounter\n)\n\n\n\n\n\nDFS\n-\nVISIT\n(\nG\n,\n \nu\n,\n \ncounter\n)\n\n    \nu\n.\ncc\n \n=\n \ncounter\n      \n// label the vertex\n\n    \ntime\n \n=\n \ntime\n \n+\n \n1\n\n    \nu\n.\nd\n \n=\n \ntime\n\n    \nu\n.\ncolor\n \n=\n \nGRAY\n\n    \nfor\n \neach\n \nv\n \n\u2208\n \nG\n.\nAdj\n[\nu\n]\n\n        \nif\n \nv\n.\ncolor\n \n==\n \nWHITE\n\n            \nv\n.\nPI\n \n=\n \nu\n\n            \nDFS\n-\nVISIT\n(\nG\n,\n \nv\n,\n \ncounter\n)\n\n    \nu\n.\ncolor\n \n=\n \nBLACK\n\n    \ntime\n \n=\n \ntime\n \n+\n \n1\n\n    \nu\n.\nf\n \n=\n \ntime\n\n\n\n\n\nThis $\\text{DFS}$ increments a counter each time $\\text{DFS-VISIT}$ is called to grow a new tree in the $\\text{DFS}$ forest. Every vertex visited (and added to the tree) by $\\text{DFS-VISIT}$ is labeled with that same counter value. Thus $u.vv = v.cc$ if and only if $u$ and $v$ are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, and the final value of the counter is the number of calls that were made to $\\text{DFS-VISIT}$ by $\\text{DFS}$. Also, since every vertex is visited eventually, every vertex is labeled.\n\n\nThus all we need to show is that the vertices visited by each call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are exactly the vertices in one connected component of $G$.\n\n\n\n\n\n\nAll vertices in a connected component are visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$:\n\n\nLet $u$ be the first vertex in component $C$ visited by $\\text{DFS-VISIT}$. Since a vertex becomes non-white only when it is visited, all vertices in $C$ are white when $\\text{DFS-VISIT}$ is called for $u$. Thus, by the white-path theorem, all vertices in $C$ become descendants of $u$ in the forest, which means that all vertices in $C$ are visited (by recursive calls to $\\text{DFS-VISIT}$) before $\\text{DFS-VISIT}$ returns to $\\text{DFS}$.\n\n\n\n\n\n\nAll vertices visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are in the same connected component:\n\n\nIf two vertices are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, they are in the same connected component, because vertices are visited only by following paths in $G$ (by following edges found in adjacency lists, starting from some vertex).\n\n\n\n\n\n\n22.3-13 $\\star$\n\n\n\n\nA directed graph $G = (V, E)$ is \nsingly connected\n if $u \\leadsto v$ implies that $G$ contains at most one simple path from $u$ to $v$ for all vertices $u, v \\in V$. Give an efficient algorithm to determine whether or not a directed graph is singly connected.\n\n\n\n\nThis can be done in time $O(|V||E|)$. To do this, first perform a topological sort of the vertices. Then, we will contain for each vertex a list of it's ancestors with $in\\text-degree$ $0$. We compute these lists for each vertex in the order starting from the earlier ones topologically. \n\n\nThen, if we ever have a vertex that has the same degree $0$ vertex appearing in the lists of two of its immediate parents, we know that the graph is not singly connected. however, if at each step we have that at each step all of the parents have disjoint sets of degree $0$ vertices as ancestors, the graph is singly connected. Since, for each vertex, the amount of time required is bounded by the number of vertices times the $in\\text-degree$ of the particular vertex, the total runtime is bounded by $O(|V||E|)$.",
            "title": "22.3 Depth-first search"
        },
        {
            "location": "/Chap22/22.3/#223-1",
            "text": "Make a 3-by-3 chart with row and column labels $\\text{WHITE}$, $\\text{GRAY}$, and $\\text{BLACK}$. In each cell $(i, j)$, indicate whether, at any point during a depth-first search of a directed graph, there can be an edge from a vertex of color $i$ to a vertex of color $j$. For each possible edge, indicate what edge types it can be. Make a second such chart for depth-first search of an undirected graph.     Directed:  \\begin{array}{c|ccc}\nfrom\\backslash to & \\text{BLACK}                & \\text{GRAY}                & \\text{WHITE} \\\\\n\\hline\n\\text{BLACK}      & \\text{Allkinds}             & \\text{Back, Cross}         & \\text{Back, Cross} \\\\\n\\text{GRAY}       & \\text{Tree, Forward, Cross} & \\text{Tree, Forward, Back} & \\text{Back, Cross} \\\\\n\\text{WHITE}      & \\text{Cross, Tree, Forward} & \\text{Cross, Back}         & \\text{Allkinds}\n\\end{array}    Undirected:  \\begin{array}{c|ccc}\nfrom\\backslash to & \\text{BLACK}    & \\text{GRAY}                & \\text{WHITE} \\\\\n\\hline\n\\text{BLACK}      & \\text{Allkinds} & \\text{Allkinds}            & \\text{Allkinds} \\\\\n\\text{GRAY}       & -               & \\text{Tree, Forward, Back} & \\text{Allkinds} \\\\\n\\text{WHITE}      & -               & -                          & \\text{Allkinds}\n\\end{array}",
            "title": "22.3-1"
        },
        {
            "location": "/Chap22/22.3/#223-2",
            "text": "Show how depth-first search works on the graph of Figure 22.6. Assume that the  for  loop of lines 5\u20137 of the $\\text{DFS}$ procedure considers the vertices in alphabetical order, and assume that each adjacency list is ordered alphabetically. Show the discovery and finishing times for each vertex, and show the classification of each edge.   The following table gives the discovery time and finish time for each vetex in the graph.  \\begin{array}{ccc}\n\\text{Vertex} & \\text{Discovered} & \\text{Finished} \\\\\n\\hline\nq &  1 & 16 \\\\\nr & 17 & 20 \\\\\ns &  2 &  7 \\\\\nt &  8 & 15 \\\\\nu & 18 & 19 \\\\\nv &  3 &  6 \\\\\nw &  4 &  5 \\\\\nx &  9 & 12 \\\\\ny & 13 & 14 \\\\\nz & 10 & 11\n\\end{array}   Tree edges:  $(q, s)$, $(s, v)$, $(v, w)$, $(q, t)$, $(t, x)$, $(x, z)$, $(t, y)$, $(r, u)$.  Back edges:  $(w, s)$, $(z, x)$, $(y, q)$.   Forward edges:  $(q, w)$.  Cross edges:  $(r, y)$, $(u, y)$.",
            "title": "22.3-2"
        },
        {
            "location": "/Chap22/22.3/#223-3",
            "text": "Show the parenthesis structure of the depth-first search of Figure 22.4.   As pointed out in figure 22.5, the parentheses structure of the $\\text{DFS}$ of figure 22.4 is $(((())()))(()())$.",
            "title": "22.3-3"
        },
        {
            "location": "/Chap22/22.3/#223-4",
            "text": "Show that using a single bit to store each vertex color suffices by arguing that the $\\text{DFS}$ procedure would produce the same result if line 3 of $\\text{DFS-VISIT}$ was removed.   $\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change.  The $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures care only whether a vertex is white or not. By coloring vertex $u$ gray when it is first visited, in line 3 of $\\text{DFS-VISIT}$, we ensure that $u$ will not be visited again. Once we have changed a vertex's color to non-white, we do not need to change it again.",
            "title": "22.3-4"
        },
        {
            "location": "/Chap22/22.3/#223-5",
            "text": "Show that edge $(u, v)$ is  a.  a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$,  b.  a back edge if and only if $v.d \\le u.d < u.f \\le v.f$, and  c.  a cross edge if and only if $v.d < v.f < u.d < u.f$.   a.  Edge $(u, v)$ is a tree edge or forward edge if and only if $v$ is a descendant of $u$ in the depth-first forest. (If $(u, v)$ is a back edge, then $u$ is a descendant of $v$, and if $(u, v)$ is a cross edge, then neither of $u$ or $v$ is a descendant of the other.) By Corollary 22.8, therefore, $(u, v)$ is a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$.  b.  First, suppose that $(u, v)$ is a back edge. A self-loop is by definition a back edge. If $(u, v)$ is a self-loop, then clearly $v.d = u.d < u.f = v.f$. If $(u, v)$ is not a self-loop, then $u$ is a descendant of $v$ in the depth-first forest, and by Corollary 22.8, $v.d < u.d < u.f < v.f$.  Now, suppose that $v.d \\le u.d < u.f \\le v.f$. If $u$ and $v$ are the same vertex, then $v.d = u.d < u.f = v.f$, and $(u, v)$ is a self-loop and hence a back edge. If $u$ and $v$ are distinct, then $v.d < u.d < u.f < v.f$. By the parenthesis theorem, interval $[u.d, u.f]$\u008d is contained entirely within the interval $[v.d, v.f]$\u008d, and $u$ is a descendant of $v$ in a depth-first tree. Thus, $(u, v)$ is a back edge.  c.  First, suppose that $(u, v)$ is a cross edge. Since neither $u$ nor $v$ is an ancestor of the other, the parenthesis theorem says that the intervals $[u.d, u.f]$ and $[v.d, v.f]$ are entirely disjoint. Thus, we must have either $u.d < u.f < v.d < v.f$ or $v.d < v.f < u.d < u.f$. We claim that we cannot have $u.d < v.d$ if $(u, v)$ is a cross edge. Why? If $u.d < v.d$, then $v$ is white at time $u.d$. By the white-path theorem, $v$ is a descendant of $u$, which contradicts $(u, v)$ being a cross edge. Thus, we must have $v.d < v.f < u.d < u.f$.  Now suppose that $v.d < v.f < u.d < u.f$. By the parenthesis theorem, neither $u$ nor $v$ is a descendant of the other, which means that $(u, v)$ must be a cross edge.",
            "title": "22.3-5"
        },
        {
            "location": "/Chap22/22.3/#223-6",
            "text": "Show that in an undirected graph, classifying an edge $(u, v)$ as a tree edge or a back edge according to whether $(u, v)$ or $(v, u)$ is encountered first during the depth-first search is equivalent to classifying it according to the ordering of the four types in the classification scheme.   By Theorem 22.10, every edge of an undirected graph is either a tree edge or a back edge. First suppose that $v$ is first discovered by exploring edge $(u, v)$. Then by definition, $(u, v)$ is a tree edge. Moreover, $(u, v)$ must have been discovered before $(v, u)$ because once $(v, u)$ is explored, $v$ is necessarily discovered. Now suppose that $v$ isn't first discovered by $(u, v)$. Then it must be discovered by $(r, v)$ for some $r\\ne u$. If $u$ hasn't yet been discovered then if $(u, v)$ is explored first, it must be a back edge since $v$ is an ancestor of $u$. If $u$ has been discovered then $u$ is an ancestor of $v$, so $(v, u)$ is a back edge.",
            "title": "22.3-6"
        },
        {
            "location": "/Chap22/22.3/#223-7",
            "text": "Rewrite the procedure $\\text{DFS}$, using a stack to eliminate recursion.   See the algorithm $\\text{DFS-STACK}(G)$. Note that by a similar justification to 22.2-3, we may remove line 8 from the original $\\text{DFS-VISIT}$ algorithm without changing the final result of the program, that is just working with the colors white and gray.",
            "title": "22.3-7"
        },
        {
            "location": "/Chap22/22.3/#223-8",
            "text": "Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, and if $u.d < v.d$ in a depth-first search of $G$, then $v$ is a descendant of $u$ in the depth-first forest produced.   Let us consider the example graph depth-first search below.  \\begin{array}{c|cc}\n  & d & f \\\\\n\\hline\nw & 1 & 6 \\\\\nu & 2 & 3 \\\\\nv & 4 & 5\n\\end{array}   Clearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced. We can see that $u.d < v.d$ in the depth-first search but $v$ is not a descendant of $u$ in the forest.",
            "title": "22.3-8"
        },
        {
            "location": "/Chap22/22.3/#223-9",
            "text": "Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, then any depth-first search must result in $v.d \\le u.f$.   Let us consider the example graph depth-first search below.  \\begin{array}{c|cc}\n  & d & f \\\\\n\\hline\nw & 1 & 6 \\\\\nu & 2 & 3 \\\\\nv & 4 & 5\n\\end{array}   Clearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced by search. However, $v.d > u.f$ and the conjecture is false.",
            "title": "22.3-9"
        },
        {
            "location": "/Chap22/22.3/#223-10",
            "text": "Modify the pseudocode for depth-first search so that it prints out every edge in the directed graph $G$, together with its type. Show what modifications, if any, you need to make if $G$ is undirected.   We need only update $\\text{DFS-VISIT}$. If $G$ is undirected we don't need to make any modifications. We simply note that lines 11 through 16 will never be executed.  DFS - VISIT - PRINT ( G ,   u ) \n   time   =   time   +   1 \n   u . d   =   time \n   u . color   =   GRAY \n   for   each   v   in   G . Adj [ u ] \n       if   v . color   ==   white \n           print   \"(u,v) is a Tree edge.\" \n           v . PI   =   u \n           DFS - VISIT - PRINT ( G ,   v ) \n       else   if   v . color   ==   gray \n           print   \"(u, v) is a Back edge.\" \n       else \n           if   v . d   >   u . d \n               print   \"(u, v) is a Forward edge.\" \n           else   print   \"(u, v) is a Cross edge.\"",
            "title": "22.3-10"
        },
        {
            "location": "/Chap22/22.3/#223-11",
            "text": "Explain how a vertex $u$ of a directed graph can end up in a depth-first tree containing only $u$, even though $u$ has both incoming and outgoing edges in $G$.   Let us consider the example graph and depth-first search below.  \\begin{array}{c|cc}\n  & d & f \\\\\n\\hline\nw & 1 & 2 \\\\\nu & 3 & 4 \\\\\nv & 5 & 6\n\\end{array}   Cleary $u$ has both incoming and outgoing edges in $G$ but a depth-first search of $G$ produced a depth-first forest where $u$ is in a tree by itself.",
            "title": "22.3-11"
        },
        {
            "location": "/Chap22/22.3/#223-12",
            "text": "Show that we can use a depth-first search of an undirected graph $G$ to identify the connected components of $G$, and that the depth-first forest contains as many trees as $G$ has connected components. More precisely, show how to modify depth-first search so that it assigns to each vertex $v$ an integer label $v.cc$ between $1$ and $k$, where $k$ is the number of connected components of $G$, such that $u.cc = v.cc$ if and only if $u$ and $v$ are in the same connected component.   The following pseudocode modifies the $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures to assign values to the $cc$ attributes of vertices.  DFS ( G ) \n     for   each   vertex   u   \u2208   G . V \n         u . color   =   WHITE \n         u . PI   =   NIL \n     time   =   0 \n     counter   =   0 \n     for   each   vertex   u   \u2208   G . V \n         if   u . color   ==   WHITE \n             counter   =   counter   +   1 \n             DFS - VISIT ( G ,   u ,   counter )   DFS - VISIT ( G ,   u ,   counter ) \n     u . cc   =   counter        // label the vertex \n     time   =   time   +   1 \n     u . d   =   time \n     u . color   =   GRAY \n     for   each   v   \u2208   G . Adj [ u ] \n         if   v . color   ==   WHITE \n             v . PI   =   u \n             DFS - VISIT ( G ,   v ,   counter ) \n     u . color   =   BLACK \n     time   =   time   +   1 \n     u . f   =   time   This $\\text{DFS}$ increments a counter each time $\\text{DFS-VISIT}$ is called to grow a new tree in the $\\text{DFS}$ forest. Every vertex visited (and added to the tree) by $\\text{DFS-VISIT}$ is labeled with that same counter value. Thus $u.vv = v.cc$ if and only if $u$ and $v$ are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, and the final value of the counter is the number of calls that were made to $\\text{DFS-VISIT}$ by $\\text{DFS}$. Also, since every vertex is visited eventually, every vertex is labeled.  Thus all we need to show is that the vertices visited by each call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are exactly the vertices in one connected component of $G$.    All vertices in a connected component are visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$:  Let $u$ be the first vertex in component $C$ visited by $\\text{DFS-VISIT}$. Since a vertex becomes non-white only when it is visited, all vertices in $C$ are white when $\\text{DFS-VISIT}$ is called for $u$. Thus, by the white-path theorem, all vertices in $C$ become descendants of $u$ in the forest, which means that all vertices in $C$ are visited (by recursive calls to $\\text{DFS-VISIT}$) before $\\text{DFS-VISIT}$ returns to $\\text{DFS}$.    All vertices visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are in the same connected component:  If two vertices are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, they are in the same connected component, because vertices are visited only by following paths in $G$ (by following edges found in adjacency lists, starting from some vertex).",
            "title": "22.3-12"
        },
        {
            "location": "/Chap22/22.3/#223-13-star",
            "text": "A directed graph $G = (V, E)$ is  singly connected  if $u \\leadsto v$ implies that $G$ contains at most one simple path from $u$ to $v$ for all vertices $u, v \\in V$. Give an efficient algorithm to determine whether or not a directed graph is singly connected.   This can be done in time $O(|V||E|)$. To do this, first perform a topological sort of the vertices. Then, we will contain for each vertex a list of it's ancestors with $in\\text-degree$ $0$. We compute these lists for each vertex in the order starting from the earlier ones topologically.   Then, if we ever have a vertex that has the same degree $0$ vertex appearing in the lists of two of its immediate parents, we know that the graph is not singly connected. however, if at each step we have that at each step all of the parents have disjoint sets of degree $0$ vertices as ancestors, the graph is singly connected. Since, for each vertex, the amount of time required is bounded by the number of vertices times the $in\\text-degree$ of the particular vertex, the total runtime is bounded by $O(|V||E|)$.",
            "title": "22.3-13 $\\star$"
        },
        {
            "location": "/Chap22/22.4/",
            "text": "22.4-1\n\n\n\n\nShow the ordering of vertices produced by $\\text{TOPOLOGICAL-SORT}$ when it is run on the dag of Figure 22.8, under the assumption of Exercise 22.3-2.\n\n\n\n\nOur start and finish times from performing the $\\text{DFS}$ are\n\n\n\\begin{array}{ccc}\n\\text{label} & d & f \\\\\n\\hline\nm & 1  & 20 \\\\\nq & 2  & 5  \\\\\nt & 3  & 4  \\\\\nr & 6  & 19 \\\\\nu & 7  & 8  \\\\\ny & 9  & 18 \\\\\nv & 10 & 17 \\\\\nw & 11 & 14 \\\\\nz & 12 & 13 \\\\\nx & 15 & 16 \\\\\nn & 21 & 26 \\\\\no & 22 & 25 \\\\\ns & 24 & 24 \\\\\np & 27 & 28\n\\end{array}\n\n\nAnd so, by reading off the entries in decreasing order of finish time, we have the sequence $p, n, o, s, m, r, y, v, x, w, z, u, q, t$.\n\n\n22.4-2\n\n\n\n\nGive a linear-time algorithm that takes as input a directed acyclic graph $G = (V, E)$ and two vertices $s$ and $t$, and returns the number of simple paths from $s$ to $t$ in $G$. For example, the directed acyclic graph of Figure 22.8 contains exactly four simple paths from vertex $p$ to vertex $v: pov$, $poryv$, $posryv$, and $psryv$. (Your algorithm needs only to count the simple paths, not list them.)\n\n\n\n\nThe algorithm works as follows. The attribute $u.paths$ of node $u$ tells the number of simple paths from $u$ to $v$, where we assume that $v$ is fixed throughout the entire process. To count the number of paths, we can sum the number of paths which leave from each of $u$'s neighbors. Since we have no cycles, we will never risk adding a partially completed number of paths. Moreover, we can never consider the same edge twice among the recursive calls. Therefore, the total number of executions of the for-loop over all recursive calls is $O(V + E)$. Calling $\\text{SIMPLE-PATHS}(s, t)$ yields the desired result.\n\n\nSIMPLE\n-\nPATHS\n(\nu\n,\n \nv\n)\n\n    \nif\n \nu\n \n==\n \nv\n\n        \nreturn\n \n1\n\n    \nelse\n \nif\n \nu\n.\npaths\n \n!=\n \nNIL\n\n        \nreturn\n \nu\n.\npaths\n\n    \nelse\n\n        \nfor\n \neach\n \nw\n \nin\n \nAdj\n[\nu\n]\n\n            \nu\n.\npahts\n \n=\n \nu\n.\npaths\n \n+\n \nSIMPLE\n-\nPATHS\n(\nw\n,\n \nv\n)\n\n        \nreturn\n \nu\n.\npaths\n\n\n\n\n\n22.4-3\n\n\n\n\nGive an algorithm that determines whether or not a given undirected graph $G = (V, E)$ contains a cycle. Your algorithm should run in $O(V)$ time, independent of $|E|$.\n\n\n\n\nAn undirected graph is acyclic (i.e., a forest) if and only if a $\\text{DFS}$ yields no back edges.\n\n\n\n\nIf there's a back edge, there's a cycle.\n\n\nIf there's no back edge, then by Theorem 22.10, there are only tree edges. Hence, the graph is acyclic.\n\n\n\n\nThus, we can run $\\text{DFS}$: if we find a back edge, there's a cycle.\n\n\n\n\nTime: $O(V)$. (Not $O(V + E)$!)\n\n\n\n\nIf we ever see $|V|$ distinct edges, we must have seen a back edge because (by Theorem B.2 on p. 1174) in an acyclic (undirected) forest, $|E| \\le |V| - 1$.\n\n\n22.4-4\n\n\n\n\nProve or disprove: If a directed graph $G$ contains cycles, then $\\text{TOPOLOGICAL-SORT}(G)$ produces a vertex ordering that minimizes the number of ''bad'' edges that are inconsistent with the ordering produced.\n\n\n\n\nThis is not true. Consider the graph $G$ consisting of vertices $a, b, c$, and $d$. Let the edges be $(a, b)$, $(b, c)$, $(a, d)$, $(d, c)$, and $(c, a)$. Suppose that we start the $\\text{DFS}$ of $\\text{TOPOLOGICAL-SORT}$ at vertex $c$. Assuming that $b$ appears before $d$ in the adjacency list of $a$, the order, from latest to earliest, of finish times is $c, a, d, b$. \n\n\nThe ''bad'' edges in this case are $(b, c)$ and $(d, c)$. However, if we had instead ordered them by $a, b, d, c$ then the only bad edges would be $(c, a)$. Thus $\\text{TOPOLOGICAL-SORT}$ doesn't always minimizes the number of ''bad'' edges\n\n\n22.4-5\n\n\n\n\nAnother way to perform topological sorting on a directed acyclic graph $G = (V, E)$ is to repeatedly find a vertex of $in\\text-degree$ $0$, output it, and remove it and all of its outgoing edges from the graph. Explain how to implement this idea so that it runs in time $O(V + E)$. What happens to this algorithm if $G$ has cycles?\n\n\n\n\nTOPOLOGICAL\n-\nSORT\n(\nG\n)\n\n    \n// Initialize in-degree, \u0398(V) time.\n\n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nu\n.\nin\n-\ndegree\n \n=\n \n0\n\n    \n// Compute in-degree, \u0398(V + E) time.\n\n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nfor\n \neach\n \nv\n \n\u2208\n \nG\n.\nAdj\n[\nu\n]\n\n            \nv\n.\nin\n-\ndegree\n \n=\n \nv\n.\nin\n-\ndegree\n \n+\n \n1\n\n    \n// Initialize Queue, \u0398(V) time.\n\n    \nQ\n \n=\n \n\u2205\n\n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nif\n \nu\n.\nin\n-\ndegree\n \n==\n \n0\n\n            \nENQUEUE\n(\nQ\n,\n \nu\n)\n\n    \n// while loop takes O(V + E) time.\n\n    \nwhile\n \nQ\n \n!=\n \n\u2205\n\n        \nu\n \n=\n \nDEQUEUE\n(\nQ\n)\n\n        \noutput\n \nu\n\n        \n// for loop executes O(E) times total.\n\n        \nfor\n \neach\n \nv\n \n\u2208\n \nG\n.\nAdj\n[\nu\n]\n\n            \nv\n.\nin\n-\ndegree\n \n=\n \nv\n.\nin\n-\ndegree\n \n-\n \n1\n\n            \nif\n \nv\n.\nin\n-\ndegree\n \n==\n \n0\n\n                \nENQUEUE\n(\nQ\n,\n \nv\n)\n\n    \n// Check for cycles, O(V) time.\n\n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nif\n \nu\n.\nin\n-\ndegree\n \n!=\n \n0\n\n            \nreport\n \nthat\n \nthere\n'\ns\n \na\n \ncycle\n\n    \n// Another way to check for cycles would be to count the vertices \n\n    \n// that are output and report a cycle if that number is < |V|.\n\n\n\n\n\nTo find and output vertices of $in\\text-degree$ $0$, we first compute all vertices' $in\\text-degree$s by making a pass through all the edges (by scanning the adjacency lists of all the vertices) and incrementing the $in\\text-degree$ of each vertex an edge enters.\n\n\n\n\nComputing all $in\\text-degree$s takes $\\Theta(V + E)$ time ($|V|$ adjacency lists accessed, $|E|$ edges total found in those lists, $\\Theta(1)$ work for each edge).\n\n\n\n\nWe keep the vertices with $in\\text-degree$ $0$ in a FIFO queue, so that they can be enqueued and dequeued in $O(1)$ time. (The order in which vertices in the queue are processed doesn't matter, so any kind of FIFO queue works.)\n\n\n\n\nInitializing the queue takes one pass over the vertices doing $\\Theta(1)$ work, for total time $\\Theta(V)$.\n\n\n\n\nAs we process each vertex from the queue, we effectively remove its outgoing edges from the graph by decrementing the $in\\text-degree$ of each vertex one of those edges enters, and we enqueue any vertex whose $in\\text-degree$ goes to $0$. We do not need to actually remove the edges from the adjacency list, because that adjacency list will never be processed again by the algorithm: Each vertex is enqueued/dequeued at most once because it is enqueued only if it starts out with $in\\text-degree$ $0$ or if its indegree becomes $0$ after being decremented (and never incremented) some number of times.\n\n\n\n\nThe processing of a vertex from the queue happens $O(V)$ times because no vertex can be enqueued more than once. The per-vertex work (dequeue and output) takes $O(1)$ time, for a total of $O(V)$ time.\n\n\nBecause the adjacency list of each vertex is scanned only when the vertex is dequeued, the adjacency list of each vertex is scanned at most once. Since the sum of the lengths of all the adjacency lists is $\\Theta(E)$, at most $O(E)$ time is spent in total scanning adjacency lists. For each edge in an adjacency list, $\\Theta(1)$ work is done, for a total of $O(E)$ time.\n\n\n\n\nThus the total time taken by the algorithm is $O(V + E)$.\n\n\nThe algorithm outputs vertices in the right order ($u$ before $v$ for every edge $(u, v)$) because vwill not be output until its $in\\text-degree$ becomes $0$, which happens only when every edge $(u, v)$ leading into $v$ has been ''removed'' due to the processing (including output) of $u$.\n\n\nIf there are no cycles, all vertices are output.\n\n\n\n\nProof: Assume that some vertex $v_0$ is not output. Vertex $v_0$ cannot start out with $in\\text-degree$ $0$ (or it would be output), so there are edges into $v_0$. Since $v_0$'s $in\\text-degree$ never becomes $0$, at least one edge $(v_1, v_0)$ is never removed, which means that at least one other vertex $v_1$ was not output. Similarly, $v_1$ not output means that some vertex $v_2$ such that $(v_2, v_1) \\in E$ was not output, and so on. Since the number of vertices is finite, this path ($\\cdots \\to v_2 \\to v_1 \\to v_0$) is finite, so we must have $v_i = v_j$ for some $i$ and $j$ in this sequence, which means there is a cycle.\n\n\n\n\nIf there are cycles, not all vertices will be output, because some $in\\text-degree$s never become $0$.\n\n\n\n\nProof: Assume that a vertex in a cycle is output (its $in\\text-degree$ becomes $0$). Let $v$ be the first vertex in its cycle to be output, and let $u$ be $v$'s predecessor in the cycle. In order for $v$'s $in\\text-degree$ to become $0$, the edge $(u, v)$ must have been ''removed,'' which happens only when $u$ is processed. But this cannot have happened, because $v$ is the first vertex in its cycle to be processed. Thus no vertices in cycles are output.",
            "title": "22.4 Topological sort"
        },
        {
            "location": "/Chap22/22.4/#224-1",
            "text": "Show the ordering of vertices produced by $\\text{TOPOLOGICAL-SORT}$ when it is run on the dag of Figure 22.8, under the assumption of Exercise 22.3-2.   Our start and finish times from performing the $\\text{DFS}$ are  \\begin{array}{ccc}\n\\text{label} & d & f \\\\\n\\hline\nm & 1  & 20 \\\\\nq & 2  & 5  \\\\\nt & 3  & 4  \\\\\nr & 6  & 19 \\\\\nu & 7  & 8  \\\\\ny & 9  & 18 \\\\\nv & 10 & 17 \\\\\nw & 11 & 14 \\\\\nz & 12 & 13 \\\\\nx & 15 & 16 \\\\\nn & 21 & 26 \\\\\no & 22 & 25 \\\\\ns & 24 & 24 \\\\\np & 27 & 28\n\\end{array}  And so, by reading off the entries in decreasing order of finish time, we have the sequence $p, n, o, s, m, r, y, v, x, w, z, u, q, t$.",
            "title": "22.4-1"
        },
        {
            "location": "/Chap22/22.4/#224-2",
            "text": "Give a linear-time algorithm that takes as input a directed acyclic graph $G = (V, E)$ and two vertices $s$ and $t$, and returns the number of simple paths from $s$ to $t$ in $G$. For example, the directed acyclic graph of Figure 22.8 contains exactly four simple paths from vertex $p$ to vertex $v: pov$, $poryv$, $posryv$, and $psryv$. (Your algorithm needs only to count the simple paths, not list them.)   The algorithm works as follows. The attribute $u.paths$ of node $u$ tells the number of simple paths from $u$ to $v$, where we assume that $v$ is fixed throughout the entire process. To count the number of paths, we can sum the number of paths which leave from each of $u$'s neighbors. Since we have no cycles, we will never risk adding a partially completed number of paths. Moreover, we can never consider the same edge twice among the recursive calls. Therefore, the total number of executions of the for-loop over all recursive calls is $O(V + E)$. Calling $\\text{SIMPLE-PATHS}(s, t)$ yields the desired result.  SIMPLE - PATHS ( u ,   v ) \n     if   u   ==   v \n         return   1 \n     else   if   u . paths   !=   NIL \n         return   u . paths \n     else \n         for   each   w   in   Adj [ u ] \n             u . pahts   =   u . paths   +   SIMPLE - PATHS ( w ,   v ) \n         return   u . paths",
            "title": "22.4-2"
        },
        {
            "location": "/Chap22/22.4/#224-3",
            "text": "Give an algorithm that determines whether or not a given undirected graph $G = (V, E)$ contains a cycle. Your algorithm should run in $O(V)$ time, independent of $|E|$.   An undirected graph is acyclic (i.e., a forest) if and only if a $\\text{DFS}$ yields no back edges.   If there's a back edge, there's a cycle.  If there's no back edge, then by Theorem 22.10, there are only tree edges. Hence, the graph is acyclic.   Thus, we can run $\\text{DFS}$: if we find a back edge, there's a cycle.   Time: $O(V)$. (Not $O(V + E)$!)   If we ever see $|V|$ distinct edges, we must have seen a back edge because (by Theorem B.2 on p. 1174) in an acyclic (undirected) forest, $|E| \\le |V| - 1$.",
            "title": "22.4-3"
        },
        {
            "location": "/Chap22/22.4/#224-4",
            "text": "Prove or disprove: If a directed graph $G$ contains cycles, then $\\text{TOPOLOGICAL-SORT}(G)$ produces a vertex ordering that minimizes the number of ''bad'' edges that are inconsistent with the ordering produced.   This is not true. Consider the graph $G$ consisting of vertices $a, b, c$, and $d$. Let the edges be $(a, b)$, $(b, c)$, $(a, d)$, $(d, c)$, and $(c, a)$. Suppose that we start the $\\text{DFS}$ of $\\text{TOPOLOGICAL-SORT}$ at vertex $c$. Assuming that $b$ appears before $d$ in the adjacency list of $a$, the order, from latest to earliest, of finish times is $c, a, d, b$.   The ''bad'' edges in this case are $(b, c)$ and $(d, c)$. However, if we had instead ordered them by $a, b, d, c$ then the only bad edges would be $(c, a)$. Thus $\\text{TOPOLOGICAL-SORT}$ doesn't always minimizes the number of ''bad'' edges",
            "title": "22.4-4"
        },
        {
            "location": "/Chap22/22.4/#224-5",
            "text": "Another way to perform topological sorting on a directed acyclic graph $G = (V, E)$ is to repeatedly find a vertex of $in\\text-degree$ $0$, output it, and remove it and all of its outgoing edges from the graph. Explain how to implement this idea so that it runs in time $O(V + E)$. What happens to this algorithm if $G$ has cycles?   TOPOLOGICAL - SORT ( G ) \n     // Initialize in-degree, \u0398(V) time. \n     for   each   vertex   u   \u2208   G . V \n         u . in - degree   =   0 \n     // Compute in-degree, \u0398(V + E) time. \n     for   each   vertex   u   \u2208   G . V \n         for   each   v   \u2208   G . Adj [ u ] \n             v . in - degree   =   v . in - degree   +   1 \n     // Initialize Queue, \u0398(V) time. \n     Q   =   \u2205 \n     for   each   vertex   u   \u2208   G . V \n         if   u . in - degree   ==   0 \n             ENQUEUE ( Q ,   u ) \n     // while loop takes O(V + E) time. \n     while   Q   !=   \u2205 \n         u   =   DEQUEUE ( Q ) \n         output   u \n         // for loop executes O(E) times total. \n         for   each   v   \u2208   G . Adj [ u ] \n             v . in - degree   =   v . in - degree   -   1 \n             if   v . in - degree   ==   0 \n                 ENQUEUE ( Q ,   v ) \n     // Check for cycles, O(V) time. \n     for   each   vertex   u   \u2208   G . V \n         if   u . in - degree   !=   0 \n             report   that   there ' s   a   cycle \n     // Another way to check for cycles would be to count the vertices  \n     // that are output and report a cycle if that number is < |V|.   To find and output vertices of $in\\text-degree$ $0$, we first compute all vertices' $in\\text-degree$s by making a pass through all the edges (by scanning the adjacency lists of all the vertices) and incrementing the $in\\text-degree$ of each vertex an edge enters.   Computing all $in\\text-degree$s takes $\\Theta(V + E)$ time ($|V|$ adjacency lists accessed, $|E|$ edges total found in those lists, $\\Theta(1)$ work for each edge).   We keep the vertices with $in\\text-degree$ $0$ in a FIFO queue, so that they can be enqueued and dequeued in $O(1)$ time. (The order in which vertices in the queue are processed doesn't matter, so any kind of FIFO queue works.)   Initializing the queue takes one pass over the vertices doing $\\Theta(1)$ work, for total time $\\Theta(V)$.   As we process each vertex from the queue, we effectively remove its outgoing edges from the graph by decrementing the $in\\text-degree$ of each vertex one of those edges enters, and we enqueue any vertex whose $in\\text-degree$ goes to $0$. We do not need to actually remove the edges from the adjacency list, because that adjacency list will never be processed again by the algorithm: Each vertex is enqueued/dequeued at most once because it is enqueued only if it starts out with $in\\text-degree$ $0$ or if its indegree becomes $0$ after being decremented (and never incremented) some number of times.   The processing of a vertex from the queue happens $O(V)$ times because no vertex can be enqueued more than once. The per-vertex work (dequeue and output) takes $O(1)$ time, for a total of $O(V)$ time.  Because the adjacency list of each vertex is scanned only when the vertex is dequeued, the adjacency list of each vertex is scanned at most once. Since the sum of the lengths of all the adjacency lists is $\\Theta(E)$, at most $O(E)$ time is spent in total scanning adjacency lists. For each edge in an adjacency list, $\\Theta(1)$ work is done, for a total of $O(E)$ time.   Thus the total time taken by the algorithm is $O(V + E)$.  The algorithm outputs vertices in the right order ($u$ before $v$ for every edge $(u, v)$) because vwill not be output until its $in\\text-degree$ becomes $0$, which happens only when every edge $(u, v)$ leading into $v$ has been ''removed'' due to the processing (including output) of $u$.  If there are no cycles, all vertices are output.   Proof: Assume that some vertex $v_0$ is not output. Vertex $v_0$ cannot start out with $in\\text-degree$ $0$ (or it would be output), so there are edges into $v_0$. Since $v_0$'s $in\\text-degree$ never becomes $0$, at least one edge $(v_1, v_0)$ is never removed, which means that at least one other vertex $v_1$ was not output. Similarly, $v_1$ not output means that some vertex $v_2$ such that $(v_2, v_1) \\in E$ was not output, and so on. Since the number of vertices is finite, this path ($\\cdots \\to v_2 \\to v_1 \\to v_0$) is finite, so we must have $v_i = v_j$ for some $i$ and $j$ in this sequence, which means there is a cycle.   If there are cycles, not all vertices will be output, because some $in\\text-degree$s never become $0$.   Proof: Assume that a vertex in a cycle is output (its $in\\text-degree$ becomes $0$). Let $v$ be the first vertex in its cycle to be output, and let $u$ be $v$'s predecessor in the cycle. In order for $v$'s $in\\text-degree$ to become $0$, the edge $(u, v)$ must have been ''removed,'' which happens only when $u$ is processed. But this cannot have happened, because $v$ is the first vertex in its cycle to be processed. Thus no vertices in cycles are output.",
            "title": "22.4-5"
        },
        {
            "location": "/Chap22/22.5/",
            "text": "22.5-1\n\n\n\n\nHow can the number of strongly connected components of a graph change if a new edge is added?\n\n\n\n\nIt can either stay the same or decrease. To see that it is possible to stay the same, just suppose you add some edge to a cycle. To see that it is possible to decrease, suppose that your original graph is on three vertices, and is just a path passing through all of them, and the edge added completes this path to a cycle. To see that it cannot increase, notice that adding an edge cannot remove any path that existed before. \n\n\nSo, if $u$ and $v$ are in the same connected component in the original graph, then there are a path from one to the other, in both directions. Adding an edge wont disturb these two paths, so we know that $u$ and $v$ will still be in the same $\\text{SCC}$ in the graph after adding the edge. Since no components can be split apart, this means that the number of them cannot increase since they form a partition of the set of vertices.\n\n\n22.5-2\n\n\n\n\nShow how the procedure $\\text{STRONGLY-CONNECTED-COMPONENTS}$ works on the graph of Figure 22.6. Specifically, show the finishing times computed in line 1 and the forest produced in line 3. Assume that the loop of lines 5\u20137 of $\\text{DFS}$ considers vertices in alphabetical order and that the adjacency lists are in alphabetical order.\n\n\n\n\nThe finishing times of each vertex were computed in exercise 22.3-2. The forest consists of 5 trees, each of which is a chain. We'll list the vertices of each tree in order from root to leaf: $r$, $u$, $q - y - t$, $x - z$, and $s - w - v$.\n\n\n22.5-3\n\n\n\n\nProfessor Bacon claims that the algorithm for strongly connected components would be simpler if it used the original (instead of the transpose) graph in the second depth-first search and scanned the vertices in order of \nincreasing\n finishing times. Does this simpler algorithm always produce correct results?\n\n\n\n\nProfessor Bacon's suggestion doesn't work out. As an example, suppose that our graph is on the three vertices $\\{1, 2, 3\\}$ and consists of the edges $(2, 1), (2, 3), (3, 2)$. Then, we should end up with $\\{2, 3\\}$ and $\\{1\\}$ as our $\\text{SCC}$'s. However, a possible $\\text{DFS}$ starting at $2$ could explore $3$ before $1$, this would mean that the finish time of $3$ is lower than of $1$ and $2$. This means that when we first perform the $\\text{DFS}$ starting at $3$. However, a $\\text{DFS}$ starting at $3$ will be able to reach all other vertices. This means that the algorithm would return that the entire graph is a single $\\text{SCC}$, even though this is clearly not the case since there is neither a path from $1$ to $2$ of from $1$ to $3$.\n\n\n22.5-4\n\n\n\n\nProve that for any directed graph $G$, we have $((G^\\text T)^{\\text{SCC}})^\\text T = G^{\\text{SCC}}$. That is, the transpose of the component graph of $G^\\text T$ is the same as the component graph of $G$.\n\n\n\n\nFirst observe that $C$ is a strongly connected component of $G$ if and only if it is a strongly connected component of $G^\\text T$. Thus the vertex sets of $G^{\\text{SCC}}$ and $(G^\\text T)^{\\text{SCC}}$ are the same, which implies the vertex sets of $((G^\\text T)^\\text{SCC})^\\text T$ and $G^{\\text{SCC}}$ are the same. It suffices to show that their edge sets are the same. Suppose $(v_i, v_j)$ is an edge in $((G^\\text T)^{\\text{SCC}})^\\text T$. Then $(v_j, v_i)$ is an edge in $(G^\\text T)^{\\text{SCC}}$. Thus there exist $x \\in C_j$ and $y \\in C_i$ such that $(x, y)$ is an edge of $G^\\text T$, which implies $(y, x)$ is an edge of $G$. Since components are preserved, this means that $(v_i, v_j)$ is an edge in $G^{\\text{SCC}}$. For the opposite implication we simply note that for any graph $G$ we have $(G^\\text T)^{\\text T} = G$.\n\n\n22.5-5\n\n\n\n\nGive an $O(V + E)$-time algorithm to compute the component graph of a directed graph $G = (V, E)$. Make sure that there is at most one edge between two vertices in the component graph your algorithm produces.\n\n\n\n\nWe have at our disposal an $O(V + E)$-time algorithm that computes strongly connected components. Let us assume that the output of this algorithm is a mapping $u.scc$, giving the number of the strongly connected component containing vertex $u$, for each vertex $u$. Without loss of generality, assume that $u.scc$ is an integer in the set $\\{1, 2, \\ldots, |V|\\}$.\n\n\nConstruct the multiset (a set that can contain the same object more than once) $T = \\{u.scc: u \\in V\\}$, and sort it by using counting sort. Since the values we are sorting are integers in the range $1$ to $|V|$, the time to sort is $O(V)$. Go through the sorted multiset $T$ and every time we find an element $x$ that is distinct from the one before it, add $x$ to $V^{\\text{SCC}}$. (Consider the first element of the sorted set as ''distinct from the one before it.'') It takes $O(V)$ time to construct $V^{\\text{SCC}}$.\n\n\nConstruct the set of ordered pairs\n\n\n$$\\text{$(x, y)$: there is an edge $(u, v) \\in E$, $x = u.scc$, and $y = v.scc$}.$$\n\n\nWe can easily construct this set in $\\Theta(E)$ time by going through all edges in $E$ and looking up $u.scc$ and $v.scc$ for each edge $(u, v) \\in E$.\n\n\nHaving constructed $S$, remove all elements of the form $(x, x)$. Alternatively, when we construct $S$, do not put an element in $S$ when we find an edge $(u, v)$ for which $u.scc = v.scc$. $S$ now has at most $|E|$ elements.\n\n\nNow sort the elements of $S$ using radix sort. Sort on one component at a time. The order does not matter. In other words, we are performing two passes of counting sort. The time to do so is $O(V + E)$, since the values we are sorting on are integers in the range $1$ to $|V|$.\n\n\nFinally, go through the sorted set $S$, and every time we find an element $(x, y)$ that is distinct from the element before it (again considering the first element of the sorted set as distinct from the one before it), add $(x, y)$ to $E^{\\text{SCC}}$. Sorting and then adding $(x, y)$ only if it is distinct from the element before it ensures that we add $(x, y)$ at most once. It takes $O(E)$ time to go through $S$ in this way, once $S$ has been sorted.\n\n\nThe total time is $O(V + E)$.\n\n\n22.5-6\n\n\n\n\nGiven a directed graph $G = (V, E)$, explain how to create another graph $G' = (V, E')$ such that (a) $G'$ has the same strongly connected components as $G$, (b) $G'$ has the same component graph as $G$, and (c) $E'$ is as small as possible. Describe a fast algorithm to compute $G'$.\n\n\n\n\nThe basic idea is to replace the edges within each $\\text{SCC}$ by one simple, directed cycle and then remove redundant edges between $\\text{SCC}$'s. Since there must be at least $k$ edges within an $\\text{SCC}$ that has $k$ vertices, a single directed cycle of $k$ edges gives the $k$-vertex $\\text{SCC}$ with the fewest possible edges.\n\n\nThe algorithm works as follows:\n\n\n\n\nIdentify all $\\text{SCC}$'s of $G$. Time: $\\Theta(V + E)$, using the $\\text{SCC}$ algorithm in Section 22.5.\n\n\nForm the component graph $G^{\\text{SCC}}$. Time: $O(V + E)$, by Exercise 22.5-5.\n\n\nStart with $E' = \\emptyset$. Time: $O(1)$.\n\n\nFor each $\\text{SCC}$ of $G$, let the vertices in the $\\text{SCC}$ be $v_1, v_2, \\ldots, v_k$, and add to $E'$ the directed edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k), (v_k, v_1)$. These edges form a simple, directed cycle that includes all vertices of the $\\text{SCC}$. Time for all $\\text{SCC}$'s: $O(V)$.\n\n\nFor each edge $(u, v)$ in the component graph $G^{\\text{SCC}}$, select any vertex $x$ in $u$'s $\\text{SCC}$ and any vertex $y$ in $v$'s $\\text{SCC}$, and add the directed edge $(x, y)$ to $E'$. Time: $O(E)$.\n\n\n\n\n22.5-7\n\n\n\n\nA directed graph $G = (V, E)$ is \nsemiconnected\n if, for all pairs of vertices $u, v \\in V$, we have $u \\leadsto v$ or $v \\leadsto u$. Give an efficient algorithm to determine whether or not $G$ is semiconnected. Prove that your algorithm is correct, and analyze its running time.\n\n\n\n\nTo determine whether $G = (V, E)$ is semiconnected, do the following:\n\n\n\n\nCall $\\text{STRONGLY-CONNECTED-COMPONENTS}$.\n\n\nForm the component graph. (By Exercise 22.5-5, you may assume that this takes $O(V + E)$ time.)\n\n\nTopologically sort the component graph. (Recall that it's a dag.) Assuming that $G$ contains $k$ $\\text{SCC}$'s, the topological sort gives a linear ordering $\\langle v_1, v_2, \\ldots, v_k \\rangle$ of the vertices.\n\n\nVerify that the sequence of vertices $\\langle v_1, v_2, \\ldots, v_k \\rangle$ given by topological sort forms a linear chain in the component graph. That is, verify that the edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k)$ exist in the component graph. If the vertices form a linear chain, then the original graph is semiconnected; otherwise it is not.\n\n\n\n\nBecause we know that all vertices in each $\\text{SCC}$ are mutually reachable from each other, it suffices to show that the component graph is semiconnected if and only if it contains a linear chain. We must also show that if there's a linear chain in the component graph, it's the one returned by topological sort.\n\n\nWe'll first show that if there's a linear chain in the component graph, then it's the one returned by topological sort. In fact, this is trivial. A topological sort has to respect every edge in the graph. So if there's a linear chain, a topological sort \nmust\n give us the vertices in order.\n\n\nNow we'll show that the component graph is semiconnected if and only if it contains a linear chain.\n\n\nFirst, suppose that the component graph contains a linear chain. Then for every pair of vertices $u$, $v$ in the component graph, there is a path between them. If $u$ precedes $v$ in the linear chain, then there's a path $u \\leadsto v$. Otherwise, $v$ precedes $u$, and there's a path $v \\leadsto u$.\n\n\nConversely, suppose that the component graph does not contain a linear chain. Then in the list returned by topological sort, there are two consecutive vertices $v_i$ and $v_{i + 1}$, but the edge$(v_i, v_{i + 1})$ is not in the component graph. Any edges out of $v_i$ are to vertices $v_j$, where $j > i + 1$, and so there is no path from $v_i$ to $v_{i + 1}$ in the component graph. And since $v_{i + 1}$ follows $v_i$ in the topological sort, there cannot be any paths at all from $v_{i + 1}$ to $v_i$. Thus, the component graph is not semiconnected.\n\n\nRunning time of each step:\n\n\n\n\n$\\Theta(V + E)$.\n\n\n$O(V + E)$.\n\n\nSince the component graph has at most $|V|$ vertices and at most $|E|$ edges, $O(V + E)$.\n\n\nAlso $O(V + E)$. We just check the adjacency list of each vertex $v_i$ in the component graph to verify that there's an edge $(v_i, v_{i + 1})$. We'll go through each adjacency list once.\n\n\n\n\nThus, the total running time is $\\Theta(V + E)$.",
            "title": "22.5 Strongly connected components"
        },
        {
            "location": "/Chap22/22.5/#225-1",
            "text": "How can the number of strongly connected components of a graph change if a new edge is added?   It can either stay the same or decrease. To see that it is possible to stay the same, just suppose you add some edge to a cycle. To see that it is possible to decrease, suppose that your original graph is on three vertices, and is just a path passing through all of them, and the edge added completes this path to a cycle. To see that it cannot increase, notice that adding an edge cannot remove any path that existed before.   So, if $u$ and $v$ are in the same connected component in the original graph, then there are a path from one to the other, in both directions. Adding an edge wont disturb these two paths, so we know that $u$ and $v$ will still be in the same $\\text{SCC}$ in the graph after adding the edge. Since no components can be split apart, this means that the number of them cannot increase since they form a partition of the set of vertices.",
            "title": "22.5-1"
        },
        {
            "location": "/Chap22/22.5/#225-2",
            "text": "Show how the procedure $\\text{STRONGLY-CONNECTED-COMPONENTS}$ works on the graph of Figure 22.6. Specifically, show the finishing times computed in line 1 and the forest produced in line 3. Assume that the loop of lines 5\u20137 of $\\text{DFS}$ considers vertices in alphabetical order and that the adjacency lists are in alphabetical order.   The finishing times of each vertex were computed in exercise 22.3-2. The forest consists of 5 trees, each of which is a chain. We'll list the vertices of each tree in order from root to leaf: $r$, $u$, $q - y - t$, $x - z$, and $s - w - v$.",
            "title": "22.5-2"
        },
        {
            "location": "/Chap22/22.5/#225-3",
            "text": "Professor Bacon claims that the algorithm for strongly connected components would be simpler if it used the original (instead of the transpose) graph in the second depth-first search and scanned the vertices in order of  increasing  finishing times. Does this simpler algorithm always produce correct results?   Professor Bacon's suggestion doesn't work out. As an example, suppose that our graph is on the three vertices $\\{1, 2, 3\\}$ and consists of the edges $(2, 1), (2, 3), (3, 2)$. Then, we should end up with $\\{2, 3\\}$ and $\\{1\\}$ as our $\\text{SCC}$'s. However, a possible $\\text{DFS}$ starting at $2$ could explore $3$ before $1$, this would mean that the finish time of $3$ is lower than of $1$ and $2$. This means that when we first perform the $\\text{DFS}$ starting at $3$. However, a $\\text{DFS}$ starting at $3$ will be able to reach all other vertices. This means that the algorithm would return that the entire graph is a single $\\text{SCC}$, even though this is clearly not the case since there is neither a path from $1$ to $2$ of from $1$ to $3$.",
            "title": "22.5-3"
        },
        {
            "location": "/Chap22/22.5/#225-4",
            "text": "Prove that for any directed graph $G$, we have $((G^\\text T)^{\\text{SCC}})^\\text T = G^{\\text{SCC}}$. That is, the transpose of the component graph of $G^\\text T$ is the same as the component graph of $G$.   First observe that $C$ is a strongly connected component of $G$ if and only if it is a strongly connected component of $G^\\text T$. Thus the vertex sets of $G^{\\text{SCC}}$ and $(G^\\text T)^{\\text{SCC}}$ are the same, which implies the vertex sets of $((G^\\text T)^\\text{SCC})^\\text T$ and $G^{\\text{SCC}}$ are the same. It suffices to show that their edge sets are the same. Suppose $(v_i, v_j)$ is an edge in $((G^\\text T)^{\\text{SCC}})^\\text T$. Then $(v_j, v_i)$ is an edge in $(G^\\text T)^{\\text{SCC}}$. Thus there exist $x \\in C_j$ and $y \\in C_i$ such that $(x, y)$ is an edge of $G^\\text T$, which implies $(y, x)$ is an edge of $G$. Since components are preserved, this means that $(v_i, v_j)$ is an edge in $G^{\\text{SCC}}$. For the opposite implication we simply note that for any graph $G$ we have $(G^\\text T)^{\\text T} = G$.",
            "title": "22.5-4"
        },
        {
            "location": "/Chap22/22.5/#225-5",
            "text": "Give an $O(V + E)$-time algorithm to compute the component graph of a directed graph $G = (V, E)$. Make sure that there is at most one edge between two vertices in the component graph your algorithm produces.   We have at our disposal an $O(V + E)$-time algorithm that computes strongly connected components. Let us assume that the output of this algorithm is a mapping $u.scc$, giving the number of the strongly connected component containing vertex $u$, for each vertex $u$. Without loss of generality, assume that $u.scc$ is an integer in the set $\\{1, 2, \\ldots, |V|\\}$.  Construct the multiset (a set that can contain the same object more than once) $T = \\{u.scc: u \\in V\\}$, and sort it by using counting sort. Since the values we are sorting are integers in the range $1$ to $|V|$, the time to sort is $O(V)$. Go through the sorted multiset $T$ and every time we find an element $x$ that is distinct from the one before it, add $x$ to $V^{\\text{SCC}}$. (Consider the first element of the sorted set as ''distinct from the one before it.'') It takes $O(V)$ time to construct $V^{\\text{SCC}}$.  Construct the set of ordered pairs  $$\\text{$(x, y)$: there is an edge $(u, v) \\in E$, $x = u.scc$, and $y = v.scc$}.$$  We can easily construct this set in $\\Theta(E)$ time by going through all edges in $E$ and looking up $u.scc$ and $v.scc$ for each edge $(u, v) \\in E$.  Having constructed $S$, remove all elements of the form $(x, x)$. Alternatively, when we construct $S$, do not put an element in $S$ when we find an edge $(u, v)$ for which $u.scc = v.scc$. $S$ now has at most $|E|$ elements.  Now sort the elements of $S$ using radix sort. Sort on one component at a time. The order does not matter. In other words, we are performing two passes of counting sort. The time to do so is $O(V + E)$, since the values we are sorting on are integers in the range $1$ to $|V|$.  Finally, go through the sorted set $S$, and every time we find an element $(x, y)$ that is distinct from the element before it (again considering the first element of the sorted set as distinct from the one before it), add $(x, y)$ to $E^{\\text{SCC}}$. Sorting and then adding $(x, y)$ only if it is distinct from the element before it ensures that we add $(x, y)$ at most once. It takes $O(E)$ time to go through $S$ in this way, once $S$ has been sorted.  The total time is $O(V + E)$.",
            "title": "22.5-5"
        },
        {
            "location": "/Chap22/22.5/#225-6",
            "text": "Given a directed graph $G = (V, E)$, explain how to create another graph $G' = (V, E')$ such that (a) $G'$ has the same strongly connected components as $G$, (b) $G'$ has the same component graph as $G$, and (c) $E'$ is as small as possible. Describe a fast algorithm to compute $G'$.   The basic idea is to replace the edges within each $\\text{SCC}$ by one simple, directed cycle and then remove redundant edges between $\\text{SCC}$'s. Since there must be at least $k$ edges within an $\\text{SCC}$ that has $k$ vertices, a single directed cycle of $k$ edges gives the $k$-vertex $\\text{SCC}$ with the fewest possible edges.  The algorithm works as follows:   Identify all $\\text{SCC}$'s of $G$. Time: $\\Theta(V + E)$, using the $\\text{SCC}$ algorithm in Section 22.5.  Form the component graph $G^{\\text{SCC}}$. Time: $O(V + E)$, by Exercise 22.5-5.  Start with $E' = \\emptyset$. Time: $O(1)$.  For each $\\text{SCC}$ of $G$, let the vertices in the $\\text{SCC}$ be $v_1, v_2, \\ldots, v_k$, and add to $E'$ the directed edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k), (v_k, v_1)$. These edges form a simple, directed cycle that includes all vertices of the $\\text{SCC}$. Time for all $\\text{SCC}$'s: $O(V)$.  For each edge $(u, v)$ in the component graph $G^{\\text{SCC}}$, select any vertex $x$ in $u$'s $\\text{SCC}$ and any vertex $y$ in $v$'s $\\text{SCC}$, and add the directed edge $(x, y)$ to $E'$. Time: $O(E)$.",
            "title": "22.5-6"
        },
        {
            "location": "/Chap22/22.5/#225-7",
            "text": "A directed graph $G = (V, E)$ is  semiconnected  if, for all pairs of vertices $u, v \\in V$, we have $u \\leadsto v$ or $v \\leadsto u$. Give an efficient algorithm to determine whether or not $G$ is semiconnected. Prove that your algorithm is correct, and analyze its running time.   To determine whether $G = (V, E)$ is semiconnected, do the following:   Call $\\text{STRONGLY-CONNECTED-COMPONENTS}$.  Form the component graph. (By Exercise 22.5-5, you may assume that this takes $O(V + E)$ time.)  Topologically sort the component graph. (Recall that it's a dag.) Assuming that $G$ contains $k$ $\\text{SCC}$'s, the topological sort gives a linear ordering $\\langle v_1, v_2, \\ldots, v_k \\rangle$ of the vertices.  Verify that the sequence of vertices $\\langle v_1, v_2, \\ldots, v_k \\rangle$ given by topological sort forms a linear chain in the component graph. That is, verify that the edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k)$ exist in the component graph. If the vertices form a linear chain, then the original graph is semiconnected; otherwise it is not.   Because we know that all vertices in each $\\text{SCC}$ are mutually reachable from each other, it suffices to show that the component graph is semiconnected if and only if it contains a linear chain. We must also show that if there's a linear chain in the component graph, it's the one returned by topological sort.  We'll first show that if there's a linear chain in the component graph, then it's the one returned by topological sort. In fact, this is trivial. A topological sort has to respect every edge in the graph. So if there's a linear chain, a topological sort  must  give us the vertices in order.  Now we'll show that the component graph is semiconnected if and only if it contains a linear chain.  First, suppose that the component graph contains a linear chain. Then for every pair of vertices $u$, $v$ in the component graph, there is a path between them. If $u$ precedes $v$ in the linear chain, then there's a path $u \\leadsto v$. Otherwise, $v$ precedes $u$, and there's a path $v \\leadsto u$.  Conversely, suppose that the component graph does not contain a linear chain. Then in the list returned by topological sort, there are two consecutive vertices $v_i$ and $v_{i + 1}$, but the edge$(v_i, v_{i + 1})$ is not in the component graph. Any edges out of $v_i$ are to vertices $v_j$, where $j > i + 1$, and so there is no path from $v_i$ to $v_{i + 1}$ in the component graph. And since $v_{i + 1}$ follows $v_i$ in the topological sort, there cannot be any paths at all from $v_{i + 1}$ to $v_i$. Thus, the component graph is not semiconnected.  Running time of each step:   $\\Theta(V + E)$.  $O(V + E)$.  Since the component graph has at most $|V|$ vertices and at most $|E|$ edges, $O(V + E)$.  Also $O(V + E)$. We just check the adjacency list of each vertex $v_i$ in the component graph to verify that there's an edge $(v_i, v_{i + 1})$. We'll go through each adjacency list once.   Thus, the total running time is $\\Theta(V + E)$.",
            "title": "22.5-7"
        },
        {
            "location": "/Chap22/Problems/22-1/",
            "text": "A depth-first forest classifies the edges of a graph into tree, back, forward, and cross edges. A breadth-first tree can also be used to classify the edges reachable from the source of the search into the same four categories.\n\n\na.\n Prove that in a breadth-first search of an undirected graph, the following properties hold:\n\n\n\n\nThere are no back edges and no forward edges.\n\n\nFor each tree edge $(u, v)$, we have $v.d = u.d + 1$.\n\n\nFor each cross edge $(u, v)$, we have $v.d = u.d$ or $v.d = u.d + 1$. \n\n\n\n\nb.\n Prove that in a breadth-first search of a directed graph, the following properties hold:\n\n\n\n\nThere are no forward edges.\n\n\nFor each tree edge $(u, v)$, we have $v.d = u.d + 1$.\n\n\nFor each cross edge $(u, v)$, we have $v.d \\le u.d + 1$.\n\n\nFor each back edge $(u, v)$, we have $0 \\le v.d \\le u.d$.\n\n\n\n\n\n\na.\n \n\n\n\n\nSuppose $(u, v)$ is a back edge or a forward edge in a $\\text{BFS}$ of an undirected graph. Then one of $u$ and $v$, say $u$, is a proper ancestor of the other ($v$) in the breadth-first tree. Since we explore all edges of $u$ before exploring any edges of any of $u$'s descendants, we must explore the edge $(u, v)$ at the time we explore $u$. But then $(u, v)$ must be a tree edge.\n\n\nIn $\\text{BFS}$, an edge $(u, v)$ is a tree edge when we set $v.\\pi \\leftarrow u$. But we only do so when we set $v.d \\leftarrow u.d + 1$. Since neither $u.d$ nor $v.d$ ever changes thereafter, we have $v.d=u.d+1$ when $\\text{BFS}$ completes.\n\n\nConsider a cross edge $(u, v)$ where, without loss of generality, $u$ is visited before $v$. At the time we visit $u$, vertex $v$ must already be on the queue, for otherwise $(u, v)$ would be a tree edge. Because $v$ is on the queue, we have $v.d \\le u.d + 1$ by Lemma 22.3. By Corollary 22.4, we have $v.d \\ge u.d$. Thus, either $v.d = u.d$ or $v.d = u.d + 1$.\n\n\n\n\nb.\n\n\n\n\nSuppose $(u, v)$ is a forward edge. Then we would have explored it while visiting $u$, and it would have been a tree edge.\n\n\nSame as for undirected graphs.\n\n\nFor any edge $(u, v)$, whether or not it's a cross edge, we cannot have $v.d > u.d + 1$, since we visit $v$ at the latest when we explore edge $(u, v)$. Thus, $v.d \\le u.d + 1$.\n\n\nClearly, $v.d \\ge 0$ for all vertices $v$. For a back edge $(u, v)$, $v$ is an ancestor of $u$ in the breadth-first tree, which means that $v.d\\le u.d$. (Note that since self-loops are considered to be back edges, we could have $u = v$.)",
            "title": "22-1 Classifying edges by breadth-first search"
        },
        {
            "location": "/Chap22/Problems/22-2/",
            "text": "Let $G = (V, E)$ be a connected, undirected graph. An \narticulation point\n of $G$ is a vertex whose removal disconnects $G$. A \nbridge\n of $G$ is an edge whose removal disconnects $G$. A \nbiconnected component\n of $G$ is a maximal set of edges such that any two edges in the set lie on a common simple cycle. Figure 22.10 illustrates these definitions. We can determine articulation points, bridges, and biconnected components using depth-first search. Let $G_\\pi = (V, E_\\pi)$ be a depth-first tree of $G$.\n\n\na.\n Prove that the root of $G_\\pi$ is an articulation point of $G$ if and only if it has at least two children in $G_\\pi$.\n\n\nb.\n Let $v$ be a nonroot vertex of $G_\\pi$. Prove that $v$ is an articulation point of $G$ if and only if $v$ has a child $s$ such that there is no back edge from $s$ or any descendant of $s$ to a proper ancestor of $v$.\n\n\nc.\n Let\n\n\n$$\nv.low = \\min\n\\begin{cases}\nv.d, \\\\\nw.d:(u,w) \\text{ is a back edge for some descendant } u \\text{ of } v.\n\\end{cases}\n$$\n\n\nShow how to computer $v.low$ for all vertices $v \\in V$ in $O(E)$ time.\n\n\nd.\n Show how to compute all articulation points in $O(E)$ time.\n\n\ne.\n Prove that an edge of $G$ is a bridge if and only if it does not lie on any simple cycle of $G$.\n\n\nf.\n Show how to compute all the bridges of $G$ in $O(E)$ time.\n\n\ng.\n Prove that the biconnected components of $G$ partition the nonbridge edges of $G$.\n\n\nh.\n Give an $O(E)$-time algorithm to label each edge $e$ of $G$ with a positive integer $e.bcc$ such that $e.bcc = e'.bcc$ if and only if $e$ and $e'$ are in the same biconnected component.\n\n\n\n\na.\n First suppose the root $r$ of $G_\\pi$ is an articulation point. Then the removal of $r$ from $G$ would cause the graph to disconnect, so $r$ has at least $2$ children in $G$. If $r$ has only one child $v$ in $G_\\pi$ then it must be the case that there is a path from $v$ to each of $r$'s other children. Since removing $r$ disconnects the graph, there must exist vertices $u$ and $w$ such that the only paths from $u$ to $w$ contain $r$. \n\n\nTo reach $r$ from $u$, the path must first reach one of $r$'s children. This child is connect to $v$ via a path which doesn't contain $r$. \n\n\nTo reach $w$, the path must also leave $r$ through one of its children, which is also reachable by $v$. This implies that there is a path from $u$ to $w$ which doesn't contain $r$, a contradiction.\n\n\nNow suppose $r$ has at least two children $u$ and $v$ in $G_\\pi$. Then there is no path from $u$ to $v$ in $G$ which doesn't go through $r$, since otherwise $u$ would be an ancestor of $v$. Thus, removing $r$ disconnects the component containing $u$ and the component containing $v$, so $r$ is an articulation point.\n\n\nb.\n Suppose that $v$ is a nonroot vertex of $G_\\pi$ and that $v$ has a child $s$ such that neither $s$ nor any of $s$'s descendants have back edges to a proper ancestor of $v$. Let $r$ be an ancestor of $v$, and remove $v$ from $G$. Since we are in the undirected case, the only edges in the graph are tree edges or back edges, which means that every edge incident with $s$ takes us to a descendant of $s$, and no descendants have back edges, so at no point can we move up the tree by taking edges. Therefore $r$ is unreachable from $s$, so the graph is disconnected and $v$ is an articulation point.\n\n\nNow suppose that for every child of $v$ there exists a descendant of that child which has a back edge to a proper ancestor of $v$. Remove $v$ from $G$. Every subtree of $v$ is a connected component. Within a given subtree, find the vertex which has a back edge to a proper ancestor of $v$. Since the set $T$ of vertices which aren't descendants of $v$ form a connected component, we have that every subtree of $v$ is connected to $T$. Thus, the graph remains connected after the deletion of $v$ so $v$ is not an articulation point.\n\n\nc.\n Since $v$ is discovered before all of its descendants, the only back edges which could affect $v.low$ are ones which go from a descendant of $v$ to a proper ancestor of $v$. If we know $u.low$ for every child $u$ of $v$, then we can compute $v.low$ easily since all the information is coded in its descendants. \n\n\nThus, we can write the algorithm recursively: If $v$ is a leaf in $G_\\pi$ then $v.low$ is the minimum of $v.d$ and $w.d$ where $(v, w)$ is a back edge. If $v$ is not a leaf, $v$ is the minimum of $v.d$, $w.d$ where $w$ is a back edge, and $u.low$, where $u$ is a child of $v$. Computing $v.low$ for a vertex is linear in its degree. The sum of the vertices' degrees gives twice the number of edges, so the total runtime is $O(E)$.\n\n\nd.\n First apply the algorithm of part (c) in $O(E)$ to compute $v.low$ for all $v \\in V$. If $v.low$ = $v.d$ if and only if no descendant of $v$ has a back edge to a proper ancestor of $v$, if and only if $v$ is not an articulation point. \n\n\nThus, we need only check $v.low$ versus $v.d$ to decide in constant time whether or not $v$ is an articulation point, so the runtime is $O(E)$.\n\n\ne.\n An edge $(u, v)$ lies on a simple cycle if and only if there exists at least one path from $u$ to $v$ which doesn't contain the edge $(u, v)$, if and only if removing $(u, v)$ doesn't disconnect the graph, if and only if $(u, v)$ is not a bridge.\n\n\nf.\n A edge $(u, v)$ lies on a simple cycle in an undirected graph if and only if either both of its endpoints are articulation points, or one of its endpoints is an articulation point and the other is a vertex of degree $1$. Since we can compute all articulation points in $O(E)$ and we can decide whether or not a vertex has degree $1$ in constant time, we can run the algorithm in part (d) and then decide whether each edge is a bridge in constant time, so we can find all bridges in $O(E)$ time.\n\n\ng.\n It is clear that every nonbridge edge is in some biconnected component, so we need to show that if $C_1$ and $C_2$ are distinct biconnected components, then they contain no common edges. Suppose to the contrary that $(u, v)$ is in both $C_1$ and $C_2$. \n\n\nLet $(a, b)$ be any edge in $C_1$ and $(c, d)$ be any edge in $C_2$.\n\n\nThen $(a, b)$ lies on a simple cycle with $(u, v)$, consisting of the path\n\n\n$$a, b, p_1, \\ldots, p_k, u, v, p_{k + 1}, \\ldots, p_n, a.$$\n\n\nSimilarly, $(c, d)$ lies on a simple cycle with $(u, v)$ consisting of the path\n\n\n$$c, d, q_1, \\ldots, q_m, u, v, q_{m + 1}, \\ldots, q_l, c.$$\n\n\nThis means\n\n\n$$a, b, p_1, \\ldots, p_k, u, q_m, \\ldots, q_1, d, c, q_l , \\ldots, q_{m + 1}, v, p_{k + 1}, \\ldots, p_n,$$\n\n\nis a simple cycle containing $(a, b)$ and $(c, d)$, a contradiction. Thus, the biconnected components form a partition.\n\n\nh.\n Locate all bridge edges in $O(E)$ time using the algorithm described in part (f). Remove each bridge from $E$. The biconnected components are now simply the edges in the connected components. Assuming this has been done, run the following algorithm, which clearly runs in $O(|E|)$ where $|E|$ is the number of edges originally in $G$.",
            "title": "22-2 Articulation points, bridges, and biconnected components"
        },
        {
            "location": "/Chap22/Problems/22-3/",
            "text": "An \nEuler tour\n of a strongly connected, directed graph $G = (V, E)$ is a cycle that traverses each edge of $G$ exactly once, although it may visit a vertex more than once.\n\n\na.\n Show that $G$ has an Euler tour if and only if $in\\text-degree(v) = out\\text-degree(v)$ for each vertex $v \\in V$.\n\n\nb.\n Describe an $O(E)$-time algorithm to find an Euler tour of $G$ if one exists. ($\\textit{Hint:}$ Merge edge-disjoint cycles.)\n\n\n\n\na.\n An Euler tour is a single cycle that traverses each edge of $G$ exactly once, but it might not be a simple cycle. An Euler tour can be decomposed into a set of edge-disjoint simple cycles, however.\n\n\nIf $G$ has an Euler tour, therefore, we can look at the simple cycles that, together, form the tour. In each simple cycle, each vertex in the cycle has one entering edge and one leaving edge. In each simple cycle, therefore, each vertex $v$ has $in\\text-degree(v) = out\\text-degree(v)$, where the degrees are either $1$ (if $v$ is on the simple cycle) or $0$ (if $v$ is not on the simple cycle). Adding the in- and out- degrees over all edges proves that if $G$ has an Euler tour, then $in\\text-degree(v) = out\\text-degree(v)$ for all vertices $v$.\n\n\nWe prove the converse\u2014that if $in\\text-degree(v) = out\\text-degree(v)$ for all vertices $v$, then $G$ has an Euler tour\u2014in two different ways. One proof is nonconstructive, and the other proof will help us design the algorithm for part (b).\n\n\nFirst, we claim that if $in\\text-degree(v) = out\\text-degree(v)$ for all vertices $v$, then we can pick any vertex $u$ for which $in\\text-degree(u) = out\\text-degree(u) \\ge 1$ and create a cycle (not necessarily simple) that contains $u$. To prove this claim, let us start by placing vertex $u$ on the cycle, and choose any leaving edge of $u$, say ($u, v$). Now we put $v$ on the cycle. Since $in\\text-degree(v) = out\\text-degree(v) \\ge 1$, we can pick some leaving edge of $v$ and continue visiting edges and vertices. Each time we pick an edge, we can remove it from further consideration. At each vertex other than $u$, at the time we visit an entering edge, there must be an unvisited leaving edge, since $in\\text-degree(v) = out\\text-degree(v)$ for all vertices $v$. The only vertex for which there might not be an unvisited leaving edge is $u$, since we started the cycle by visiting one of $u$'s leaving edges. Since there's always a leaving edge we can visit from all vertices other than $u$, eventually the cycle must return to $u$, thus proving the claim.\n\n\nThe nonconstructive proof proves the contrapositive\u2014that if $G$ does not have an Euler tour, then $in\\text-degree(v) \\ne out\\text-degree(v)$ for some vertex $v$\u2014by contradiction. Choose a graph $G = (V, E)$ that does not have an Euler tour but has at least one edge and for which $in\\text-degree(v) = out\\text-degree(v)$ for all vertices $v$, and let $G$ have the fewest edges of any such graph. By the above claim, $G$ contains a cycle. Let $C$ be a cycle of $G$ with the greatest number of edges, and let $V_C$ be the set of vertices visited by cycle $C$. By our assumption, $C$ is not an Euler tour, and so the set of edges $E' = E - C$ is nonempty. If we use the set $V$ of vertices and the set $E'$ of edges, we get the graph $G' = (V, E')$; this graph has $in\\text-degree(v) = out\\text-degree(v)$ for all vertices $v$, since we have removed one entering edge and one leaving edge for each vertex on cycle $C$. Consider any component $G'' = (V'' , E'')$ of $G'$, and observe that $G''$ also has $in\\text-degree(v) = out\\text-degree(v)$ for all vertices $v$. Since $E'' \\subseteq E' \\subsetneq E$, it follows from how we chose $G$ that $G''$  must have an Euler tour, say $C'$. Because the original graph G is connected, there must be some vertex $x \\in V'' \\cup V_C$ and, without loss of generality, consider $x$ to be the first and last vertex on both $C$ and $C'$. But then the cycle $C''$ formed by first traversing $C$ and then traversing $C'$ is a cycle of $G$ with more edges than $C$, contradicting our choice of $C$. We conclude that $C$ must have been an Euler tour.\n\n\nThe constructive proof uses the same ideas. Let us start at a vertex $u$ and, via random traversal of edges, create a cycle. We know that once we take any edge entering a vertex $v \\ne u$, we can find an edge leaving $v$ that we have not yet taken. Eventually, we get back to vertex $u$, and if there are still edges leaving $u$ that we have not taken, we can continue the cycle. Eventually, we get back to vertex $u$ and there are no untaken edges leaving $u$. If we have visited every edge in the graph $G$, we are done. Otherwise, since $G$ is connected, there must be some unvisited edge leaving a vertex, say $v$, on the cycle. We can traverse a new cycle starting at $v$, visiting only previously unvisited edges, and we can splice this cycle into the cycle we already know. That is, if the original cycle is $\\langle u, \\ldots, v, w, \\ldots, u \\rangle$, and the new cycle is $\\langle v, x, \\ldots, v\\rangle$, then we can create the cycle $\\langle u, \\ldots, v, x, \\ldots, v, w, \\ldots, u \\rangle$. We continue this process of finding a vertex with an unvisited leaving edge on a visited cycle, visiting a cycle starting and ending at this vertex, and splicing in the newly visited cycle, until we have visited every edge.\n\n\nb.\n The algorithm is based on the idea in the constructive proof above.\n\n\nWe assume that $G$ is represented by adjacency lists, and we work with a copy of the adjacency lists, so that as we visit each edge, we can remove it from its adjacency list. The singly linked form of adjacency list will suffice. The output of this algorithm is a doubly linked list $T$ of vertices which, read in list order, will give an Euler tour. The algorithm constructs $T$ by finding cycles (also represented by doubly linked lists) and splicing them into $T$. By using doubly linked lists for cycles and the Euler tour, splicing a cycle into the Euler tour takes constant time.\n\n\nWe also maintain a singly linked list $L$, in which each list element consists of two parts:\n\n\n\n\na vertex $v$, and\n\n\na pointer to some appearance of $v$ in $T$.\n\n\n\n\nInitially, $L$ contains one vertex, which may be any vertex of $G$.\n\n\nHere is the algorithm.\n\n\nEULER\n-\nTOUR\n(\nG\n)\n\n    \nT\n \n=\n \nempty\n \nlist\n\n    \nL\n \n=\n \n(\nany\n \nvertex\n \nv\n \n\u2208\n \nG\n.\nV\n,\n \nNIL\n)\n\n    \nwhile\n \nL\n \nis\n \nnot\n \nempty\n\n        \nremore\n \n(\nv\n,\n \nlocation\n \n-\n \nin\n-\nT\n)\n \nfrom\n \nL\n\n        \nC\n \n=\n \nVISIT\n(\nG\n,\n \nL\n,\n \nv\n)\n\n        \nif\n \nlocation\n \n-\n \nin\n-\nT\n \n==\n \nNIL\n\n            \nT\n \n=\n \nC\n\n        \nelse\n \nsplice\n \nC\n \ninto\n \nT\n \njust\n \nbefore\n \nlocation\n \n-\n \nin\n-\nT\n\n    \nreturn\n \nT\n\n\n\n\n\nVISIT\n(\nG\n,\n \nL\n,\n \nv\n)\n\n    \nC\n \n=\n \nempty\n \nsequence\n \nof\n \nvertices\n\n    \nu\n \n=\n \nv\n\n    \nwhile\n \nout\n-\ndegree\n(\nu\n)\n \n>\n \n0\n\n        \nlet\n \nw\n \nbe\n \nthe\n \nfirst\n \nvertex\n \nin\n \nG\n.\nAdj\n[\nu\n]\n\n        \nremove\n \nw\n \nfrom\n \nG\n.\nAdj\n[\nu\n],\n \ndecrementing\n \nout\n-\ndegree\n(\nu\n)\n\n        \nadd\n \nu\n \nonto\n \nthe\n \nend\n \nof\n \nC\n\n        \nif\n \nout\n-\ndegree\n(\nu\n)\n \n>\n \n0\n\n            \nadd\n \n(\nu\n,\n \nu\n'\ns\n \nlocation\n \nin\n \nC\n)\n \nto\n \nL\n\n        \nu\n \n=\n \nw\n\n    \nreturn\n \nC\n\n\n\n\n\nThe use of $\\text{NIL}$ in the initial assignment to $L$ ensures that the first cycle $C$ returned by $\\text{VISIT}$ becomes the current version of the Euler tour $T$. All cycles returned by $\\text{VISIT}$ thereafter are spliced into $T$. We assume that whenever an empty cycle is returned by $\\text{VISIT}$, splicing it into $T$ leaves $T$ unchanged.\n\n\nEach time that $\\text{EULER-TOUR}$ removes a vertex $v$ from the list $L$, it calls $\\text{VISIT}(G, L, v)$ to find a cycle $C$, possibly empty and possibly not simple, that starts and ends at $v$; the cycle $C$ is represented by a list that starts with $v$ and ends with the last vertex on the cycle before the cycle ends at $v$. $\\text{EULER-TOUR}$ then splices this cycle $C$ into the Euler tour $T$ just before some appearance of $v$ in $T$.\n\n\nWhen $\\text{VISIT}$ is at a vertex $u$, it looks for some vertex $w$ such that the edge $(u, w)$ has not yet been visited. Removing $w$ from $Adj[u]$\u008d ensures that we will never visit $(u, w)$ again. $\\text{VISIT}$ adds $u$ onto the cycle $C$ that it constructs. If, after removing edge $(u, w)$, vertex $u$ still has any leaving edges, then $u$, along with its location in $C$, is added to $L$. The cycle construction continues from $w$, and it ceases once a vertex with no unvisited leaving edges is found. Using the argument from part (a), at that point, this vertex must close up a cycle. At that point, therefore, the cycle $C$ is returned.\n\n\nIt is possible that a vertex $u$ has unvisited leaving edges at the time it is added to list $L$ in $\\text{VISIT}$, but that by the time that $u$ is removed from $L$ in $\\text{EULER-TOUR}$, all of its leaving edges have been visited. In this case, the \nwhile\n loop of $\\text{VISIT}$ executes $0$ iterations, and $\\text{VISIT}$ returns an empty cycle.\n\n\nOnce the list $L$ is empty, every edge has been visited. The resulting cycle $T$ is then an Euler tour.\n\n\nTo see that $\\text{EULER-TOUR}$ takes $O(E)$ time, observe that because we remove each edge from its adjacency list as it is visited, no edge is visited more than once. Since each edge is visited at some time, the number of times that a vertex is added to $L$, and thus removed from $L$, is at most $|E|$. Thus, the \nwhile\n loop in $\\text{EULER-TOUR}$ executes at most $E$ iterations. The \nwhile\n loop in $\\text{VISIT}$ executes one iteration per edge in the graph, and so it executes at most $E$ iterations as well. Since adding vertex $u$ to the doubly linked list $C$ takes constant time and splicing $C$ into $T$ takes constant time, the entire algorithm takes $O(E)$ time.",
            "title": "22-3 Euler tour"
        },
        {
            "location": "/Chap22/Problems/22-4/",
            "text": "Let $G = (V, E)$ be a directed graph in which each vertex $u \\in V$ is labeled with a unique integer $L(U)$ from the set $\\{1, 2, \\ldots, |V|\\}$. For each vertex $u \\in V$, let $R(u) = \\{v \\in V: u \\leadsto v \\}$ be the set of vertices that are reachable from $u$. Define $\\min(u)$ to be the vertex in $R(u)$ whose label is minimum, i.e., $\\min(u)$ is the vertex $v$ such that $L(v) = \\min \\{L(w): w \\in R(u) \\}$. Give an $O(V + E)$-time algorithm that computes $\\min(u)$ for all vertices $u \\in V$.\n\n\n\n\nCompute $G^\\text T$ in the usual way, so that $G^\\text T$ is $G$ with its edges reversed. Then do a depth-first search on $G^\\text T$ , but in the main loop of $\\text{DFS}$, consider the vertices in order of increasing values of $L(v)$. If vertex $u$ is in the depth-first tree with root $v$, then $\\min(u) = v$. Clearly, this algorithm takes $O(V + E)$ time.\n\n\nTo show correctness, first note that if $u$ is in the depth-first tree rooted at $v$ in $G^\\text T$, then there is a path $v \\leadsto u$ in $G^\\text T$, and so there is a path $u \\leadsto v$ in $G$. Thus, the minimum vertex label of all vertices reachable from $u$ is at most $L(v)$, or in other words, $L(v) \\ge \\min \\{L(w): w \\in R(u)\\}$.\n\n\nNow suppose that $L(v) > \\min \\{L(w): w \\in R(u) \\}$, so that there is a vertex $w \\in R(u)$ such that $L(w) < L(v)$. At the time $v.d$ that we started the depthfirst search from $v$, we would have already discovered $w$, so that $w.d < v.d$. By the parenthesis theorem, either the intervals $[v.d, v.f]$\u008d, and $[w.d, w.f]$\u008d are disjoint and neither $v$ nor $w$ is a descendant of the other, or we have the ordering $w.d < v.d < v.f < w.f$ and $v$ is a descendant of $w$. The latter case cannot occur, since $v$ is a root in the depth-first forest (which means that $v$ cannot be a descendant of any other vertex). In the former case, since $w.d < v.d$, we must have $w.d < w.f < v.d < v.f$. In this case, since $u$ is reachable from $w$ in $G^\\text T$ , we would have discovered $u$ by the time $w.f$, so that $u.d < w.f$. Since we discovered $u$ during a search that started at $v$, we have $v.d \\le u.d$. Thus, $v.d \\le u.d < w.f < v.d$, which is a contradiction. We conclude that no such vertex $w$ can exist.",
            "title": "22-4 Reachability"
        },
        {
            "location": "/Chap23/23.1/",
            "text": "23.1-1\n\n\n\n\nLet $(u, v)$ be a minimum-weight edge in a connected graph $G$. Show that $(u, v)$ belongs to some minimum spanning tree of $G$.\n\n\n\n\nTheorem 23.1 shows this.\n\n\nLet $A$ be the empty set and $S$ be any set containing $u$ but not $v$.\n\n\n23.1-2\n\n\n\n\nProfessor Sabatier conjectures the following converse of Theorem 23.1. Let $G = (V, E)$ be a connected, undirected graph with a real-valued weight function $w$ defined on $E$. Let $A$ be a subset of $E$ that is included in some minimum spanning tree for $G$, let $(S, V - S)$ be any cut of $G$ that respects $A$, and let $(u, v)$ be a safe edge for $A$ crossing $(S, V - S)$. Then, $(u, v)$ is a light edge for the cut. Show that the professor's conjecture is incorrect by giving a counterexample.\n\n\n\n\nLet $G$ be the graph with $4$ vertices: $u, v, w, z$. Let the edges of the graph be $(u, v), (u, w), (w, z)$ with weights $3$, $1$, and $2$ respectively. \n\n\nSuppose $A$ is the set $\\{(u, w)\\}$. Let $S = A$. Then $S$ clearly respects $A$. Since $G$ is a tree, its minimum spanning tree is itself, so $A$ is trivially a subset of a minimum spanning tree.\n\n\nMoreover, every edge is safe. In particular, $(u, v)$ is safe but not a light edge for the cut. Therefore Professor Sabatier's conjecture is false.\n\n\n23.1-3\n\n\n\n\nShow that if an edge $(u, v)$ is contained in some minimum spanning tree, then it is a light edge crossing some cut of the graph.\n\n\n\n\nLet $T_0$ and $T_1$ be the two trees that are obtained by removing edge $(u, v)$ from a $\\text{MST}$. Suppose that $V_0$ and $V_1$ are the vertices of $T_0$ and $T_1$ respectively.\n\n\nConsider the cut which separates $V_0$ from $V_1$. Suppose to a contradiction that there is some edge that has weight less than that of $(u, v)$ in this cut. Then, we could construct a minimum spanning tree of the whole graph by adding that edge to $T_1 \\cup T_0$. This would result in a minimum spanning tree that has weight less than the original minimum spanning tree that contained $(u, v)$.\n\n\n23.1-4\n\n\n\n\nGive a simple example of a connected graph such that the set of edges $\\{(u, v):$ there exists a cut $(S, V - S)$ such that $(u, v)$ is a light edge crossing $(S, V - S)\\}$ does not form a minimum spanning tree.\n\n\n\n\nA triangle whose edge weights are all equal is a graph in which every edge is a light edge crossing some cut. But the triangle is cyclic, so it is not a minimum spanning tree.\n\n\n23.1-5\n\n\n\n\nLet $e$ be a maximum-weight edge on some cycle of connected graph $G = (V, E)$. Prove that there is a minimum spanning tree of $G' = (V, E - \\{e\\})$ that is also a minimum spanning tree of $G$. That is, there is a minimum spanning tree of $G$ that does not include $e$.\n\n\n\n\nLet $A$ be any cut that causes some vertices in the cycle on once side of the cut, and some vertices in the cycle on the other. For any of these cuts, we know that the edge $e$ is not a light edge for this cut. Since all the other cuts won't have the edge $e$ crossing it, we won't have that the edge is light for any of those cuts either. This means that we have that e is not safe.\n\n\n23.1-6\n\n\n\n\nShow that a graph has a unique minimum spanning tree if, for every cut of the graph, there is a unique light edge crossing the cut. Show that the converse is not true by giving a counterexample.\n\n\n\n\nSuppose that for every cut of $G$, there is a unique light edge crossing the cut. Let us consider two distinct minimum spanning trees, $T$ and $T'$, of $G$. Because $T$ and $T'$are distinct, $T$ contains some edge $(u, v)$ that is not in $T'$. If we remove $(u, v)$ from $T$, then $T$ becomes disconnected, resulting in a cut $(S, V - S)$. The edge $(u, v)$ is a light edge crossing the cut $(S, V - S)$ (by Exercise 23.1-3) and, by our assumption, it's the only light edge crossing this cut. Because $(u, v)$ is the only light edge crossing $(S, V - S)$ and $(u, v)$ is not in $T'$, each edge in $T'$ that crosses $(S, V - S)$ must have weight strictly greater than w$(u, v)$. As in the proof of Theorem 23.1, we can identify the unique edge $(x, y)$ in $T'$ that crosses $(S, V - S)$ and lies on the cycle that results if we add $(u, v)$ to $T'$. By our assumption, we know that $w(u, v) < w(x, y)$. Then, we can then remove $(x, y)$ from $T'$ and replace it by $(u, v)$, giving a spanning tree with weight strictly less than $w(T')$. Thus, $T'$ was not a minimum spanning tree, contradicting the assumption that the graph had two unique minimum spanning trees.\n\n\nHere's a counterexample for the converse:\n\n\n\n\nHere, the graph is its own minimum spanning tree, and so the minimum spanning tree is unique. Consider the cut $(\\{x\\}, \\{y, z\\})$. Both of the edges $(x, y)$ and $(x, z)$ are light edges crossing the cut, and they are both light edges.\n\n\n23.1-7\n\n\n\n\nArgue that if all edge weights of a graph are positive, then any subset of edges that connects all vertices and has minimum total weight must be a tree. Give an example to show that the same conclusion does not follow if we allow some weights to be nonpositive.\n\n\n\n\nFirst, we show that the subset of edges of minimum total weight that connects all the vertices is a tree. To see this, suppose not, that it had a cycle. This would mean that removing any of the edges in this cycle would mean that the remaining edges would still connect all the vertices, but would have a total weight that's less by the weight of the edge that was removed. This would contradict the minimality of the total weight of the subset of vertices. Since the subset of edges forms a tree, and has minimal total weight, it must also be a minimum spanning tree.\n\n\nTo see that this conclusion is not true if we allow negative edge weights, we provide a construction. Consider the graph $K_3$ with all edge weights equal to $-1$. The only minimum weight set of edges that connects the graph has total weight $-3$, and consists of all the edges. This is clearly not a $\\text{MST}$ because it is not a tree, which can be easily seen because it has one more edge than a tree on three vertices should have. Any $\\text{MST}$ of this weighted graph must have weight that is at least $-2$.\n\n\n23.1-8\n\n\n\n\nLet $T$ be a minimum spanning tree of a graph $G$, and let $L$ be the sorted list of the edge weights of $T$. Show that for any other minimum spanning tree $T'$ of $G$, the list $L$ is also the sorted list of edge weights of $T'$.\n\n\n\n\nSuppose that $L'$ is another sorted list of edge weights of a minimum spanning tree. If $L' \\ne L$, there must be a first edge $(u, v)$ in $T$ or $T'$ which is of smaller weight than the corresponding edge $(x, y)$ in the other set. Without loss of generality, assume $(u, v)$ is in $T$. \n\n\nLet $C$ be the graph obtained by adding $(u, v)$ to $L'$. Then we must have introduced a cycle. If there exists an edge on that cycle which is of larger weight than $(u, v)$, we can remove it to obtain a tree $C'$ of weight strictly smaller than the weight of $T'$, contradicting the fact that $T'$ is a minimum spanning tree. \n\n\nThus, every edge on the cycle must be of lesser or equal weight than $(u, v)$. Suppose that every edge is of strictly smaller weight. Remove $(u, v)$ from $T$ to disconnect it into two components. There must exist some edge besides $(u, v)$ on the cycle which would connect these, and since it has smaller weight we can use that edge instead to create a spanning tree with less weight than $T$, a contradiction. Thus, some edge on the cycle has the same weight as $(u, v)$. Replace that edge by $(u, v)$. The corresponding lists $L$ and $L'$ remain unchanged since we have swapped out an edge of equal weight, but the number of edges which $T$ and $T'$ have in common has increased by $1$.\n\n\nIf we continue in this way, eventually they must have every edge in common, contradicting the fact that their edge weights differ somewhere. Therefore all minimum spanning trees have the same sorted list of edge weights.\n\n\n23.1-9\n\n\n\n\nLet $T$ be a minimum spanning tree of a graph $G = (V, E)$, and let $V'$ be a subset of $V$. Let $T'$ be the subgraph of $T$ induced by $V'$, and let $G'$ be the subgraph of $G$ induced by $V'$. Show that if $T'$ is connected, then $T'$ is a minimum spanning tree of $G'$.\n\n\n\n\nSuppose that there was some cheaper spanning tree than $T'$. That is, we have that there is some $T''$ so that $w(T'') < w(T')$. Then, let $S$ be the edges in $T$ but not in $T'$. We can then construct a minimum spanning tree of $G$ by considering $S \\cup T''$. This is a spanning tree since $S \\cup T'$ is, and $T''$ makes all the vertices in $V'$ connected just like $T'$ does.\n\n\nHowever, we have that\n\n\n$$w(S \\cup T'') = w(S) + w(T'') < w(S) + w(T') = w(S \\cup T') = w(T).$$\n\n\nThis means that we just found a spanning tree that has a lower total weight than a minimum spanning tree. This is a contradiction, and so our assumption that there was a spanning tree of $V'$ cheaper than $T'$ must be false.\n\n\n23.1-10\n\n\n\n\nGiven a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges in $T$. Show that $T$ is still a minimum spanning tree for $G$. More formally, let $T$ be a minimum spanning tree for $G$ with edge weights given by weight function $w$. Choose one edge $(x, y) \\in T$ and a positive number $k$, and define the weight function $w'$ by\n\n\n$$\nw'(u, v) =\n\\begin{cases}\nw(u, v)     & \\text{ if }(u, v) \\ne (x, y), \\\\\nw(x, y) - k & \\text{ if }(u, v) =   (x, y).\n\\end{cases}\n$$\n\n\nShow that $T$ is a minimum spanning tree for $G$ with edge weights given by $w'$.\n\n\n\n\nLet $x(T) = \\sum_{(x, y) \\in T} w(x, y)$. We have $w'(T) = w(T) - k$. Consider any other spanning tree $T'$, so that $w(T) \\le w(T')$.\n\n\nIf $(x, y) \\ne T'$, then $w'(T') = w(T') \\ge w(T) > w'(T)$.\n\n\nIf $(x, y) \\in T'$, then $w'(T') = w(T') - k \\ge w(T) - k = w'(T)$.\n\n\nEither way, $w'(T) \\le w'(T')$, and so $T$ is a minimum spanning tree for weight function $w'$.\n\n\n23.1-11 $\\star$\n\n\n\n\nGiven a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges not in $T$. Give an algorithm for finding the minimum spanning tree in the modified graph.\n\n\n\n\nIf we were to add in this newly decreased edge to the given tree, we would be creating a cycle. Then, if we were to remove any one of the edges along this cycle, we would still have a spanning tree. This means that we look at all the weights along this cycle formed by adding in the decreased edge, and remove the edge in the cycle of maximum weight. This does exactly what we want since we could only possibly want to add in the single decreased edge, and then, from there we change the graph back to a tree in the way that makes its total weight minimized.",
            "title": "23.1 Growing a minimum spanning tree"
        },
        {
            "location": "/Chap23/23.1/#231-1",
            "text": "Let $(u, v)$ be a minimum-weight edge in a connected graph $G$. Show that $(u, v)$ belongs to some minimum spanning tree of $G$.   Theorem 23.1 shows this.  Let $A$ be the empty set and $S$ be any set containing $u$ but not $v$.",
            "title": "23.1-1"
        },
        {
            "location": "/Chap23/23.1/#231-2",
            "text": "Professor Sabatier conjectures the following converse of Theorem 23.1. Let $G = (V, E)$ be a connected, undirected graph with a real-valued weight function $w$ defined on $E$. Let $A$ be a subset of $E$ that is included in some minimum spanning tree for $G$, let $(S, V - S)$ be any cut of $G$ that respects $A$, and let $(u, v)$ be a safe edge for $A$ crossing $(S, V - S)$. Then, $(u, v)$ is a light edge for the cut. Show that the professor's conjecture is incorrect by giving a counterexample.   Let $G$ be the graph with $4$ vertices: $u, v, w, z$. Let the edges of the graph be $(u, v), (u, w), (w, z)$ with weights $3$, $1$, and $2$ respectively.   Suppose $A$ is the set $\\{(u, w)\\}$. Let $S = A$. Then $S$ clearly respects $A$. Since $G$ is a tree, its minimum spanning tree is itself, so $A$ is trivially a subset of a minimum spanning tree.  Moreover, every edge is safe. In particular, $(u, v)$ is safe but not a light edge for the cut. Therefore Professor Sabatier's conjecture is false.",
            "title": "23.1-2"
        },
        {
            "location": "/Chap23/23.1/#231-3",
            "text": "Show that if an edge $(u, v)$ is contained in some minimum spanning tree, then it is a light edge crossing some cut of the graph.   Let $T_0$ and $T_1$ be the two trees that are obtained by removing edge $(u, v)$ from a $\\text{MST}$. Suppose that $V_0$ and $V_1$ are the vertices of $T_0$ and $T_1$ respectively.  Consider the cut which separates $V_0$ from $V_1$. Suppose to a contradiction that there is some edge that has weight less than that of $(u, v)$ in this cut. Then, we could construct a minimum spanning tree of the whole graph by adding that edge to $T_1 \\cup T_0$. This would result in a minimum spanning tree that has weight less than the original minimum spanning tree that contained $(u, v)$.",
            "title": "23.1-3"
        },
        {
            "location": "/Chap23/23.1/#231-4",
            "text": "Give a simple example of a connected graph such that the set of edges $\\{(u, v):$ there exists a cut $(S, V - S)$ such that $(u, v)$ is a light edge crossing $(S, V - S)\\}$ does not form a minimum spanning tree.   A triangle whose edge weights are all equal is a graph in which every edge is a light edge crossing some cut. But the triangle is cyclic, so it is not a minimum spanning tree.",
            "title": "23.1-4"
        },
        {
            "location": "/Chap23/23.1/#231-5",
            "text": "Let $e$ be a maximum-weight edge on some cycle of connected graph $G = (V, E)$. Prove that there is a minimum spanning tree of $G' = (V, E - \\{e\\})$ that is also a minimum spanning tree of $G$. That is, there is a minimum spanning tree of $G$ that does not include $e$.   Let $A$ be any cut that causes some vertices in the cycle on once side of the cut, and some vertices in the cycle on the other. For any of these cuts, we know that the edge $e$ is not a light edge for this cut. Since all the other cuts won't have the edge $e$ crossing it, we won't have that the edge is light for any of those cuts either. This means that we have that e is not safe.",
            "title": "23.1-5"
        },
        {
            "location": "/Chap23/23.1/#231-6",
            "text": "Show that a graph has a unique minimum spanning tree if, for every cut of the graph, there is a unique light edge crossing the cut. Show that the converse is not true by giving a counterexample.   Suppose that for every cut of $G$, there is a unique light edge crossing the cut. Let us consider two distinct minimum spanning trees, $T$ and $T'$, of $G$. Because $T$ and $T'$are distinct, $T$ contains some edge $(u, v)$ that is not in $T'$. If we remove $(u, v)$ from $T$, then $T$ becomes disconnected, resulting in a cut $(S, V - S)$. The edge $(u, v)$ is a light edge crossing the cut $(S, V - S)$ (by Exercise 23.1-3) and, by our assumption, it's the only light edge crossing this cut. Because $(u, v)$ is the only light edge crossing $(S, V - S)$ and $(u, v)$ is not in $T'$, each edge in $T'$ that crosses $(S, V - S)$ must have weight strictly greater than w$(u, v)$. As in the proof of Theorem 23.1, we can identify the unique edge $(x, y)$ in $T'$ that crosses $(S, V - S)$ and lies on the cycle that results if we add $(u, v)$ to $T'$. By our assumption, we know that $w(u, v) < w(x, y)$. Then, we can then remove $(x, y)$ from $T'$ and replace it by $(u, v)$, giving a spanning tree with weight strictly less than $w(T')$. Thus, $T'$ was not a minimum spanning tree, contradicting the assumption that the graph had two unique minimum spanning trees.  Here's a counterexample for the converse:   Here, the graph is its own minimum spanning tree, and so the minimum spanning tree is unique. Consider the cut $(\\{x\\}, \\{y, z\\})$. Both of the edges $(x, y)$ and $(x, z)$ are light edges crossing the cut, and they are both light edges.",
            "title": "23.1-6"
        },
        {
            "location": "/Chap23/23.1/#231-7",
            "text": "Argue that if all edge weights of a graph are positive, then any subset of edges that connects all vertices and has minimum total weight must be a tree. Give an example to show that the same conclusion does not follow if we allow some weights to be nonpositive.   First, we show that the subset of edges of minimum total weight that connects all the vertices is a tree. To see this, suppose not, that it had a cycle. This would mean that removing any of the edges in this cycle would mean that the remaining edges would still connect all the vertices, but would have a total weight that's less by the weight of the edge that was removed. This would contradict the minimality of the total weight of the subset of vertices. Since the subset of edges forms a tree, and has minimal total weight, it must also be a minimum spanning tree.  To see that this conclusion is not true if we allow negative edge weights, we provide a construction. Consider the graph $K_3$ with all edge weights equal to $-1$. The only minimum weight set of edges that connects the graph has total weight $-3$, and consists of all the edges. This is clearly not a $\\text{MST}$ because it is not a tree, which can be easily seen because it has one more edge than a tree on three vertices should have. Any $\\text{MST}$ of this weighted graph must have weight that is at least $-2$.",
            "title": "23.1-7"
        },
        {
            "location": "/Chap23/23.1/#231-8",
            "text": "Let $T$ be a minimum spanning tree of a graph $G$, and let $L$ be the sorted list of the edge weights of $T$. Show that for any other minimum spanning tree $T'$ of $G$, the list $L$ is also the sorted list of edge weights of $T'$.   Suppose that $L'$ is another sorted list of edge weights of a minimum spanning tree. If $L' \\ne L$, there must be a first edge $(u, v)$ in $T$ or $T'$ which is of smaller weight than the corresponding edge $(x, y)$ in the other set. Without loss of generality, assume $(u, v)$ is in $T$.   Let $C$ be the graph obtained by adding $(u, v)$ to $L'$. Then we must have introduced a cycle. If there exists an edge on that cycle which is of larger weight than $(u, v)$, we can remove it to obtain a tree $C'$ of weight strictly smaller than the weight of $T'$, contradicting the fact that $T'$ is a minimum spanning tree.   Thus, every edge on the cycle must be of lesser or equal weight than $(u, v)$. Suppose that every edge is of strictly smaller weight. Remove $(u, v)$ from $T$ to disconnect it into two components. There must exist some edge besides $(u, v)$ on the cycle which would connect these, and since it has smaller weight we can use that edge instead to create a spanning tree with less weight than $T$, a contradiction. Thus, some edge on the cycle has the same weight as $(u, v)$. Replace that edge by $(u, v)$. The corresponding lists $L$ and $L'$ remain unchanged since we have swapped out an edge of equal weight, but the number of edges which $T$ and $T'$ have in common has increased by $1$.  If we continue in this way, eventually they must have every edge in common, contradicting the fact that their edge weights differ somewhere. Therefore all minimum spanning trees have the same sorted list of edge weights.",
            "title": "23.1-8"
        },
        {
            "location": "/Chap23/23.1/#231-9",
            "text": "Let $T$ be a minimum spanning tree of a graph $G = (V, E)$, and let $V'$ be a subset of $V$. Let $T'$ be the subgraph of $T$ induced by $V'$, and let $G'$ be the subgraph of $G$ induced by $V'$. Show that if $T'$ is connected, then $T'$ is a minimum spanning tree of $G'$.   Suppose that there was some cheaper spanning tree than $T'$. That is, we have that there is some $T''$ so that $w(T'') < w(T')$. Then, let $S$ be the edges in $T$ but not in $T'$. We can then construct a minimum spanning tree of $G$ by considering $S \\cup T''$. This is a spanning tree since $S \\cup T'$ is, and $T''$ makes all the vertices in $V'$ connected just like $T'$ does.  However, we have that  $$w(S \\cup T'') = w(S) + w(T'') < w(S) + w(T') = w(S \\cup T') = w(T).$$  This means that we just found a spanning tree that has a lower total weight than a minimum spanning tree. This is a contradiction, and so our assumption that there was a spanning tree of $V'$ cheaper than $T'$ must be false.",
            "title": "23.1-9"
        },
        {
            "location": "/Chap23/23.1/#231-10",
            "text": "Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges in $T$. Show that $T$ is still a minimum spanning tree for $G$. More formally, let $T$ be a minimum spanning tree for $G$ with edge weights given by weight function $w$. Choose one edge $(x, y) \\in T$ and a positive number $k$, and define the weight function $w'$ by  $$\nw'(u, v) =\n\\begin{cases}\nw(u, v)     & \\text{ if }(u, v) \\ne (x, y), \\\\\nw(x, y) - k & \\text{ if }(u, v) =   (x, y).\n\\end{cases}\n$$  Show that $T$ is a minimum spanning tree for $G$ with edge weights given by $w'$.   Let $x(T) = \\sum_{(x, y) \\in T} w(x, y)$. We have $w'(T) = w(T) - k$. Consider any other spanning tree $T'$, so that $w(T) \\le w(T')$.  If $(x, y) \\ne T'$, then $w'(T') = w(T') \\ge w(T) > w'(T)$.  If $(x, y) \\in T'$, then $w'(T') = w(T') - k \\ge w(T) - k = w'(T)$.  Either way, $w'(T) \\le w'(T')$, and so $T$ is a minimum spanning tree for weight function $w'$.",
            "title": "23.1-10"
        },
        {
            "location": "/Chap23/23.1/#231-11-star",
            "text": "Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges not in $T$. Give an algorithm for finding the minimum spanning tree in the modified graph.   If we were to add in this newly decreased edge to the given tree, we would be creating a cycle. Then, if we were to remove any one of the edges along this cycle, we would still have a spanning tree. This means that we look at all the weights along this cycle formed by adding in the decreased edge, and remove the edge in the cycle of maximum weight. This does exactly what we want since we could only possibly want to add in the single decreased edge, and then, from there we change the graph back to a tree in the way that makes its total weight minimized.",
            "title": "23.1-11 $\\star$"
        },
        {
            "location": "/Chap23/23.2/",
            "text": "23.2-1\n\n\n\n\nKruskal's algorithm can return different spanning trees for the same input graph $G$, depending on how it breaks ties when the edges are sorted into order. Show that for each minimum spanning tree $T$ of $G$, there is a way to sort the edges of $G$ in Kruskal's algorithm so that the algorithm returns $T$.\n\n\n\n\nSuppose that we wanted to pick $T$ as our minimum spanning tree. Then, to obtain this tree with Kruskal's algorithm, we will order the edges first by their weight, but then will resolve ties in edge weights by picking an edge first if it is contained in the minimum spanning tree, and treating all the edges that aren't in $T$ as being slightly larger, even though they have the same actual weight.\n\n\nWith this ordering, we will still be finding a tree of the same weight as all the minimum spanning trees $w(T)$. However, since we prioritize the edges in $T$, we have that we will pick them over any other edges that may be in other minimum spanning trees.\n\n\n23.2-2\n\n\n\n\nSuppose that we represent the graph $G = (V, E)$ as an adjacency matrix. Give a simple implementation of Prim's algorithm for this case that runs in $O(V^2)$ time.\n\n\n\n\nAt each step of the algorithm we will add an edge from a vertex in the tree created so far to a vertex not in the tree, such that this edge has minimum weight. Thus, it will be useful to know, for each vertex not in the tree, the edge from that vertex to some vertex in the tree of minimal weight. We will store this information in an array $A$, where $A[u] = (v, w)$ if $w$ is the weight of $(u, v)$ and is minimal among the weights of edges from $u$ to some vertex $v$ in the tree built so far. We'll use $A[u].1$ to access $v$ and $A[u].2$ to access $w$.\n\n\nPRIM\n-\nADJ\n(\nG\n,\n \nw\n,\n \nr\n)\n\n    \ninitialize\n \nA\n \nwith\n \nevery\n \nentry\n \n=\n \n(\nNIL\n,\n \n\u221e\n)\n\n    \nT\n \n=\n \n{\nr\n}\n\n    \nfor\n \ni\n \n=\n \n1\n \nto\n \nV\n\n        \nif\n \nAdj\n[\nr\n,\n \ni\n]\n \n!=\n \n0\n\n            \nA\n[\ni\n]\n \n=\n \n(\nr\n,\n \nw\n(\nr\n,\n \ni\n))\n\n    \nfor\n \neach\n \nu\n \nin\n \nV\n \n-\n \nT\n\n        \nk\n \n=\n \nmin\n(\nA\n[\ni\n]\n.2\n)\n\n        \nT\n \n=\n \nT\n \n\u222a\n \n{\nk\n}\n\n        \nk\n.\nPI\n \n=\n \nA\n[\nk\n]\n.1\n\n        \nfor\n \ni\n \n=\n \n1\n \nto\n \nV\n\n            \nif\n \nAdf\n[\nk\n,\n \ni\n]\n \n!=\n \n0\n \nand\n \nAdj\n[\nk\n,\n \ni\n]\n \n<\n \nA\n[\ni\n]\n.2\n\n                \nA\n[\ni\n]\n \n=\n \n(\nk\n,\n \nAdj\n[\nk\n,\n \ni\n])\n\n\n\n\n\n23.2-3\n\n\n\n\nFor a sparse graph $G = (V, E)$, where $|E| = \\Theta(V)$, is the implementation of Prim's algorithm with a Fibonacci heap asymptotically faster than the binary-heap implementation? What about for a dense graph, where $|E| = \\Theta(V^2)$? How must the sizes $|E|$ and $|V|$ be related for the Fibonacci-heap implementation to be asymptotically faster than the binary-heap implementation?\n\n\n\n\nPrim's algorithm implemented with a Binary heap has runtime $O((V + E)\\lg V)$, which in the sparse case, is just $O(V\\lg V)$. The implementation with Fibonacci heaps is\n\n\n$$O(E + V\\lg V) = O(V + V\\lg V) = O(V \\lg V).$$\n\n\n\n\nIn the sparse case, the two algorithms have the same asymptotic runtimes.\n\n\n\n\nIn the dense case.\n\n\n\n\n\n\nThe binary heap implementation has a runtime of \n\n\n$$O((V + E)\\lg V) = O((V + V^2)\\lg V) = O(V^2\\lg V).$$\n\n\n\n\n\n\nThe Fibonacci heap implementation has a runtime of\n\n\n$$O(E + V\\lg V) = O(V^2 + V\\lg V) = O(V^2).$$\n\n\n\n\n\n\nSo, in the dense case, we have that the Fibonacci heap implementation is asymptotically faster.\n\n\n\n\n\n\nThe Fibonacci heap implementation will be asymptotically faster so long as $E = \\omega(V)$. Suppose that we have some function that grows more quickly than linear, say $f$, and $E = f(V)$. \n\n\n\n\nThe binary heap implementation will have runtime of\n\n\n\n\n$$O((V + E)\\lg V) = O((V + f(V))\\lg V) = O(f(V)\\lg V).$$\n\n\nHowever, we have that the runtime of the Fibonacci heap implementation will have runtime of\n\n\n$$O(E + V\\lg V) = O(f(V) + V\\lg V).$$\n\n\nThis runtime is either $O(f(V))$ or $O(V\\lg V)$ depending on if $f(V)$ grows more or less quickly than $V\\lg V$ respectively.\n\n\nIn either case, we have that the runtime is faster than $O(f(V)\\lg V)$.\n\n\n23.2-4\n\n\n\n\nSuppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Kruskal's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$?\n\n\n\n\nWe know that Kruskal's algorithm takes $O(V)$ time for initialization, $O(E\\lg E)$ time to sort the edges, and $O(E\\alpha(V))$ time for the disjoint-set operations, for a total running time of $O(V + E\\lg E + E\\alpha(V)) = O(E\\lg E)$.\n\n\nIf we knew that all of the edge weights in the graph were integers in the range from $1$ to $|V|$, then we could sort the edges in $O(V + E)$ time using counting sort. Since the graph is connected, $V = O(E)$, and so the sorting time is reduced to $O(E)$. This would yield a total running time of $O(V + E + E\\alpha(V)) = O(E\\alpha(V))$, again since $V = O(E)$, and since $E = O(E\\alpha(V))$. The time to process the edges, not the time to sort them, is now the dominant term. Knowledge about the weights won't help speed up any other part of the algorithm, since nothing besides the sort uses the weight values.\n\n\nIf the edge weights were integers in the range from $1$ to $W$ for some constant $W$, then we could again use counting sort to sort the edges more quickly. This time, sorting would take $O(E + W) = O(E)$ time, since $W$ is a constant. As in the first part, we get a total running time of $O(E\\alpha(V))$.\n\n\n23.2-5\n\n\n\n\nSuppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Prim's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$?\n\n\n\n\nThe time taken by Prim's algorithm is determined by the speed of the queue operations. With the queue implemented as a Fibonacci heap, it takes $O(E + V\\lg V)$ time.\n\n\nSince the keys in the priority queue are edge weights, it might be possible to implement the queue even more efficiently when there are restrictions on the possible edge weights.\n\n\nWe can improve the running time of Prim's algorithm if $W$ is a constant by implementing the queue as an array $Q[0..W + 1]$\u008d (using the $W + 1$ slot for $\\text{key} = \\infty$), where each slot holds a doubly linked list of vertices with that weight as their key. Then $\\text{EXTRACT-MIN}$ takes only $O(W) = O(1)$ time (just scan for the first nonempty slot), and $\\text{DECREASE-KEY}$ takes only $O(1)$ time (just remove the vertex from the list it's in and insert it at the front of the list indexed by the new key). This gives a total running time of $O(E)$, which is the best possible asymptotic time (since $\\Omega(E)$ edges must be processed).\n\n\nHowever, if the range of edge weights is $1$ to $|V|$, then $\\text{EXTRACT-MIN}$ takes $\\Theta(V)$ time with this data structure. So the total time spent doing $\\text{EXTRACT-MIN}$ is $\\Theta(V^2)$, slowing the algorithm to $\\Theta(E + V^2) = \\Theta(V^2)$. In this case, it is better to keep the Fibonacci-heap priority queue, which gave the $\\Theta(E + V\\lg V)$ time.\n\n\nOther data structures yield better running times:\n\n\n\n\nvan Emde Boas trees (see Chapter 20) give an upper bound of $O(E + V\\lg\\lg V)$ time for Prim's algorithm.\n\n\nA redistributive heap (used in the single-source shortest-paths algorithm of Ahuja, Mehlhorn, Orlin, and Tarjan, and mentioned in the chapter notes for Chapter 24) gives an upper bound of $O(E + V \\sqrt{\\lg V})$ for Prim's algorithm.\n\n\n\n\n23.2-6 $\\star$\n\n\n\n\nSuppose that the edge weights in a graph are uniformly distributed over the halfopen interval $[0, 1)$. Which algorithm, Kruskal's or Prim's, can you make run faster?\n\n\n\n\nFor input drawn from a uniform distribution I would use bucket sort with Kruskal's algorithm, for expected linear time sorting of edges by weight. This would achieve expected runtime $O(E\\alpha(V))$.\n\n\n23.2-7 $\\star$\n\n\n\n\nSuppose that a graph $G$ has a minimum spanning tree already computed. How quickly can we update the minimum spanning tree if we add a new vertex and incident edges to $G$?\n\n\n\n\nWe start with the following lemma.\n\n\nLemma\n\n\nLet $T$ be a minimum spanning tree of $G = (V, E)$, and consider a graph $G' = (V', E')$ for which $G$ is a subgraph, i.e., $V \\subseteq V'$ and $E \\subseteq E'$. Let $\\overline T = E - T$ be the edges of $G$ that are not in $T$. Then there is a minimum spanning tree of $G'$ that includes no edges in $\\overline T$.\n\n\nProof\n \n\n\nBy Exercise 23.2-1, there is a way to order the edges of $E$ so that Kruskal's algorithm, when run on $G$, produces the minimum spanning tree $T$. We will show that Kruskal's algorithm, run on $G'$, produces a minimum spanning tree $T'$ that includes no edges in $\\overline T$. We assume that the edges in $E$ are considered in the same relative order when Kruskal's algorithm is run on $G$ and on $G'$. We first state and prove the following claim.\n\n\nClaim\n \n\n\nFor any pair of vertices $u, v \\in V$, if these vertices are in the same set after Kruskal's algorithm run on $G$ considers any edge $(x, y) \\in E$, then they are in the same set after Kruskal's algorithm run on $G'$ considers $(x, y)$.\n\n\nProof of claim\n \n\n\nLet us order the edges of $E$ by nondecreasing weight as $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_k, y_k) \\rangle$, where $k = |E|$. This sequence gives the order in which the edges of $E$ are considered by Kruskal's algorithm, whether it is run on $G$ or on $G'$. We will use induction, with the inductive hypothesis that if $u$ and $v$ are in the same set after Kruskal's algorithm run on $G$ considers an edge $(x_i, y_i)$, then they are in the same set after Kruskal's algorithm run on $G'$ considers the same edge. We use induction on $i$.\n\n\nBasis:\n For the basis, $i = 0$. Kruskal's algorithm run on $G$ has not considered any edges, and so all vertices are in different sets. The inductive hypothesis holds trivially.\n\n\nInductive step:\n We assume that any vertices that are in the same set after Kruskal's algorithm run on $G$ has considered edges $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_{i - 1}, y_{i - 1}) \\rangle$ are in the same set after Kruskal's algorithm run on $G'$ has considered the same edges. When Kruskal's algorithm runs on $G'$, after it considers $(x_{i - 1}, y_{i - 1})$, it may consider some edges in $E' - E$ before considering $(x_i, y_i)$. The edges in $E' - E$ may cause $\\text{UNION}$ operations to occur, but sets are never divided. Hence, any vertices that are in the same set after Kruskal's algorithm run on $G'$ considers $(x_{i - 1}, y_{i - 1})$ are still in the same set when $(x_i, y_i)$ is considered.\n\n\nWhen Kruskal's algorithm run on $G$ considers $(x_i, y_i)$, either $x_i$ and $y_i$ are found to be in the same set or they are not.\n\n\n\n\nIf Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in the same set, then no $\\text{UNION}$ operation occurs. The sets of vertices remain the same, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$.\n\n\nIf Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in different sets, then the operation $\\text{UNION}(x_i, y_i)$ will occur. Kruskal's algorithm run on $G'$ will find that either $x_i$ and $y_i$ are in the same set or they are not. By the inductive hypothesis, when edge $(x_i, y_i)$ is considered, all vertices in $x_i$'s set when Kruskal's algorithm runs on $G$ are in $x_i$'s set when Kruskal's algorithm runs on $G'$, and the same holds for $y_i$. Regardless of whether Kruskal's algorithm run on $G'$ finds $x_i$ and $y_i$ to already be in the same set, their sets are united after considering $(x_i, y_i)$, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$. (#claim)\n\n\n\n\nWith the claim in hand, we suppose that some edge $(u, v) \\in \\overline T$ is placed into $T'$. That means that Kruskal's algorithm run on $G$ found $u$ and $v$ to be in the same set (since $(u, v) \\in \\overline T$ ) but Kruskal's algorithm run on $G'$ found $u$ and $v$ to be in different sets (since $(u, v)$ is placed into $T'$). This fact contradicts the claim, and we conclude that no edge in $\\overline T$ is placed into $T'$. Thus, by running Kruskal's algorithm on $G$ and $G'$, we demonstrate that there exists a minimum spanning tree of $G'$ that includes no edges in $\\overline T$. (#lemma)\n\n\nWe use this lemma as follows. Let $G' = (V', E')$ be the graph $G = (V, E)$ with the one new vertex and its incident edges added. Suppose that we have a minimum spanning tree $T$ for $G$. We compute a minimum spanning tree for $G'$ by creating the graph $G'' = (V', E'')$, where $E''$ consists of the edges of $T$ and the edges in $E' - E$ (i.e., the edges added to $G$ that made $G'$), and then finding a minimum spanning tree $T'$ for $G''$. By the lemma, there is a minimum spanning tree for $G'$ that includes no edges of $E - T$. In other words, $G'$ has a minimum spanning tree that includes only edges in $T$ and $E' - E$ ; these edges comprise exactly the set $E''$. Thus, the the minimum spanning tree $T'$ of $G''$ is also a minimum spanning tree of $G'$.\n\n\nEven though the proof of the lemma uses Kruskal's algorithm, we are not required to use this algorithm to find $T'$. We can find a minimum spanning tree by any means we choose. Let us use Prim's algorithm with a Fibonacci-heap priority queue. Since $|V'| = |V| + 1$ and $|E''| \\le 2|V| - 1$ ($E''$ contains the $|V| - 1$ edges of $T$ and at most $|V|$ edges in $E' - E$ ), it takes $O(V)$ time to construct $G''$, and the run of Prim's algorithm with a Fibonacci-heap priority queue takes time $O(E'' + V'\\lg V) = O(V\\lg V)$. Thus, if we are given a minimum spanning tree of $G$, we can compute a minimum spanning tree of $G'$ in $O(V\\lg V)$ time.\n\n\n23.2-8\n\n\n\n\nProfessor Borden proposes a new divide-and-conquer algorithm for computing minimum spanning trees, which goes as follows. Given a graph $G = (V, E)$, partition the set $V$ of vertices into two sets $V_1$ and $V_2$ such that $|V_1|$ and $|V_2|$ differ by at most $1$. Let $E_1$ be the set of edges that are incident only on vertices in $V_1$, and let $E_2$ be the set of edges that are incident only on vertices in $V_2$. Recursively solve a minimum-spanning-tree problem on each of the two subgraphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$. Finally, select the minimum-weight edge in $E$ that crosses the cut $(V_1, V_2)$, and use this edge to unite the resulting two minimum spanning trees into a single spanning tree.\n\n\nEither argue that the algorithm correctly computes a minimum spanning tree of $G$, or provide an example for which the algorithm fails.\n\n\n\n\nThe algorithm fails. Suppose $E = \\{(u, v), (u, w), (v, w)\\}$, the weight of $(u, v)$ and $(u, w)$ is $1$, and the weight of $(v, w)$ is $1000$, partition the set into two sets $V_1 = \\{u\\}$ and $V_2 = \\{v, w\\}$.",
            "title": "23.2 The algorithms of Kruskal and Prim"
        },
        {
            "location": "/Chap23/23.2/#232-1",
            "text": "Kruskal's algorithm can return different spanning trees for the same input graph $G$, depending on how it breaks ties when the edges are sorted into order. Show that for each minimum spanning tree $T$ of $G$, there is a way to sort the edges of $G$ in Kruskal's algorithm so that the algorithm returns $T$.   Suppose that we wanted to pick $T$ as our minimum spanning tree. Then, to obtain this tree with Kruskal's algorithm, we will order the edges first by their weight, but then will resolve ties in edge weights by picking an edge first if it is contained in the minimum spanning tree, and treating all the edges that aren't in $T$ as being slightly larger, even though they have the same actual weight.  With this ordering, we will still be finding a tree of the same weight as all the minimum spanning trees $w(T)$. However, since we prioritize the edges in $T$, we have that we will pick them over any other edges that may be in other minimum spanning trees.",
            "title": "23.2-1"
        },
        {
            "location": "/Chap23/23.2/#232-2",
            "text": "Suppose that we represent the graph $G = (V, E)$ as an adjacency matrix. Give a simple implementation of Prim's algorithm for this case that runs in $O(V^2)$ time.   At each step of the algorithm we will add an edge from a vertex in the tree created so far to a vertex not in the tree, such that this edge has minimum weight. Thus, it will be useful to know, for each vertex not in the tree, the edge from that vertex to some vertex in the tree of minimal weight. We will store this information in an array $A$, where $A[u] = (v, w)$ if $w$ is the weight of $(u, v)$ and is minimal among the weights of edges from $u$ to some vertex $v$ in the tree built so far. We'll use $A[u].1$ to access $v$ and $A[u].2$ to access $w$.  PRIM - ADJ ( G ,   w ,   r ) \n     initialize   A   with   every   entry   =   ( NIL ,   \u221e ) \n     T   =   { r } \n     for   i   =   1   to   V \n         if   Adj [ r ,   i ]   !=   0 \n             A [ i ]   =   ( r ,   w ( r ,   i )) \n     for   each   u   in   V   -   T \n         k   =   min ( A [ i ] .2 ) \n         T   =   T   \u222a   { k } \n         k . PI   =   A [ k ] .1 \n         for   i   =   1   to   V \n             if   Adf [ k ,   i ]   !=   0   and   Adj [ k ,   i ]   <   A [ i ] .2 \n                 A [ i ]   =   ( k ,   Adj [ k ,   i ])",
            "title": "23.2-2"
        },
        {
            "location": "/Chap23/23.2/#232-3",
            "text": "For a sparse graph $G = (V, E)$, where $|E| = \\Theta(V)$, is the implementation of Prim's algorithm with a Fibonacci heap asymptotically faster than the binary-heap implementation? What about for a dense graph, where $|E| = \\Theta(V^2)$? How must the sizes $|E|$ and $|V|$ be related for the Fibonacci-heap implementation to be asymptotically faster than the binary-heap implementation?   Prim's algorithm implemented with a Binary heap has runtime $O((V + E)\\lg V)$, which in the sparse case, is just $O(V\\lg V)$. The implementation with Fibonacci heaps is  $$O(E + V\\lg V) = O(V + V\\lg V) = O(V \\lg V).$$   In the sparse case, the two algorithms have the same asymptotic runtimes.   In the dense case.    The binary heap implementation has a runtime of   $$O((V + E)\\lg V) = O((V + V^2)\\lg V) = O(V^2\\lg V).$$    The Fibonacci heap implementation has a runtime of  $$O(E + V\\lg V) = O(V^2 + V\\lg V) = O(V^2).$$    So, in the dense case, we have that the Fibonacci heap implementation is asymptotically faster.    The Fibonacci heap implementation will be asymptotically faster so long as $E = \\omega(V)$. Suppose that we have some function that grows more quickly than linear, say $f$, and $E = f(V)$.    The binary heap implementation will have runtime of   $$O((V + E)\\lg V) = O((V + f(V))\\lg V) = O(f(V)\\lg V).$$  However, we have that the runtime of the Fibonacci heap implementation will have runtime of  $$O(E + V\\lg V) = O(f(V) + V\\lg V).$$  This runtime is either $O(f(V))$ or $O(V\\lg V)$ depending on if $f(V)$ grows more or less quickly than $V\\lg V$ respectively.  In either case, we have that the runtime is faster than $O(f(V)\\lg V)$.",
            "title": "23.2-3"
        },
        {
            "location": "/Chap23/23.2/#232-4",
            "text": "Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Kruskal's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$?   We know that Kruskal's algorithm takes $O(V)$ time for initialization, $O(E\\lg E)$ time to sort the edges, and $O(E\\alpha(V))$ time for the disjoint-set operations, for a total running time of $O(V + E\\lg E + E\\alpha(V)) = O(E\\lg E)$.  If we knew that all of the edge weights in the graph were integers in the range from $1$ to $|V|$, then we could sort the edges in $O(V + E)$ time using counting sort. Since the graph is connected, $V = O(E)$, and so the sorting time is reduced to $O(E)$. This would yield a total running time of $O(V + E + E\\alpha(V)) = O(E\\alpha(V))$, again since $V = O(E)$, and since $E = O(E\\alpha(V))$. The time to process the edges, not the time to sort them, is now the dominant term. Knowledge about the weights won't help speed up any other part of the algorithm, since nothing besides the sort uses the weight values.  If the edge weights were integers in the range from $1$ to $W$ for some constant $W$, then we could again use counting sort to sort the edges more quickly. This time, sorting would take $O(E + W) = O(E)$ time, since $W$ is a constant. As in the first part, we get a total running time of $O(E\\alpha(V))$.",
            "title": "23.2-4"
        },
        {
            "location": "/Chap23/23.2/#232-5",
            "text": "Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Prim's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$?   The time taken by Prim's algorithm is determined by the speed of the queue operations. With the queue implemented as a Fibonacci heap, it takes $O(E + V\\lg V)$ time.  Since the keys in the priority queue are edge weights, it might be possible to implement the queue even more efficiently when there are restrictions on the possible edge weights.  We can improve the running time of Prim's algorithm if $W$ is a constant by implementing the queue as an array $Q[0..W + 1]$\u008d (using the $W + 1$ slot for $\\text{key} = \\infty$), where each slot holds a doubly linked list of vertices with that weight as their key. Then $\\text{EXTRACT-MIN}$ takes only $O(W) = O(1)$ time (just scan for the first nonempty slot), and $\\text{DECREASE-KEY}$ takes only $O(1)$ time (just remove the vertex from the list it's in and insert it at the front of the list indexed by the new key). This gives a total running time of $O(E)$, which is the best possible asymptotic time (since $\\Omega(E)$ edges must be processed).  However, if the range of edge weights is $1$ to $|V|$, then $\\text{EXTRACT-MIN}$ takes $\\Theta(V)$ time with this data structure. So the total time spent doing $\\text{EXTRACT-MIN}$ is $\\Theta(V^2)$, slowing the algorithm to $\\Theta(E + V^2) = \\Theta(V^2)$. In this case, it is better to keep the Fibonacci-heap priority queue, which gave the $\\Theta(E + V\\lg V)$ time.  Other data structures yield better running times:   van Emde Boas trees (see Chapter 20) give an upper bound of $O(E + V\\lg\\lg V)$ time for Prim's algorithm.  A redistributive heap (used in the single-source shortest-paths algorithm of Ahuja, Mehlhorn, Orlin, and Tarjan, and mentioned in the chapter notes for Chapter 24) gives an upper bound of $O(E + V \\sqrt{\\lg V})$ for Prim's algorithm.",
            "title": "23.2-5"
        },
        {
            "location": "/Chap23/23.2/#232-6-star",
            "text": "Suppose that the edge weights in a graph are uniformly distributed over the halfopen interval $[0, 1)$. Which algorithm, Kruskal's or Prim's, can you make run faster?   For input drawn from a uniform distribution I would use bucket sort with Kruskal's algorithm, for expected linear time sorting of edges by weight. This would achieve expected runtime $O(E\\alpha(V))$.",
            "title": "23.2-6 $\\star$"
        },
        {
            "location": "/Chap23/23.2/#232-7-star",
            "text": "Suppose that a graph $G$ has a minimum spanning tree already computed. How quickly can we update the minimum spanning tree if we add a new vertex and incident edges to $G$?   We start with the following lemma.  Lemma  Let $T$ be a minimum spanning tree of $G = (V, E)$, and consider a graph $G' = (V', E')$ for which $G$ is a subgraph, i.e., $V \\subseteq V'$ and $E \\subseteq E'$. Let $\\overline T = E - T$ be the edges of $G$ that are not in $T$. Then there is a minimum spanning tree of $G'$ that includes no edges in $\\overline T$.  Proof    By Exercise 23.2-1, there is a way to order the edges of $E$ so that Kruskal's algorithm, when run on $G$, produces the minimum spanning tree $T$. We will show that Kruskal's algorithm, run on $G'$, produces a minimum spanning tree $T'$ that includes no edges in $\\overline T$. We assume that the edges in $E$ are considered in the same relative order when Kruskal's algorithm is run on $G$ and on $G'$. We first state and prove the following claim.  Claim    For any pair of vertices $u, v \\in V$, if these vertices are in the same set after Kruskal's algorithm run on $G$ considers any edge $(x, y) \\in E$, then they are in the same set after Kruskal's algorithm run on $G'$ considers $(x, y)$.  Proof of claim    Let us order the edges of $E$ by nondecreasing weight as $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_k, y_k) \\rangle$, where $k = |E|$. This sequence gives the order in which the edges of $E$ are considered by Kruskal's algorithm, whether it is run on $G$ or on $G'$. We will use induction, with the inductive hypothesis that if $u$ and $v$ are in the same set after Kruskal's algorithm run on $G$ considers an edge $(x_i, y_i)$, then they are in the same set after Kruskal's algorithm run on $G'$ considers the same edge. We use induction on $i$.  Basis:  For the basis, $i = 0$. Kruskal's algorithm run on $G$ has not considered any edges, and so all vertices are in different sets. The inductive hypothesis holds trivially.  Inductive step:  We assume that any vertices that are in the same set after Kruskal's algorithm run on $G$ has considered edges $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_{i - 1}, y_{i - 1}) \\rangle$ are in the same set after Kruskal's algorithm run on $G'$ has considered the same edges. When Kruskal's algorithm runs on $G'$, after it considers $(x_{i - 1}, y_{i - 1})$, it may consider some edges in $E' - E$ before considering $(x_i, y_i)$. The edges in $E' - E$ may cause $\\text{UNION}$ operations to occur, but sets are never divided. Hence, any vertices that are in the same set after Kruskal's algorithm run on $G'$ considers $(x_{i - 1}, y_{i - 1})$ are still in the same set when $(x_i, y_i)$ is considered.  When Kruskal's algorithm run on $G$ considers $(x_i, y_i)$, either $x_i$ and $y_i$ are found to be in the same set or they are not.   If Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in the same set, then no $\\text{UNION}$ operation occurs. The sets of vertices remain the same, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$.  If Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in different sets, then the operation $\\text{UNION}(x_i, y_i)$ will occur. Kruskal's algorithm run on $G'$ will find that either $x_i$ and $y_i$ are in the same set or they are not. By the inductive hypothesis, when edge $(x_i, y_i)$ is considered, all vertices in $x_i$'s set when Kruskal's algorithm runs on $G$ are in $x_i$'s set when Kruskal's algorithm runs on $G'$, and the same holds for $y_i$. Regardless of whether Kruskal's algorithm run on $G'$ finds $x_i$ and $y_i$ to already be in the same set, their sets are united after considering $(x_i, y_i)$, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$. (#claim)   With the claim in hand, we suppose that some edge $(u, v) \\in \\overline T$ is placed into $T'$. That means that Kruskal's algorithm run on $G$ found $u$ and $v$ to be in the same set (since $(u, v) \\in \\overline T$ ) but Kruskal's algorithm run on $G'$ found $u$ and $v$ to be in different sets (since $(u, v)$ is placed into $T'$). This fact contradicts the claim, and we conclude that no edge in $\\overline T$ is placed into $T'$. Thus, by running Kruskal's algorithm on $G$ and $G'$, we demonstrate that there exists a minimum spanning tree of $G'$ that includes no edges in $\\overline T$. (#lemma)  We use this lemma as follows. Let $G' = (V', E')$ be the graph $G = (V, E)$ with the one new vertex and its incident edges added. Suppose that we have a minimum spanning tree $T$ for $G$. We compute a minimum spanning tree for $G'$ by creating the graph $G'' = (V', E'')$, where $E''$ consists of the edges of $T$ and the edges in $E' - E$ (i.e., the edges added to $G$ that made $G'$), and then finding a minimum spanning tree $T'$ for $G''$. By the lemma, there is a minimum spanning tree for $G'$ that includes no edges of $E - T$. In other words, $G'$ has a minimum spanning tree that includes only edges in $T$ and $E' - E$ ; these edges comprise exactly the set $E''$. Thus, the the minimum spanning tree $T'$ of $G''$ is also a minimum spanning tree of $G'$.  Even though the proof of the lemma uses Kruskal's algorithm, we are not required to use this algorithm to find $T'$. We can find a minimum spanning tree by any means we choose. Let us use Prim's algorithm with a Fibonacci-heap priority queue. Since $|V'| = |V| + 1$ and $|E''| \\le 2|V| - 1$ ($E''$ contains the $|V| - 1$ edges of $T$ and at most $|V|$ edges in $E' - E$ ), it takes $O(V)$ time to construct $G''$, and the run of Prim's algorithm with a Fibonacci-heap priority queue takes time $O(E'' + V'\\lg V) = O(V\\lg V)$. Thus, if we are given a minimum spanning tree of $G$, we can compute a minimum spanning tree of $G'$ in $O(V\\lg V)$ time.",
            "title": "23.2-7 $\\star$"
        },
        {
            "location": "/Chap23/23.2/#232-8",
            "text": "Professor Borden proposes a new divide-and-conquer algorithm for computing minimum spanning trees, which goes as follows. Given a graph $G = (V, E)$, partition the set $V$ of vertices into two sets $V_1$ and $V_2$ such that $|V_1|$ and $|V_2|$ differ by at most $1$. Let $E_1$ be the set of edges that are incident only on vertices in $V_1$, and let $E_2$ be the set of edges that are incident only on vertices in $V_2$. Recursively solve a minimum-spanning-tree problem on each of the two subgraphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$. Finally, select the minimum-weight edge in $E$ that crosses the cut $(V_1, V_2)$, and use this edge to unite the resulting two minimum spanning trees into a single spanning tree.  Either argue that the algorithm correctly computes a minimum spanning tree of $G$, or provide an example for which the algorithm fails.   The algorithm fails. Suppose $E = \\{(u, v), (u, w), (v, w)\\}$, the weight of $(u, v)$ and $(u, w)$ is $1$, and the weight of $(v, w)$ is $1000$, partition the set into two sets $V_1 = \\{u\\}$ and $V_2 = \\{v, w\\}$.",
            "title": "23.2-8"
        },
        {
            "location": "/Chap23/Problems/23-1/",
            "text": "Let $G = (V, E)$ be an undirected, connected graph whose weight function is $w: E \\rightarrow \\mathbb R$, and suppose that $|E| \\ge |V|$ and all edge weights are distinct.\n\n\nWe define a second-best minimum spanning tree as follows. Let $\\mathcal T$ be the set of all spanning trees of $G$, and let $T'$ be a minimum spanning tree of $G$. Then a \nsecond-best minimum spanning tree\n is a spanning tree $T$ such that $W(T) = \\min_{T'' \\in \\mathcal T - \\{T'\\}} \\{w(T'')\\}$.\n\n\na.\n Show that the minimum spanning tree is unique, but that the second-best minimum spanning tree need not be unique.\n\n\nb.\n Let $T$ be the minimum spanning tree of $G$. Prove that $G$ contains edges $(u, v) \\in T$ and $(x, y) \\notin T$ such that $T - \\{(u, v)\\} \\cup \\{(x, y)\\}$ is a second-best minimum spanning tree of $G$.\n\n\nc.\n Let $T$ be a spanning tree of $G$ and, for any two vertices $u, v \\in V$, let $max[u, v]$ denote an edge of maximum weight on the unique simple path between $u$ and $v$ in $T$. Describe an $O(V^2)$-time algorithm that, given $T$, computes $max[u, v]$ for all $u, v \\in V$.\n\n\nd.\n Give an efficient algorithm to compute the second-best minimum spanning tree of $G$.\n\n\n\n\na.\n To see that the minimum spanning tree is unique, observe that since the graph is connected and all edge weights are distinct, then there is a unique light edge crossing every cut. By Exercise 23.1-6, the minimum spanning tree is unique.\n\n\nTo see that the second-best minimum spanning tree need not be unique, here is a weighted, undirected graph with a unique minimum spanning tree of weight $7$ and two second-best minimum spanning trees of weight $8$:\n\n\n\n\nb.\n Since any spanning tree has exactly $|V| - 1$ edges, any second-best minimum spanning tree must have at least one edge that is not in the (best) minimum spanning tree. If a second-best minimum spanning tree has exactly one edge, say $(x, y)$, that is not in the minimum spanning tree, then it has the same set of edges as the minimum spanning tree, except that $(x, y)$ replaces some edge, say $(u, v)$, of the minimum spanning tree. In this case, $T' = T - \\{(u, v)\\} \\cup \\{(x, y)\\}$, as we wished to show.\n\n\nThus, all we need to show is that by replacing two or more edges of the minimum spanning tree, we cannot obtain a second-best minimum spanning tree. Let $T$ be the minimum spanning tree of $G$, and suppose that there exists a second-best minimum spanning tree $T'$ that differs from $T$ by two or more edges. There are at least two edges in $T - T'$, and let $(u, v)$ be the edge in $T - T'$ with minimum weight. If we were to add $(u, v)$ to $T'$, we would get a cycle $c$. This cycle contains some edge $(x, y)$ in $T' - T$ (since otherwise, $T$ would contain a cycle).\n\n\nWe claim that $w(x, y) > w(u, v)$. We prove this claim by contradiction, so let us assume that $w(x, y) < w(u, v)$. (Recall the assumption that edge weights are distinct, so that we do not have to concern ourselves with $w(x, y) = w(u, v)$.) If we add $(x, y)$ to $T$, we get a cycle $c'$, which contains some edge$(u', v')$ in $T - T'$ (since otherwise, $T'$ would contain a cycle). Therefore, the set of edges $T'' = T - \\{(u', v')\\} \\cup \\{(x, y)\\}$ forms a spanning tree, and we must also have $w(u', v') < w(x, y)$, since otherwise $T''$ would be a spanning tree with weight less than $w(T)$. Thus, $w(u', v') < w(x, y) < w(u, v)$, which contradicts our choice of $(u, v)$ as the edge in $T - T'$ of minimum weight.\n\n\nSince the edges $(u, v)$ and $(x, y)$ would be on a common cycle $c$ if we were to add $(u, v)$ to $T'$, the set of edges $T' - \\{(x, y)\\} \\cup \\{(u, v)\\}$ is a spanning tree, and its weight is less than $w(T')$. Moreover, it differs from $T$ (because it differs from $T'$ by only one edge). Thus, we have formed a spanning tree whose weight is less than $w(T')$ but is not $T$. Hence, $T'$ was not a second-best minimum spanning tree.\n\n\nc.\n We can fill in $max[u, v]$ for all $u, v \\in V$ in $O(V^2)$ time by simply doing a search from each vertex $u$, having restricted the edges visited to those of the spanning tree $T$. It doesn't matter what kind of search we do: breadth-first, depth-first, or any other kind.\n\n\nWe'll give pseudocode for both breadth-first and depth-first approaches. Each approach differs from the pseudocode given in Chapter 22 in that we don't need to compute $d$ or $f$ values, and we'll use the $max$ table itself to record whether a vertex has been visited in a given search. In particular, $max[u, v] = \\text{NIL}$ if and only if $u = v$ or we have not yet visited vertex $v$ in a search from vertex $u$. Note also that since we're visiting via edges in a spanning tree of an undirected graph, we are guaranteed that the search from each vertex $u$\u2014whether breadth-first or depth-first\u2014will visit all vertices. There will be no need to ''restart'' the search as is done in the $\\text{DFS}$ procedure of Section 22.3. Our pseudocode assumes that the adjacency list of each vertex consists only of edges in the spanning tree $T$.\n\n\nHere's the breadth-first search approach:\n\n\nBFS\n-\nFILL\n-\nMAX\n(\nG\n,\n \nT\n,\n \nw\n)\n\n    \nlet\n \nmax\n \nbe\n \na\n \nnew\n \ntable\n \nwith\n \nan\n \nentry\n \nmax\n[\nu\n,\n \nv\n]\n \nfor\n \neach\n \nu\n,\n \nv\n \n\u2208\n \nG\n.\nV\n\n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nfor\n \neach\n \nvertex\n \nv\n \n\u2208\n \nG\n.\nV\n\n            \nmax\n[\nu\n,\n \nv\n]\n \n=\n \nNIL\n\n        \nQ\n \n=\n \n\u2205\n\n        \nENQUEUE\n(\nQ\n,\n \nu\n)\n\n        \nwhile\n \nQ\n \n!=\n \n\u2205\n\n            \nx\n \n=\n \nDEQUEUE\n(\nQ\n)\n\n            \nfor\n \neach\n \nv\n \n\u2208\n \nG\n.\nAdj\n[\nx\n]\n\n                \nif\n \nmax\n[\nu\n,\n \nv\n]\n \n==\n \nNIL\n \nand\n \nv\n \n!=\n \nu\n\n                    \nif\n \nx\n \n==\n \nu\n \nor\n \nw\n(\nx\n,\n \nv\n)\n \n>\n \nmax\n[\nu\n,\n \nx\n]\n\n                        \nmax\n[\nu\n,\n \nv\n]\n \n=\n \n(\nx\n,\n \nv\n)\n\n                    \nelse\n \nmax\n[\nu\n,\n \nv\n]\n \n=\n \nmax\n[\nu\n,\n \nx\n]\n\n                    \nENQUEUE\n(\nQ\n,\n \nv\n)\n\n    \nreturn\n \nmax\n\n\n\n\n\nHere's the depth-first search approach:\n\n\nDFS\n-\nFILL\n-\nMAX\n(\nG\n,\n \nT\n,\n \nw\n)\n\n    \nlet\n \nmax\n \nbe\n \na\n \nnew\n \ntable\n \nwith\n \nan\n \nentry\n \nmax\n[\nu\n,\n \nv\n]\n \nfor\n \neach\n \nu\n,\n \nv\n \n\u2208\n \nG\n.\nV\n \n    \nfor\n \neach\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n        \nfor\n \neach\n \nvertex\n \nv\n \n\u2208\n \nG\n.\nV\n\n            \nmax\n[\nu\n,\n \nv\n]\n \n=\n \nNIL\n\n        \nDFS\n-\nFILL\n-\nMAX\n-\nVISIT\n(\nG\n,\n \nu\n,\n \nu\n,\n \nmax\n)\n\n    \nreturn\n \nmax\n\n\n\n\n\nDFS\n-\nFILL\n-\nMAX\n-\nVISIT\n(\nG\n,\n \nu\n,\n \nx\n,\n \nmax\n)\n\n    \nfor\n \neach\n \nvertex\n \nv\n \n\u2208\n \nG\n.\nAdj\n[\nx\n]\n\n        \nif\n \nmax\n[\nu\n,\n \nv\n]\n \n==\n \nNIL\n \nand\n \nv\n \n!=\n \nu\n\n            \nif\n \nx\n \n==\n \nu\n \nor\n \nw\n(\nx\n,\n \nv\n)\n \n>\n \nmax\n[\nu\n,\n \nx\n]\n\n                \nmax\n[\nu\n,\n \nv\n]\n \n=\n \n(\nx\n,\n \nv\n)\n\n            \nelse\n \nmax\n[\nu\n,\n \nv\n]\n \n=\n \nmax\n[\nu\n,\n \nx\n]\n\n            \nDFS\n-\nFILL\n-\nMAX\n-\nVISIT\n(\nG\n,\n \nu\n,\n \nv\n,\n \nmax\n)\n\n\n\n\n\nFor either approach, we are filling in $|V|$ rows of the $max$ table. Since the number of edges in the spanning tree is $|V| - 1$, each row takes $O(V)$ time to fill in. Thus, the total time to fill in the $max$ table is $O(V^2)$.\n\n\nd.\n In part (b), we established that we can find a second-best minimum spanning tree by replacing just one edge of the minimum spanning tree $T$ by some edge $(u, v)$ not in $T$. As we know, if we create spanning tree $T'$ by replacing edge $(x, y) \\in T$ by edge $(u, v) \\ne T$, then $w(T') = w(T) - w(x, y) + w(u, v)$. For a given edge $(u, v)$, the edge $(x, y) \\in T$ that minimizes $w(T')$ is the edge of maximum weight on the unique path between $u$ and $v$ in $T$. If we have already computed the $max$ table from part (c) based on $T$, then the identity of this edge is precisely what is stored in $max[u, v]$\u008d. All we have to do is determine an edge $(u, v) \\ne T$ for which $w(max[u, v]) - w(u, v)$ is minimum.\n\n\nThus, our algorithm to find a second-best minimum spanning tree goes as follows:\n\n\n\n\nCompute the minimum spanning tree $T$. Time: $O(E + V\\lg V)$, using Prim's algorithm with a Fibonacci-heap implementation of the priority queue. Since $|E| < |V|^2$, this running time is $O(V^2)$.\n\n\nGiven the minimum spanning tree $T$, compute the $max$ table, as in part (c). Time: $O(V^2)$.\n\n\nFind an edge $(u, v) \\ne T$ that minimizes $w(max[u, v]) - w(u, v)$. Time: $O(E)$, which is $O(V^2)$.\n\n\nHaving found an edge $(u, v)$ in step 3, return $T' = T - \\{max[u, v]\\} \\cup \\{(u, v)\\}$ as a second-best minimum spanning tree.\n\n\n\n\nThe total time is $O(V^2)$.",
            "title": "23-1 Second-best minimum spanning tree"
        },
        {
            "location": "/Chap23/Problems/23-2/",
            "text": "For a very sparse connected graph $G = (V, E)$, we can further improve upon the $O(E + V\\lg V)$ running time of Prim's algorithm with Fibonacci heaps by preprocessing $G$ to decrease the number of vertices before running Prim's algorithm. In particular, we choose, for each vertex $u$, the minimum-weight edge $(u, v)$ incident on $u$, and we put $(u, v)$ into the minimum spanning tree under construction. We then contract all chosen edges (see Section B.4). Rather than contracting these edges one at a time, we first identify sets of vertices that are united into the same new vertex. Then we create the graph that would have resulted from contracting these edges one at a time, but we do so by ''renaming'' edges according to the sets into which their endpoints were placed. Several edges from the original graph may be renamed the same as each other. In such a case, only one edge results, and its weight is the minimum of the weights of the corresponding original edges.\n\n\nInitially, we set the minimum spanning tree $T$ being constructed to be empty, and for each edge $(u, v) \\in E$, we initialize the attributes $(u, v).orig = (u, v)$ and $(u, v).c = w(u, v)$. We use the $orig$ attribute to reference the edge from the initial graph that is associated with an edge in the contracted graph. The $c$ attribute holds the weight of an edge, and as edges are contracted, we update it according to the above scheme for choosing edge weights. The procedure $\\text{MST-REDUCE}$ takes inputs $G$ and $T$, and it returns a contracted graph $G'$ with updated attributes $orig'$ and $c'$. The procedure also accumulates edges of $G$ into the minimum spanning tree $T$.\n\n\nMST\n-\nREDUCE\n(\nG\n,\n \nT\n)\n\n    \nfor\n \neach\n \nv\n \nin\n \nG\n.\nV\n\n        \nv\n.\nmark\n \n=\n \nFALSE\n\n        \nMAKE\n-\nSET\n(\nv\n)\n\n    \nfor\n \neach\n \nu\n \nin\n \nG\n.\nV\n\n        \nif\n \nu\n.\nmark\n \n==\n \nFALSE\n\n            \nchoose\n \nv\n \nin\n \nG\n.\nAdj\n[\nu\n]\n \nsuch\n \nthat\n \n(\nu\n,\n \nv\n).\nc\n \nis\n \nminimized\n\n                \nUNION\n(\nu\n,\n \nv\n)\n\n                \nT\n \n=\n \nT\n \n\u222a\n \n{(\nu\n,\n \nv\n).\norig\n}\n\n                \nu\n.\nmark\n \n=\n \nv\n.\nmark\n \n=\n \nTRUE\n\n    \nG\n'\n.\nV\n \n=\n \n{\nFIND\n-\nSET\n(\nv\n)\n \n:\n \nv\n \n\u2208\n \nG\n.\nV\n}\n\n    \nG\n'\n.\nE\n \n=\n \n\u2205\n\n    \nfor\n \neach\n \n(\nx\n,\n \ny\n)\n \n\u2208\n \nG\n.\nE\n\n        \nu\n \n=\n \nFIND\n-\nSET\n(\nx\n)\n\n        \nv\n \n=\n \nFIND\n-\nSET\n(\ny\n)\n\n        \nif\n \n(\nu\n,\n \nv\n)\n \n\u2209\n \nin\n \nG\n'\n.\nE\n\n             \nG\n'\n.\nE\n \n=\n \nG\n'\n.\nE\n \n\u222a\n \n{(\nu\n,\n \nv\n)}\n\n             \n(\nu\n,\n \nv\n).\norig\n'\n \n=\n \n(\nx\n,\n \ny\n).\norig\n\n             \n(\nu\n,\n \nv\n).\nc\n'\n \n=\n \n(\nx\n,\n \ny\n).\nc\n\n        \nelse\n \nif\n \n(\nx\n,\n \ny\n).\nc\n \n<\n \n(\nu\n,\nv\n).\nc\n'\n\n             \n(\nu\n,\n \nv\n).\norig\n'\n \n=\n \n(\nx\n,\n \ny\n).\norig\n\n             \n(\nu\n,\n \nv\n).\nc\n'\n \n=\n \n(\nx\n,\n \ny\n).\nc\n\n    \nconstruct\n \nadjacency\n \nlists\n \nG\n'\n.\nAdj\n \nfor\n \nG\n'\n\n    \nreturn\n \nG\n'\n \nand\n \nT\n\n\n\n\n\na.\n Let $T$ be the set of edges returned by $\\text{MST-REDUCE}$, and let $A$ be the minimum spanning tree of the graph $G'$ formed by the call $\\text{MST-PRIM}(G', c', r)$, where $c'$ is the weight attribute on the edges of $G'.E$ and $r$ is any vertex in $G'.V$. Prove that $T \\cup \\{(x,y).orig': (x, y) \\in A\\}$ is a minimum spanning tree of $G$.\n\n\nb.\n Argue that $|G'.V| \\le |V| / 2$.\n\n\nc.\n Show how to implement $\\text{MST-REDUCE}$ so that it runs in $O(E)$ time. ($\\textit{Hint:}$ Use simple data structures.)\n\n\nd.\n Suppose that we run $k$ phases of $\\text{MST-REDUCE}$, using the output $G'$ produced by one phase as the input $G$ to the next phase and accumulating edges in $T$. Argue that the overall running time of the $k$ phases is $O(kE)$.\n\n\ne.\n Suppose that after running $k$ phases of $\\text{MST-REDUCE}$, as in part (d), we run Prim's algorithm by calling $\\text{MST-PRIM}(G', c', r)$, where $G'$, with weight attribute $c'$, is returned by the last phase and $r$ is any vertex in $G'.V$. Show how to pick $k$ so that the overall running time is $O(E\\lg\\lg V)$. Argue that your choice of $k$ minimizes the overall asymptotic running time.\n\n\nf.\n For what values of $|E|$ (in terms of $|V|$) does Prim's algorithm with preprocessing asymptotically beat Prim's algorithm without preprocessing?\n\n\n\n\na.\n We'll show that the edges added at each step are safe. Consider an unmarked vertex $u$. Set $S = {u}$ and let $A$ be the set of edges in the tree so far. Then the cut respects $A$, and the next edge we add is a light edge, so it is safe for $A$. Thus, every edge in $T$ before we run Prim's algorithm is safe for $T$. Any edge that Prim's would normally add at this point would have to connect two of the trees already created, and it would be chosen as minimal. Moreover, we choose exactly one between any two trees. Thus, the fact that we only have the smallest edges available to us is not a problem. The resulting tree must be minimal.\n\n\nb.\n We argue by induction on the number of vertices in $G$. We'll assume that $|V| > 1$, since otherwise $\\text{MST-REDUCE}$ will encounter an error on line 6 because there is no way to choose $v$. Let $|V| = 2$. Since $G$ is connected, there must be an edge between $u$ and $v$, and it is trivially of minimum weight. They are joined, and $|G'.V| = 1 = |V| / 2$.\n\n\nSuppose the claim holds for $|V| = n$. Let $G$ be a connected graph on $n + 1$ vertices. Then $G'.V \\le n / 2$ prior to the final vertex $v$ being examined in the for-loop of line 4. If $v$ is marked then we're done, and if $v$ isn't marked then we'll connect it to some other vertex, which must be marked since $v$ is the last to be processed.\n\n\nEither way, $v$ can't contribute an additional vertex to $G'.V$. so\n\n\n$$|G'.V| \\le n / 2 \\le (n + 1) / 2.$$\n\n\nc.\n Rather than using the disjoint set structures of chapter 21, we can simply use an array to keep track of which component a vertex is in. Let $A$ be an array of length $|V|$ such that $A[u] = v$ if $v = \\text{FIND-SET}(u)$. Then $\\text{FIND-SET}(u)$ can now be replaced with $A[u]$ and $\\text{UNION}(u, v)$ can be replaced by $A[v] = A[u]$. Since these operations run in constant time, the runtime is $O(E)$.\n\n\nd.\n The number of edges in the output is monotonically decreasing, so each call is $O(E)$. Thus, $k$ calls take $O(kE)$ time.\n\n\ne.\n The runtime of Prim's algorithm is $O(E + V\\lg V)$. Each time we run $\\text{MST-REDUCE}$, we cut the number of vertices at least in half. Thus, after $k$ calls, the number of vertices is at most $|V| / 2^k$. We need to minimize\n\n\n$$E + V / 2^k\\lg(V / 2^k) + kE = E + \\frac{V\\lg V}{2^k} - \\frac{Vk}{2^k} + kE$$\n\n\nwith respect to $k$. If we choose $k = \\lg\\lg V$ then we achieve the overall running time of $O(E\\lg\\lg V)$ as desired. \n\n\nTo see that this value of $k$ minimizes, note that the $\\frac{Vk}{2^k}$ term is always less than the $kE$ term since $E \\ge V$. As $k$ decreases, the contribution of $kE$ decreases, and the contribution of $\\frac{V\\lg V}{2^k}$ increases. Thus, we need to find the value of $k$ which makes them approximately equal in the worst case, when $E = V$. To do this, we set $\\frac{\\lg V}{2^k} = k$. Solving this exactly would involve the Lambert W function, but the nicest elementary function which gets close is $k = \\lg\\lg V$.\n\n\nf.\n We simply set up the inequality\n\n\n$$E\\lg\\lg V < E + V\\lg V$$\n\n\nto find that we need\n\n\n$$E < \\frac{V\\lg V}{\\lg\\lg V-1} = O(\\frac{V\\lg V}{\\lg\\lg V}).$$",
            "title": "23-2 Minimum spanning tree in sparse graphs"
        },
        {
            "location": "/Chap23/Problems/23-3/",
            "text": "A bottleneck spanning tree $T$ of an undirected graph $G$ is a spanning tree of $G$ whose largest edge weight is minimum over all spanning trees of $G$. We say that the value of the bottleneck spanning tree is the weight of the maximum-weight edge in $T$.\n\n\na.\n Argue that a minimum spanning tree is a bottleneck spanning tree. \n\n\nPart (a) shows that finding a bottleneck spanning tree is no harder than finding a minimum spanning tree. In the remaining parts, we will show how to find a bottleneck spanning tree in linear time.\n\n\nb.\n Give a linear-time algorithm that given a graph $G$ and an integer $b$, determines whether the value of the bottleneck spanning tree is at most $b$.\n\n\nc.\n Use your algorithm for part (b) as a subroutine in a linear-time algorithm for the bottleneck-spanning-tree problem. ($\\textit{Hint:}$ You may want to use a subroutine that contracts sets of edges, as in the $\\text{MST-REDUCE}$ procedure described in Problem 23-2.)\n\n\n\n\na.\n To see that every minimum spanning tree is also a bottleneck spanning tree. Suppose that $T$ is a minimum spanning tree. Suppose there is some edge in it $(u, v)$ that has a weight that's greater than the weight of the bottleneck spanning tree. Then, let $V_1$ be the subset of vertices of $V$ that are reachable from $u$ in $T$, without going though $v$. Define $V_2$ symmetrically. Then, consider the cut that separates $V_1$ from $V_2$. The only edge that we could add across this cut is the one of minimum weight, so we know that there are no edge across this cut of weight less than $w(u, v)$.\n\n\nHowever, we have that there is a bottleneck spanning tree with less than that weight. This is a contradiction because a bottleneck spanning tree, since it is a spanning tree, must have an edge across this cut.\n\n\nb.\n To do this, we first process the entire graph, and remove any edges that have weight greater than $b$. If the remaining graph is selected, we can just arbitrarily select any tree in it, and it will be a bottleneck spanning tree of weight at most $b$. Testing connectivity of a graph can be done in linear time by running a breadth first search and then making sure that no vertices remain white at the end.\n\n\nc.\n Write down all of the edge weights of vertices. Use the algorithm from section 9.3 to find the median of this list of numbers in time $O(E)$. Then, run the procedure from part b with this median value as the one that you are testing for there to be a bottleneck spanning tree with weight at most. Then there are two cases: \n\n\nFirst, we could have that there is a bottleneck spanning tree with weight at most this median. Then just throw the edges with weight more than the median, and repeat the procedure on this new graph with half the edges.\n\n\nSecond, we could have that there is no bottleneck spanning tree with at most that weight. Then, we should run the procedure from problem 23-2 to contract all of the edges that have weight at most this median weight. This takes time $O(E\\lg\\lg V)$ and then we are left solving the problem on a graph that now has half the vertices.",
            "title": "23-3 Bottleneck spanning tree"
        },
        {
            "location": "/Chap23/Problems/23-4/",
            "text": "In this problem, we give pseudocode for three different algorithms. Each one takes a connected graph and a weight function as input and returns a set of edges $T$. For each algorithm, either prove that $T$ is a minimum spanning tree or prove that $T$ is not a minimum spanning tree. Also describe the most efficient implementation of each algorithm, whether or not it computes a minimum spanning tree.\n\n\na.\n \n\n\nMAYBE\n-\nMST\n-\nA\n(\nG\n,\n \nw\n)\n\n    \nsort\n \nthe\n \nedges\n \ninto\n \nnonincreasing\n \norder\n \nof\n \nedge\n \nweights\n \nw\n\n    \nT\n \n=\n \nE\n\n    \nfor\n \neach\n \nedge\n \ne\n,\n \ntaken\n \nin\n \nnonincreasing\n \norder\n \nby\n \nweight\n\n        \nif\n \nT\n \n-\n \n{\ne\n}\n \nis\n \na\n \nconnected\n \ngraph\n\n            \nT\n \n=\n \nT\n \n-\n \n{\ne\n}\n\n    \nreturn\n \nT\n\n\n\n\n\nb.\n \n\n\nMAYBE\n-\nMST\n-\nB\n(\nG\n,\n \nw\n)\n\n    \nT\n \n=\n \nemptyset\n\n    \nfor\n \neach\n \nedge\n \ne\n,\n \ntaken\n \nin\n \narbitrary\n \norder\n\n        \nif\n \nT\n \n\u222a\n \n{\ne\n}\n \nhas\n \nno\n \ncycles\n\n            \nT\n \n=\n \nT\n \n\u222a\n \n{\ne\n}\n\n    \nreturn\n \nT\n\n\n\n\n\nc.\n \n\n\nMAYBE\n-\nMST\n-\nC\n(\nG\n,\n \nw\n)\n\n    \nT\n \n=\n \nemptyset\n\n    \nfor\n \neach\n \nedge\n \ne\n,\n \ntaken\n \nin\n \narbitrary\n \norder\n\n        \nT\n \n=\n \nT\n \n\u222a\n \n{\ne\n}\n\n        \nif\n \nT\n \nhas\n \na\n \ncycle\n \nc\n\n            \nlet\n \ne\n'\n \nbe\n \na\n \nmaximum\n-\nweight\n \nedge\n \non\n \nc\n\n            \nT\n \n=\n \nT\n \n-\n \n{\ne\n}\n\n    \nreturn\n \nT\n\n\n\n\n\n\n\na.\n This does return an $\\text{MST}$. To see this, we'll show that we never remove an edge which must be part of a minimum spanning tree. If we remove $e$, then $e$ cannot be a bridge, which means that e lies on a simple cycle of the graph. Since we remove edges in nonincreasing order, the weight of every edge on the cycle must be less than or equal to that of $e$. By exercise 23.1-5, there is a minimum spanning tree on $G$ with edge $e$ removed.\n\n\nTo implement this, we begin by sorting the edges in $O(E \\lg E)$ time. For each edge we need to check whether or not $T - {e}$ is connected, so we'll need to run a $\\text{DFS}$. Each one takes $O(V + E)$, so doing this for all edges takes $O(E(V + E))$. This dominates the running time, so the total time is $O(E^2)$.\n\n\nb.\n This doesn't return an $\\text{MST}$. To see this, let $G$ be the graph on 3 vertices $a$, $b$, and $c$. Let the eges be $(a, b)$, $(b, c)$, and $(c, a)$ with weights $3, 2$, and $1$ respectively. If the algorithm examines the edges in their order listed, it will take the two heaviest edges instead of the two lightest. \n\n\nAn efficient implementation will use disjoint sets to keep track of connected components, as in $\\text{MST-REDUCE}$ in problem 23-2. Trying to union within the same component will create a cycle. Since we make $|V|$ calls to $\\text{MAKESET}$ and at most $3|E|$ calls to $\\text{FIND-SET}$ and $\\text{UNION}$, the runtime is $O(E\\alpha(V))$.\n\n\nc.\n This does return an $\\text{MST}$. To see this, we simply quote the result from exercise 23.1-5. The only edges we remove are the edges of maximum weight on some cycle, and there always exists a minimum spanning tree which doesn't include these edges. Moreover, if we remove an edge from every cycle then the resulting graph cannot have any cycles, so it must be a tree.\n\n\nTo implement this, we use the approach taken in part (b), except now we also need to find the maximum weight edge on a cycle. For each edge which introduces a cycle we can perform a $\\text{DFS}$ to find the cycle and max weight edge. Since the tree at that time has at most one cycle, it has at most $|V|$ edges, so we can run $\\text{DFS}$ in $O(V)$. The runtime is thus $O(EV)$.",
            "title": "23-4 Alternative minimum-spanning-tree algorithms"
        },
        {
            "location": "/Chap24/24.1/",
            "text": "24.1-1\n\n\n\n\nRun the Bellman-Ford algorithm on the directed graph of Figure 24.4, using vertex $z$ as the source. In each pass, relax edges in the same order as in the figure, and show the $d$ and $\\pi$ values after each pass. Now, change the weight of edge $(z, x)$ to $4$ and run the algorithm again, using $s$ as the source.\n\n\n\n\n\\begin{array}{c|ccccc}\n  & s & t & x & y & z \\\\\n\\hline\nd & 2 & 4 & 6 & 9 & 0 \\\\\n\\pi & z & x & y & z & \\text{NIL}\n\\end{array}\n\n\n\\begin{array}{c|ccccc}\n  & s & t & x & y & z \\\\\n\\hline\nd & 0 & 0 & 2 & 7 & -2 \\\\\n\\pi & \\text{NIL} & x & z & s & t\n\\end{array}\n\n\n24.1-2\n\n\n\n\nProve Corollary 24.3.\n\n\n\n\nSuppose there is a path from $s$ to $v$. Then there must be a shortest such path of length $\\delta(s, v)$. It must have finite length since it contains at most $|V| - 1$ edges and each edge has finite length. By Lemma 24.2, $v.d = \\delta(s, v) < \\infty$ upon termination. \n\n\nOn the other hand, suppose $v.d < \\infty$ when $\\text{BELLMAN-FORD}$ terminates. Recall that $v.d$ is monotonically decreasing throughout the algorithm, and $\\text{RELAX}$ will update $v.d$ only if $u.d + w(u, v) < v.d$ for some $u$ adjacent to $v$. Moreover, we update $v.\\pi = u$ at this point, so $v$ has an ancestor in the predecessor subgraph. Since this is a tree rooted at $s$, there must be a path from $s$ to $v$ in this tree. Every edge in the tree is also an edge in $G$, so there is also a path in $G$ from $s$ to $v$.\n\n\n24.1-3\n\n\n\n\nGiven a weighted, directed graph $G = (V, E)$ with no negative-weight cycles, let $m$ be the maximum over all vertices $v \\in V$ of the minimum number of edges in a shortest path from the source $s$ to $v$. (Here, the shortest path is by weight, not the number of edges.) Suggest a simple change to the Bellman-Ford algorithm that allows it to terminate in $m + 1$ passes, even if $m$ is not known in advance.\n\n\n\n\nIf the greatest number of edges on any shortest path from the source is $m$, then the path-relaxation property tells us that after $m$ iterations of $\\text{BELLMAN-FORD}$, every vertex $v$ has achieved its shortest-path weight in $v.d$. By the upper-bound property, after $m$ iterations, no $d$ values will ever change. Therefore, no $d$ values will change in the $(m + 1)$st iteration. Because we do not know $m$ in advance, we cannot make the algorithm iterate exactly $m$ times and then terminate. But if we just make the algorithm stop when nothing changes any more, it will stop after $m + 1$ iterations.\n\n\nBELLMAN\n-\nFORD\n-\n(\nM\n \n+\n \n1\n)(\nG\n,\n \nw\n,\n \ns\n)\n\n    \nINITIALIZE\n-\nSINGLE\n-\nSOURCE\n(\nG\n,\n \ns\n)\n\n    \nchanges\n \n=\n \nTRUE\n\n    \nwhile\n \nchanges\n \n==\n \nTRUE\n\n        \nchanges\n \n=\n \nFALSE\n\n        \nfor\n \neach\n \nedge\n(\nu\n,\n \nv\n)\n \n\u2208\n \nG\n.\nE\n\n            \nRELAX\n-\nM\n(\nu\n,\n \nv\n,\n \nw\n)\n\n\n\n\n\nRELAX\n-\nM\n(\nu\n,\n \nv\n,\n \nw\n)\n\n    \nif\n \nv\n.\nd\n \n>\n \nu\n.\nd\n \n+\n \nw\n(\nu\n,\n \nv\n)\n\n        \nv\n.\nd\n \n=\n \nu\n.\nd\n \n+\n \nw\n(\nu\n,\n \nv\n)\n\n        \nv\n.\nPI\n \n=\n \nu\n\n        \nchanges\n \n=\n \nTRUE\n\n\n\n\n\nThe test for a negative-weight cycle (based on there being a $d$ value that would change if another relaxation step was done) has been removed above, because this version of the algorithm will never get out of the \nwhile\n loop unless all $d$ values stop changing.\n\n\n24.1-4\n\n\n\n\nModify the Bellman-Ford algorithm so that it sets $v.d$ to $-\\infty$ for all vertices $v$ for which there is a negative-weight cycle on some path from the source to $v$.\n\n\n\n\nIn particular, replace line 7 with $v.d = -\\infty$.\n\n\nif\n \nv\n.\nd\n \n>\n \nu\n.\nd\n \n+\n \nw\n(\nu\n,\n \nv\n)\n\n    \nv\n.\nd\n \n=\n \n-\n\u221e\n\n\n\n\n\n24.1-5 $\\star$\n\n\n\n\nLet $G = (V, E)$ be a weighted, directed graph with weight function $w : E \\rightarrow \\mathbb R$. Give an $O(VE)$-time algorithm to find, for each vertex $v \\in V$, the value $\\delta^*(v) = \\min_{u \\in V} {\\delta(u, v)}$.\n\n\n\n\nRELAX\n(\nu\n,\n \nv\n,\n \nw\n)\n\n    \nif\n \nv\n.\nd\n \n>\n \nmin\n(\nw\n(\nu\n,\n \nv\n),\n \nw\n(\nu\n,\n \nv\n)\n \n+\n \nu\n.\nd\n)\n\n        \nv\n.\nd\n \n>\n \nmin\n(\nw\n(\nu\n,\n \nv\n),\n \nw\n(\nu\n,\n \nv\n)\n \n+\n \nu\n.\nd\n)\n\n        \nv\n.\nPI\n \n=\n \nu\n.\nPI\n\n\n\n\n\n24.1-6 $\\star$\n\n\n\n\nSuppose that a weighted, directed graph $G = (V, E)$ has a negative-weight cycle. Give an efficient algorithm to list the vertices of one such cycle. Prove that your algorithm is correct.\n\n\n\n\nBased on exercise 24.1-4, $\\text{DFS}$ from a vertex $u$ that $u.d = -\\infty$, if the weight sum on the search path is negative and the next vertex is $\\text{BLACK}$, then the search path forms a negative-weight cycle.",
            "title": "24.1 The Bellman-Ford algorithm"
        },
        {
            "location": "/Chap24/24.1/#241-1",
            "text": "Run the Bellman-Ford algorithm on the directed graph of Figure 24.4, using vertex $z$ as the source. In each pass, relax edges in the same order as in the figure, and show the $d$ and $\\pi$ values after each pass. Now, change the weight of edge $(z, x)$ to $4$ and run the algorithm again, using $s$ as the source.   \\begin{array}{c|ccccc}\n  & s & t & x & y & z \\\\\n\\hline\nd & 2 & 4 & 6 & 9 & 0 \\\\\n\\pi & z & x & y & z & \\text{NIL}\n\\end{array}  \\begin{array}{c|ccccc}\n  & s & t & x & y & z \\\\\n\\hline\nd & 0 & 0 & 2 & 7 & -2 \\\\\n\\pi & \\text{NIL} & x & z & s & t\n\\end{array}",
            "title": "24.1-1"
        },
        {
            "location": "/Chap24/24.1/#241-2",
            "text": "Prove Corollary 24.3.   Suppose there is a path from $s$ to $v$. Then there must be a shortest such path of length $\\delta(s, v)$. It must have finite length since it contains at most $|V| - 1$ edges and each edge has finite length. By Lemma 24.2, $v.d = \\delta(s, v) < \\infty$ upon termination.   On the other hand, suppose $v.d < \\infty$ when $\\text{BELLMAN-FORD}$ terminates. Recall that $v.d$ is monotonically decreasing throughout the algorithm, and $\\text{RELAX}$ will update $v.d$ only if $u.d + w(u, v) < v.d$ for some $u$ adjacent to $v$. Moreover, we update $v.\\pi = u$ at this point, so $v$ has an ancestor in the predecessor subgraph. Since this is a tree rooted at $s$, there must be a path from $s$ to $v$ in this tree. Every edge in the tree is also an edge in $G$, so there is also a path in $G$ from $s$ to $v$.",
            "title": "24.1-2"
        },
        {
            "location": "/Chap24/24.1/#241-3",
            "text": "Given a weighted, directed graph $G = (V, E)$ with no negative-weight cycles, let $m$ be the maximum over all vertices $v \\in V$ of the minimum number of edges in a shortest path from the source $s$ to $v$. (Here, the shortest path is by weight, not the number of edges.) Suggest a simple change to the Bellman-Ford algorithm that allows it to terminate in $m + 1$ passes, even if $m$ is not known in advance.   If the greatest number of edges on any shortest path from the source is $m$, then the path-relaxation property tells us that after $m$ iterations of $\\text{BELLMAN-FORD}$, every vertex $v$ has achieved its shortest-path weight in $v.d$. By the upper-bound property, after $m$ iterations, no $d$ values will ever change. Therefore, no $d$ values will change in the $(m + 1)$st iteration. Because we do not know $m$ in advance, we cannot make the algorithm iterate exactly $m$ times and then terminate. But if we just make the algorithm stop when nothing changes any more, it will stop after $m + 1$ iterations.  BELLMAN - FORD - ( M   +   1 )( G ,   w ,   s ) \n     INITIALIZE - SINGLE - SOURCE ( G ,   s ) \n     changes   =   TRUE \n     while   changes   ==   TRUE \n         changes   =   FALSE \n         for   each   edge ( u ,   v )   \u2208   G . E \n             RELAX - M ( u ,   v ,   w )   RELAX - M ( u ,   v ,   w ) \n     if   v . d   >   u . d   +   w ( u ,   v ) \n         v . d   =   u . d   +   w ( u ,   v ) \n         v . PI   =   u \n         changes   =   TRUE   The test for a negative-weight cycle (based on there being a $d$ value that would change if another relaxation step was done) has been removed above, because this version of the algorithm will never get out of the  while  loop unless all $d$ values stop changing.",
            "title": "24.1-3"
        },
        {
            "location": "/Chap24/24.1/#241-4",
            "text": "Modify the Bellman-Ford algorithm so that it sets $v.d$ to $-\\infty$ for all vertices $v$ for which there is a negative-weight cycle on some path from the source to $v$.   In particular, replace line 7 with $v.d = -\\infty$.  if   v . d   >   u . d   +   w ( u ,   v ) \n     v . d   =   - \u221e",
            "title": "24.1-4"
        },
        {
            "location": "/Chap24/24.1/#241-5-star",
            "text": "Let $G = (V, E)$ be a weighted, directed graph with weight function $w : E \\rightarrow \\mathbb R$. Give an $O(VE)$-time algorithm to find, for each vertex $v \\in V$, the value $\\delta^*(v) = \\min_{u \\in V} {\\delta(u, v)}$.   RELAX ( u ,   v ,   w ) \n     if   v . d   >   min ( w ( u ,   v ),   w ( u ,   v )   +   u . d ) \n         v . d   >   min ( w ( u ,   v ),   w ( u ,   v )   +   u . d ) \n         v . PI   =   u . PI",
            "title": "24.1-5 $\\star$"
        },
        {
            "location": "/Chap24/24.1/#241-6-star",
            "text": "Suppose that a weighted, directed graph $G = (V, E)$ has a negative-weight cycle. Give an efficient algorithm to list the vertices of one such cycle. Prove that your algorithm is correct.   Based on exercise 24.1-4, $\\text{DFS}$ from a vertex $u$ that $u.d = -\\infty$, if the weight sum on the search path is negative and the next vertex is $\\text{BLACK}$, then the search path forms a negative-weight cycle.",
            "title": "24.1-6 $\\star$"
        },
        {
            "location": "/Chap24/24.2/",
            "text": "24.2-1\n\n\n\n\nRun $\\text{DAG-SHORTEST-PATHS}$ on the directed graph of Figure 24.5, using vertex $r$ as the source.\n\n\n\n\n\n\n\n\n$d$ values:\n\n\n\\begin{array}{cccccc}\nr & s & t & x & y & z \\\\\n\\hline\n0 & \\infty & \\infty & \\infty & \\infty & \\infty \\\\\n0 & 5 & 3 & \\infty & \\infty & \\infty \\\\\n0 & 5 & 3 & 11 & \\infty & \\infty \\\\\n0 & 5 & 3 & 10 & 7 & 5 \\\\\n0 & 5 & 3 & 10 & 7 & 5 \\\\\n0 & 5 & 3 & 10 & 7 & 5\n\\end{array}\n\n\n\n\n\n\n$\\pi$ values:\n\n\n\\begin{array}{cccccc}\nr & s & t & x & y & z \\\\\n\\hline\n\\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & r & r & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & r & r & s & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & r & r & t & t & t \\\\\n\\text{NIL} & r & r & t & t & t \\\\\n\\text{NIL} & r & r & t & t & t \n\\end{array}\n\n\n\n\n\n\n24.2-2\n\n\n\n\nSuppose we change line 3 of $\\text{DAG-SHORTEST-PATHS}$ to read \n\n\n \n3\n  \nfor\n \nthe\n \nfirst\n \n|\nV\n|\n \n-\n \n1\n \nvertices\n,\n \ntaken\n \nin\n \ntopologically\n \nsorted\n \norder\n\n\n\n\n\nShow that the procedure would remain correct.\n\n\n\n\nWhen we reach vertex $v$, the last vertex in the topological sort, it must have $out\\text-degree$ $0$. Otherwise there would be an edge pointing from a later vertex to an earlier vertex in the ordering, a contradiction. Thus, the body of the for-loop of line 4 is never entered for this final vertex, so we may as well not consider it.\n\n\n24.2-3\n\n\n\n\nThe PERT chart formulation given above is somewhat unnatural. In a more natural structure, vertices would represent jobs and edges would represent sequencing constraints; that is, edge $(u, v)$ would indicate that job $u$ must be performed before job $v$. We would then assign weights to vertices, not edges. Modify the $\\text{DAG-SHORTEST-PATHS}$ procedure so that it finds a longest path in a directed acyclic graph with weighted vertices in linear time.\n\n\n\n\nInstead of modifying the $\\text{DAG-SHORTEST-PATHS}$ procedure, we'll modify the structure of the graph so that we can run $\\text{DAG-SHORTEST-PATHS}$ on it. In fact, we'll give two ways to transform a PERT chart $G = (V, E)$ with weights on vertices to a PERT chart $G' = (V', E')$ with weights on edges. In each way, we'll have that $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$. We can then run on $G'$ the same algorithm to find a longest path through a dag as is given in Section 24.2 of the text.\n\n\nIn the first way, we transform each vertex $v \\in V$ into two vertices $v'$ and $v''$ in $V'$. All edges in $E$ that enter $v$ will enter $v'$ in $E'$, and all edges in $E$ that leave $v$ will leave $v''$ in $E'$. In other words, if $(u, v) \\in E$, then $(u'', v') \\in E'$. All such edges have weight $0$. We also put edges $(v', v'')$ into $E'$ for all vertices $v \\in V$, and these edges are given the weight of the corresponding vertex $v$ in $G$. Thus, $|V'| = 2|V|$, $|E'| = |V| + |E|$, and the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$.\n\n\nIn the second way, we leave vertices in $V$ alone, but we add one new source vertex $s$ to $V'$, so that $V' = V \\cup {s}$. All edges of $E$ are in $E'$, and $E'$ also includes an edge $(s, v)$ for every vertex $v \\in V$ that has $in\\text-degree$ $0$ in $G$. Thus, the only vertex with $in\\text-degree$ $0$ in $G'$ is the new source $s$. The weight of edge $(u, v) \\in E'$ is the weight of vertex $v$ in $G$. In other words, the weight of each entering edge in $G'$ is the weight of the vertex it enters in $G$. In effect, we have ''pushed back'' the weight of each vertex onto the edges that enter it. Here, $|V'| = |V| + 1$, $|E'| \\le |V| + |E|$ (since no more than $|V|$ vertices have $in\\text-degree$ $0$ in $G$), and again the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$.\n\n\n24.2-4\n\n\n\n\nGive an efficient algorithm to count the total number of paths in a directed acyclic graph. Analyze your algorithm.\n\n\n\n\nWe will compute the total number of paths by counting the number of paths whose start point is at each vertex $v$, which will be stored in an attribute $v.paths$. Assume that initial we have $v.paths = 0$ for all $v \\in V$. Since all vertices adjacent to $u$ occur later in the topological sort and the final vertex has no neighbors, line 4 is well-defined. Topological sort takes $O(V + E)$ and the nested for-loops take $O(V + E)$ so the total runtime is $O(V + E)$.\n\n\nPATHS\n(\nG\n)\n\n    \ntopologically\n \nsort\n \nthe\n \nvertices\n \nof\n \nG\n\n    \nfor\n \neach\n \nvertex\n \nu\n,\n \ntaken\n \nin\n \nreverse\n \ntopologically\n \nsorted\n \norder\n\n        \nfor\n \neach\n \nvertex\n \nv\n \n\u2208\n \nG\n.\nAdj\n[\nu\n]\n\n            \nu\n.\npaths\n \n=\n \nu\n.\npaths\n \n+\n \n1\n \n+\n \nv\n.\npaths",
            "title": "24.2 Single-source shortest paths in directed acyclic graphs"
        },
        {
            "location": "/Chap24/24.2/#242-1",
            "text": "Run $\\text{DAG-SHORTEST-PATHS}$ on the directed graph of Figure 24.5, using vertex $r$ as the source.     $d$ values:  \\begin{array}{cccccc}\nr & s & t & x & y & z \\\\\n\\hline\n0 & \\infty & \\infty & \\infty & \\infty & \\infty \\\\\n0 & 5 & 3 & \\infty & \\infty & \\infty \\\\\n0 & 5 & 3 & 11 & \\infty & \\infty \\\\\n0 & 5 & 3 & 10 & 7 & 5 \\\\\n0 & 5 & 3 & 10 & 7 & 5 \\\\\n0 & 5 & 3 & 10 & 7 & 5\n\\end{array}    $\\pi$ values:  \\begin{array}{cccccc}\nr & s & t & x & y & z \\\\\n\\hline\n\\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & r & r & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & r & r & s & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & r & r & t & t & t \\\\\n\\text{NIL} & r & r & t & t & t \\\\\n\\text{NIL} & r & r & t & t & t \n\\end{array}",
            "title": "24.2-1"
        },
        {
            "location": "/Chap24/24.2/#242-2",
            "text": "Suppose we change line 3 of $\\text{DAG-SHORTEST-PATHS}$ to read     3    for   the   first   | V |   -   1   vertices ,   taken   in   topologically   sorted   order   Show that the procedure would remain correct.   When we reach vertex $v$, the last vertex in the topological sort, it must have $out\\text-degree$ $0$. Otherwise there would be an edge pointing from a later vertex to an earlier vertex in the ordering, a contradiction. Thus, the body of the for-loop of line 4 is never entered for this final vertex, so we may as well not consider it.",
            "title": "24.2-2"
        },
        {
            "location": "/Chap24/24.2/#242-3",
            "text": "The PERT chart formulation given above is somewhat unnatural. In a more natural structure, vertices would represent jobs and edges would represent sequencing constraints; that is, edge $(u, v)$ would indicate that job $u$ must be performed before job $v$. We would then assign weights to vertices, not edges. Modify the $\\text{DAG-SHORTEST-PATHS}$ procedure so that it finds a longest path in a directed acyclic graph with weighted vertices in linear time.   Instead of modifying the $\\text{DAG-SHORTEST-PATHS}$ procedure, we'll modify the structure of the graph so that we can run $\\text{DAG-SHORTEST-PATHS}$ on it. In fact, we'll give two ways to transform a PERT chart $G = (V, E)$ with weights on vertices to a PERT chart $G' = (V', E')$ with weights on edges. In each way, we'll have that $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$. We can then run on $G'$ the same algorithm to find a longest path through a dag as is given in Section 24.2 of the text.  In the first way, we transform each vertex $v \\in V$ into two vertices $v'$ and $v''$ in $V'$. All edges in $E$ that enter $v$ will enter $v'$ in $E'$, and all edges in $E$ that leave $v$ will leave $v''$ in $E'$. In other words, if $(u, v) \\in E$, then $(u'', v') \\in E'$. All such edges have weight $0$. We also put edges $(v', v'')$ into $E'$ for all vertices $v \\in V$, and these edges are given the weight of the corresponding vertex $v$ in $G$. Thus, $|V'| = 2|V|$, $|E'| = |V| + |E|$, and the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$.  In the second way, we leave vertices in $V$ alone, but we add one new source vertex $s$ to $V'$, so that $V' = V \\cup {s}$. All edges of $E$ are in $E'$, and $E'$ also includes an edge $(s, v)$ for every vertex $v \\in V$ that has $in\\text-degree$ $0$ in $G$. Thus, the only vertex with $in\\text-degree$ $0$ in $G'$ is the new source $s$. The weight of edge $(u, v) \\in E'$ is the weight of vertex $v$ in $G$. In other words, the weight of each entering edge in $G'$ is the weight of the vertex it enters in $G$. In effect, we have ''pushed back'' the weight of each vertex onto the edges that enter it. Here, $|V'| = |V| + 1$, $|E'| \\le |V| + |E|$ (since no more than $|V|$ vertices have $in\\text-degree$ $0$ in $G$), and again the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$.",
            "title": "24.2-3"
        },
        {
            "location": "/Chap24/24.2/#242-4",
            "text": "Give an efficient algorithm to count the total number of paths in a directed acyclic graph. Analyze your algorithm.   We will compute the total number of paths by counting the number of paths whose start point is at each vertex $v$, which will be stored in an attribute $v.paths$. Assume that initial we have $v.paths = 0$ for all $v \\in V$. Since all vertices adjacent to $u$ occur later in the topological sort and the final vertex has no neighbors, line 4 is well-defined. Topological sort takes $O(V + E)$ and the nested for-loops take $O(V + E)$ so the total runtime is $O(V + E)$.  PATHS ( G ) \n     topologically   sort   the   vertices   of   G \n     for   each   vertex   u ,   taken   in   reverse   topologically   sorted   order \n         for   each   vertex   v   \u2208   G . Adj [ u ] \n             u . paths   =   u . paths   +   1   +   v . paths",
            "title": "24.2-4"
        },
        {
            "location": "/Chap24/24.3/",
            "text": "24.3-1\n\n\n\n\nRun Dijkstra's algorithm on the directed graph of Figure 24.2, first using vertex $s$ as the source and then using vertex $z$ as the source. In the style of Figure 24.6, show the $d$ and $\\pi$ values and the vertices in set $S$ after each iteration of the \nwhile\n loop.\n\n\n\n\n\n\n\n\n$s$ as the source:\n\n\n\n\n$d$ values:\n\n\n\n\n\\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\n0 & 3 & \\infty & 5 & \\infty \\\\\n0 & 3 & 9 & 5 & \\infty \\\\\n0 & 3 & 9 & 5 & 11 \\\\\n0 & 3 & 9 & 5 & 11 \\\\\n0 & 3 & 9 & 5 & 11\n\\end{array}\n\n\n\n\n$\\pi$ values:\n\n\n\n\n\\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\n\\text{NIL} & s & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & s & t & s & \\text{NIL} \\\\\n\\text{NIL} & s & t & s & y \\\\\n\\text{NIL} & s & t & s & y \\\\\n\\text{NIL} & s & t & s & y\n\\end{array}\n\n\n\n\n\n\n$z$ as the source:\n\n\n\n\n$d$ values:\n\n\n\n\n\\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\n3 & \\infty & 7 & \\infty & 0 \\\\\n3 & 6 & 7 & 8 & 0           \\\\\n3 & 6 & 7 & 8 & 0           \\\\\n3 & 6 & 7 & 8 & 0           \\\\\n3 & 6 & 7 & 8 & 0\n\\end{array}\n\n\n\n\n$\\pi$ values:\n\n\n\n\n\\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\nz & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\\nz & s & z & s & \\text{NIL}     \\\\\nz & s & z & s & \\text{NIL}     \\\\\nz & s & z & s & \\text{NIL}     \\\\\nz & s & z & s & \\text{NIL}\n\\end{array}\n\n\n\n\n\n\n24.3-2\n\n\n\n\nGive a simple example of a directed graph with negative-weight edges for which Dijkstra's algorithm produces incorrect answers. Why doesn't the proof of Theorem 24.6 go through when negative-weight edges are allowed?\n\n\n\n\nConsider any graph with a negative cycle. $\\text{RELAX}$ is called a finite number of times but the distance to any vertex on the cycle is $-\\infty$, so Dijkstra's algorithm cannot possibly be correct here. The proof of theorem 24.6 doesn't go through because we can no longer guarantee that\n\n\n$$\\delta(s, y) \\le \\delta(s, u).$$\n\n\n24.3-3\n\n\n\n\nSuppose we change line 4 of Dijkstra's algorithm to the following. \n\n\n \n4\n  \nwhile\n \n|\nQ\n|\n \n>\n \n1\n\n\n\n\n\nThis change causes the \nwhile\n loop to execute $|V| - 1$ times instead of $|V|$ times. Is this proposed algorithm correct?\n\n\n\n\nYes, the algorithm still works. Let $u$ be the leftover vertex that does not get extracted from the priority queue $Q$. If $u$ is not reachable from $s$, then $u.d = \\delta(s, u) = \\infty$. If $u$ is reachable from $s$, then there is a shortest path $p = s \\leadsto x \\to u$. When the vertex $x$ was extracted, $x.d = \\delta(s, x)$ and then the edge $(x, u)$ was relaxed; thus, $u.d = \\delta(s, u)$.\n\n\n24.3-4\n\n\n\n\nProfessor Gaedel has written a program that he claims implements Dijkstra's algorithm. The program produces $v.d$ and $v.\\pi$ for each vertex $v \\in V$. Give an $O(V + E)$-time algorithm to check the output of the professor's program. It should determine whether the $d$ and $\\pi$ attributes match those of some shortest-paths tree. You may assume that all edge weights are nonnegative.\n\n\n\n\n\n\nVerify that $s.d = 0$ and $s.\\pi = \\text{NIL}$\n\n\nVerify that $v.d = v.\\pi + w(v.\\pi, v)$ for all $v \\in s$.\n\n\nVerify that $v.d = \\infty$ if and only if $v.\\beta = \\text{NIL}$ for all $v \\in s$.\n\n\nIf any of the above verification tests fail, declare the output to be incorrect. Otherwise, run one pass of Bellman-Ford, i.e., relax each edge $(u, v) \\in E$ one time. If any values of $v.d$ change, then declare the output to be incorrect; otherwise, declare the output to be correct.\n\n\n\n\n24.3-5\n\n\n\n\nProfessor Newman thinks that he has worked out a simpler proof of correctness for Dijkstra's algorithm. He claims that Dijkstra's algorithm relaxes the edges of every shortest path in the graph in the order in which they appear on the path, and therefore the path-relaxation property applies to every vertex reachable from the source. Show that the professor is mistaken by constructing a directed graph for which Dijkstra's algorithm could relax the edges of a shortest path out of order.\n\n\n\n\nLet the graph have vertices $s$, $x$, $y$, $z$ and edges $(s, x)$, $(x, y)$, $(y, z)$, $(s, y)$, and let every edge have weight $0$. Dijkstra's algorithm could relax edges in the order $(s, y)$, $(s, x)$, $(y, z)$, $(x, y)$. The graph has two shortest paths from $s$ to $z: \\langle s, x, y, z \\rangle$ and $\\langle s, y, z \\rangle$, both with weight $0$. The edges on the shortest path $\\langle s, x, y, z \\rangle$ are relaxed out of order, because $(x, y)$ is relaxed after $(y, z)$.\n\n\n24.3-6\n\n\n\n\nWe are given a directed graph $G = (V, E)$ on which each edge $(u, v) \\in E$ has an associated value $r(u, v)$, which is a real number in the range $0 \\le r(u, v) \\le 1$ that represents the reliability of a communication channel from vertex $u$ to vertex $v$. We interpret $r(u, v)$ as the probability that the channel from $u$ to $v$ will not fail, and we assume that these probabilities are independent. Give an efficient algorithm to find the most reliable path between two given vertices.\n\n\n\n\nTo find the most reliable path between $s$ and $t$, run Dijkstra's algorithm with edge weights $w(u, v) = -\\lg r(u, v)$ to find shortest paths from $s$ in $O(E + V\\lg V)$ time. The most reliable path is the shortest path from $s$ to $t$, and that path's reliability is the product of the reliabilities of its edges.\n\n\nHere's why this method works. Because the probabilities are independent, the probability that a path will not fail is the product of the probabilities that its edges will not fail. We want to find a path $s \\overset{p}{\\leadsto} t$ such that $\\prod_{(u, v) \\in p} r(u, v)$ is maximized. This is equivalent to maximizing $\\lg(\\prod_{(u, v) \\in p} r(u, v)) = \\sum_{(u, v) \\in p} \\lg r(u, v)$, which is in turn equivalent to minimizing $\\sum_{(u, v) \\in p} -\\lg r(u, v)$. (Note: $r(u, v)$ can be $0$, and $\\lg 0$ is undefined. So in this algorithm, define $\\lg 0 = -\\infty$.) Thus if we assign weights $w(u, v) = -\\lg r(u, v)$, we have a shortest-path problem.\n\n\nSince $\\lg 1 = 0$, $\\lg x < 0$ for $0 < x < 1$, and we have defined $\\lg 0 = -\\infty$, all the weights $w$ are nonnegative, and we can use Dijkstra's algorithm to find the shortest paths from $s$ in $O(E + V\\lg V)$ time.\n\n\nAlternative solution\n\n\nYou can also work with the original probabilities by running a modified version of Dijkstra's algorithm that maximizes the product of reliabilities along a path instead of minimizing the sum of weights along a path.\n\n\nIn Dijkstra's algorithm, use the reliabilities as edge weights and substitute\n\n\n\n\nmax (and $\\text{EXTRACT-MAX}$) for min (and $\\text{EXTRACT-MIN}$) in relaxation and the queue,\n\n\n$\\cdot$ for $+$ in relaxation, \n\n\n$1$ (identity for $\\cdot$) for $0$ (identity for $+$) and $-\\infty$ (identity for min) for $\\infty$ (identity for max).\n\n\n\n\nFor example, we would use the following instead of the usual $\\text{RELAX}$ procedure:\n\n\nRELAX\n-\nRELIABILITY\n(\nu\n,\n \nv\n,\n \nr\n)\n\n    \nif\n \nv\n.\nd\n \n<\n \nu\n.\nd\n\uff0e\nr\n(\nu\n,\n \nv\n)\n\n        \nv\n.\nd\n \n=\n \nu\n.\nd\n\uff0e\nr\n(\nu\n,\n \nv\n)\n\n        \nv\n.\nPI\n \n=\n \nu\n\n\n\n\n\nThis algorithm is isomorphic to the one above: it performs the same operations except that it is working with the original probabilities instead of the transformed ones.\n\n\n24.3-7\n\n\n\n\nLet $G = (V, E)$ be a weighted, directed graph with positive weight function $w: E \\rightarrow \\{1, 2, \\ldots, W\\}$ for some positive integer $W$, and assume that no two vertices have the same shortest-path weights from source vertex $s$. Now suppose that we define an unweighted, directed graph $G' = (V \\cup V', E')$ by replacing each edge $(u, v) \\in E$ with $w(u, v)$ unit-weight edges in series. How many vertices does $G'$ have? Now suppose that we run a breadth-first search on $G'$. Show that the order in which the breadth-first search of $G'$ colors vertices in $V$ black is the same as the order in which Dijkstra's algorithm extracts the vertices of $V$ from the priority queue when it runs on $G$.\n\n\n\n\n$V + \\sum_{(u, v) \\in E} w(u, v) - E$.\n\n\n24.3-8\n\n\n\n\nLet $G = (V, E)$ be a weighted, directed graph with nonnegative weight function $w: E \\rightarrow \\{0, 1, \\ldots, W\\}$ for some nonnegative integer $W$. Modify Dijkstra's algorithm to compute the shortest paths from a given source vertex s in $O(WV + E)$ time.\n\n\n\n\nObserve that if a shortest-path estimate is not $\\infty$, then it's at most $(|V| - 1)W$. Why? In order to have $v.d < 1$, we must have relaxed an edge $(u, v)$ with $u.d < \\infty$. By induction, we can show that if we relax $(u, v)$, then $v.d$ is at most the number of edges on a path from $s$ to $v$ times the maximum edge weight. Since any acyclic path has at most $|V| - 1$ edges and the maximum edge weight is $W$, we see that $v.d \\le (|V| - 1)W$. Note also that $v.d$ must also be an integer, unless it is $\\infty$.\n\n\nWe also observe that in Dijkstra's algorithm, the values returned by the $\\text{EXTRACT-MIN}$ calls are monotonically increasing over time. Why? After we do our initial $|V|$ $\\text{INSERT}$ operations, we never do another. The only other way that a key value can change is by a $\\text{DECREASE-KEY}$ operation. Since edge weights are nonnegative, when we relax an edge $(u, v)$, we have that $u.d \\le v.d$. Since $u$ is the minimum vertex that we just extracted, we know that any other vertex we extract later has a key value that is at least $u.d$.\n\n\nWhen keys are known to be integers in the range $0$ to $k$ and the key values extracted are monotonically increasing over time, we can implement a min-priority queue so that any sequence of $m$ $\\text{INSERT}$, $\\text{EXTRACT-MIN}$, and $\\text{DECREASE-KEY}$ operations takes $O(m + k)$ time. Here's how. We use an array, say $A[0..k]$\u008d, where $A[j]$ is a linked list of each element whose key is $j$. Think of $A[j]$ as a bucket for all elements with key $j$. We implement each bucket by a circular, doubly linked list with a sentinel, so that we can insert into or delete from each bucket in $O(1)$ time. We perform the min-priority queue operations as follows:\n\n\n\n\n$\\text{INSERT}$: To insert an element with key $j$, just insert it into the linked list in $A[j]$\u008d. Time: $O(1)$ per $\\text{INSERT}$.\n\n\n$\\text{EXTRACT-MIN}$: We maintain an index $min$ of the value of the smallest key extracted. Initially, $min$ is $0$. To find the smallest key, look in $A[min]$ and, if this list is nonempty, use any element in it, removing the element from the list and returning it to the caller. Otherwise, we rely on the monotonicity property and increment $min$ until we either find a list $A[min]$ that is nonempty (using any element in $A[min]$ as before) or we run off the end of the array $A$ (in which case the min-priority queue is empty).\n\n    Since there are at most $m$ $\\text{INSERT}$ operations, there are at most $m$ elements in the min-priority queue. We increment $min$ at most $k$ times, and we remove and return some element at most $m$ times. Thus, the total time over all $\\text{EXTRACT-MIN}$ operations is $O(m + k)$.  \n\n\n$\\text{DECREASE-KEY}$: To decrease the key of an element from $j$ to $i$, first check whether $i \\le j$, \ufb02agging an error if not. Otherwise, we remove the element from its list $A[j]$ in $O(1)$ time and insert it into the list $A[i]$ in $O(1)$ time. Time: $O(1)$ per $\\text{DECREASE-KEY}$.\n\n    To apply this kind of min-priority queue to Dijkstra's algorithm, we need to let $k = (|V| - 1)W$, and we also need a separate list for keys with value $\\infty$. The number of operations $m$ is $O(V + E)$ (since there are $|V|$ $\\text{INSERT}$ and $|V|$ $\\text{EXTRACT-MIN}$ operations and at most $|E|$ $\\text{DECREASE-KEY}$ operations), and so the total time is $O(V + E + VW) = O(VW + E)$.\n\n\n\n\n24.3-9\n\n\n\n\nModify your algorithm from Exercise 24.3-8 to run in $O((V + E) \\lg W)$ time. ($\\textit{Hint:}$ How many distinct shortest-path estimates can there be in $V - S$ at any point in time?)\n\n\n\n\nFirst, observe that at any time, there are at most $W + 2$ distinct key values in the priority queue. Why? A key value is either $1$ or it is not. Consider what happens whenever a key value $v.d$ becomes finite. It must have occurred due to the relaxation of an edge $(u, v)$. At that time, $u$ was being placed into $S$, and $u.d \\le y.d$ for all vertices $y \\in V - S$. After relaxing edge $(u, v)$, we have $v.d \\le u.d + W$. Since any other vertex $y \\in V - S$ with $y.d < \\infty$ also had its estimate changed by a relaxation of some edge $x$ with $x.d \\le u.d$, we must have $y.d \\le x.d + W \\le u.d + W$. Thus, at the time that we are relaxing edges from a vertex $u$, we must have, for all vertices $v \\in V - S$, that $u.d \\le v.d \\le u.d + W$ or $v.d = \\infty$. Since shortest-path estimates are integer values (except for $\\infty$), at any given moment we have at most $W + 2$ different ones: $u.d$, $u.d + 1$, $u.d + 2$, $\\ldots$, $u.d + W$ and $\\infty$.\n\n\nTherefore, we can maintain the min-priorty queue as a binary min-heap in which each node points to a doubly linked list of all vertices with a given key value. There are at most $W + 2$ nodes in the heap, and so $\\text{EXTRACT-MIN}$ runs in $O(\\lg W)$ time. To perform $\\text{DECREASE-KEY}$, we need to be able to find the heap node corresponding to a given key in $O(\\lg W)$ time. We can do so in $O(1)$ time as follows. First, keep a pointer $inf$ to the node containing all the $\\infty$ keys. Second, maintain an array $loc[0..W]$\u008d, where $loc[i]$ points to the unique heap entry whose key value is congruent to $i(\\mod(W + 1))$. As keys move around in the heap, we can update this array in $O(1)$ time per movement.\n\n\nAlternatively, instead of using a binary min-heap, we could use a red-black tree. Now $\\text{INSERT}$, $\\text{DELETE}$, $\\text{MINIMUM}$, and $\\text{SEARCH}$\u2014from which we can construct the priority-queue operations\u2014each run in $O(\\lg W)$ time.\n\n\n24.3-10\n\n\n\n\nSuppose that we are given a weighted, directed graph $G = (V, E)$ in which edges that leave the source vertex $s$ may have negative weights, all other edge weights are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra's algorithm correctly finds shortest paths from $s$ in this graph.\n\n\n\n\nThe proof of correctness, Theorem 24.6, goes through exactly as stated in the text. The key fact was that $\\delta(s, y) \\le \\delta(s, u)$. It is claimed that this holds because there are no negative edge weights, but in fact that is stronger than is needed. This always holds if $y$ occurs on a shortest path from $s$ to $u$ and $y \\ne s$ because all edges on the path from $y$ to $u$ have nonnegative weight. If any had negative weight, this would imply that we had ''gone back'' to an edge incident with $s$, which implies that a cycle is involved in the path, which would only be the case if it were a negative-weight cycle. However, these are still forbidden.",
            "title": "24.3 Dijkstra's algorithm"
        },
        {
            "location": "/Chap24/24.3/#243-1",
            "text": "Run Dijkstra's algorithm on the directed graph of Figure 24.2, first using vertex $s$ as the source and then using vertex $z$ as the source. In the style of Figure 24.6, show the $d$ and $\\pi$ values and the vertices in set $S$ after each iteration of the  while  loop.     $s$ as the source:   $d$ values:   \\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\n0 & 3 & \\infty & 5 & \\infty \\\\\n0 & 3 & 9 & 5 & \\infty \\\\\n0 & 3 & 9 & 5 & 11 \\\\\n0 & 3 & 9 & 5 & 11 \\\\\n0 & 3 & 9 & 5 & 11\n\\end{array}   $\\pi$ values:   \\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\n\\text{NIL} & s & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\\n\\text{NIL} & s & t & s & \\text{NIL} \\\\\n\\text{NIL} & s & t & s & y \\\\\n\\text{NIL} & s & t & s & y \\\\\n\\text{NIL} & s & t & s & y\n\\end{array}    $z$ as the source:   $d$ values:   \\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\n3 & \\infty & 7 & \\infty & 0 \\\\\n3 & 6 & 7 & 8 & 0           \\\\\n3 & 6 & 7 & 8 & 0           \\\\\n3 & 6 & 7 & 8 & 0           \\\\\n3 & 6 & 7 & 8 & 0\n\\end{array}   $\\pi$ values:   \\begin{array}{ccccc}\ns & t & x & y & z \\\\\n\\hline\nz & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\\nz & s & z & s & \\text{NIL}     \\\\\nz & s & z & s & \\text{NIL}     \\\\\nz & s & z & s & \\text{NIL}     \\\\\nz & s & z & s & \\text{NIL}\n\\end{array}",
            "title": "24.3-1"
        },
        {
            "location": "/Chap24/24.3/#243-2",
            "text": "Give a simple example of a directed graph with negative-weight edges for which Dijkstra's algorithm produces incorrect answers. Why doesn't the proof of Theorem 24.6 go through when negative-weight edges are allowed?   Consider any graph with a negative cycle. $\\text{RELAX}$ is called a finite number of times but the distance to any vertex on the cycle is $-\\infty$, so Dijkstra's algorithm cannot possibly be correct here. The proof of theorem 24.6 doesn't go through because we can no longer guarantee that  $$\\delta(s, y) \\le \\delta(s, u).$$",
            "title": "24.3-2"
        },
        {
            "location": "/Chap24/24.3/#243-3",
            "text": "Suppose we change line 4 of Dijkstra's algorithm to the following.     4    while   | Q |   >   1   This change causes the  while  loop to execute $|V| - 1$ times instead of $|V|$ times. Is this proposed algorithm correct?   Yes, the algorithm still works. Let $u$ be the leftover vertex that does not get extracted from the priority queue $Q$. If $u$ is not reachable from $s$, then $u.d = \\delta(s, u) = \\infty$. If $u$ is reachable from $s$, then there is a shortest path $p = s \\leadsto x \\to u$. When the vertex $x$ was extracted, $x.d = \\delta(s, x)$ and then the edge $(x, u)$ was relaxed; thus, $u.d = \\delta(s, u)$.",
            "title": "24.3-3"
        },
        {
            "location": "/Chap24/24.3/#243-4",
            "text": "Professor Gaedel has written a program that he claims implements Dijkstra's algorithm. The program produces $v.d$ and $v.\\pi$ for each vertex $v \\in V$. Give an $O(V + E)$-time algorithm to check the output of the professor's program. It should determine whether the $d$ and $\\pi$ attributes match those of some shortest-paths tree. You may assume that all edge weights are nonnegative.    Verify that $s.d = 0$ and $s.\\pi = \\text{NIL}$  Verify that $v.d = v.\\pi + w(v.\\pi, v)$ for all $v \\in s$.  Verify that $v.d = \\infty$ if and only if $v.\\beta = \\text{NIL}$ for all $v \\in s$.  If any of the above verification tests fail, declare the output to be incorrect. Otherwise, run one pass of Bellman-Ford, i.e., relax each edge $(u, v) \\in E$ one time. If any values of $v.d$ change, then declare the output to be incorrect; otherwise, declare the output to be correct.",
            "title": "24.3-4"
        },
        {
            "location": "/Chap24/24.3/#243-5",
            "text": "Professor Newman thinks that he has worked out a simpler proof of correctness for Dijkstra's algorithm. He claims that Dijkstra's algorithm relaxes the edges of every shortest path in the graph in the order in which they appear on the path, and therefore the path-relaxation property applies to every vertex reachable from the source. Show that the professor is mistaken by constructing a directed graph for which Dijkstra's algorithm could relax the edges of a shortest path out of order.   Let the graph have vertices $s$, $x$, $y$, $z$ and edges $(s, x)$, $(x, y)$, $(y, z)$, $(s, y)$, and let every edge have weight $0$. Dijkstra's algorithm could relax edges in the order $(s, y)$, $(s, x)$, $(y, z)$, $(x, y)$. The graph has two shortest paths from $s$ to $z: \\langle s, x, y, z \\rangle$ and $\\langle s, y, z \\rangle$, both with weight $0$. The edges on the shortest path $\\langle s, x, y, z \\rangle$ are relaxed out of order, because $(x, y)$ is relaxed after $(y, z)$.",
            "title": "24.3-5"
        },
        {
            "location": "/Chap24/24.3/#243-6",
            "text": "We are given a directed graph $G = (V, E)$ on which each edge $(u, v) \\in E$ has an associated value $r(u, v)$, which is a real number in the range $0 \\le r(u, v) \\le 1$ that represents the reliability of a communication channel from vertex $u$ to vertex $v$. We interpret $r(u, v)$ as the probability that the channel from $u$ to $v$ will not fail, and we assume that these probabilities are independent. Give an efficient algorithm to find the most reliable path between two given vertices.   To find the most reliable path between $s$ and $t$, run Dijkstra's algorithm with edge weights $w(u, v) = -\\lg r(u, v)$ to find shortest paths from $s$ in $O(E + V\\lg V)$ time. The most reliable path is the shortest path from $s$ to $t$, and that path's reliability is the product of the reliabilities of its edges.  Here's why this method works. Because the probabilities are independent, the probability that a path will not fail is the product of the probabilities that its edges will not fail. We want to find a path $s \\overset{p}{\\leadsto} t$ such that $\\prod_{(u, v) \\in p} r(u, v)$ is maximized. This is equivalent to maximizing $\\lg(\\prod_{(u, v) \\in p} r(u, v)) = \\sum_{(u, v) \\in p} \\lg r(u, v)$, which is in turn equivalent to minimizing $\\sum_{(u, v) \\in p} -\\lg r(u, v)$. (Note: $r(u, v)$ can be $0$, and $\\lg 0$ is undefined. So in this algorithm, define $\\lg 0 = -\\infty$.) Thus if we assign weights $w(u, v) = -\\lg r(u, v)$, we have a shortest-path problem.  Since $\\lg 1 = 0$, $\\lg x < 0$ for $0 < x < 1$, and we have defined $\\lg 0 = -\\infty$, all the weights $w$ are nonnegative, and we can use Dijkstra's algorithm to find the shortest paths from $s$ in $O(E + V\\lg V)$ time.  Alternative solution  You can also work with the original probabilities by running a modified version of Dijkstra's algorithm that maximizes the product of reliabilities along a path instead of minimizing the sum of weights along a path.  In Dijkstra's algorithm, use the reliabilities as edge weights and substitute   max (and $\\text{EXTRACT-MAX}$) for min (and $\\text{EXTRACT-MIN}$) in relaxation and the queue,  $\\cdot$ for $+$ in relaxation,   $1$ (identity for $\\cdot$) for $0$ (identity for $+$) and $-\\infty$ (identity for min) for $\\infty$ (identity for max).   For example, we would use the following instead of the usual $\\text{RELAX}$ procedure:  RELAX - RELIABILITY ( u ,   v ,   r ) \n     if   v . d   <   u . d \uff0e r ( u ,   v ) \n         v . d   =   u . d \uff0e r ( u ,   v ) \n         v . PI   =   u   This algorithm is isomorphic to the one above: it performs the same operations except that it is working with the original probabilities instead of the transformed ones.",
            "title": "24.3-6"
        },
        {
            "location": "/Chap24/24.3/#243-7",
            "text": "Let $G = (V, E)$ be a weighted, directed graph with positive weight function $w: E \\rightarrow \\{1, 2, \\ldots, W\\}$ for some positive integer $W$, and assume that no two vertices have the same shortest-path weights from source vertex $s$. Now suppose that we define an unweighted, directed graph $G' = (V \\cup V', E')$ by replacing each edge $(u, v) \\in E$ with $w(u, v)$ unit-weight edges in series. How many vertices does $G'$ have? Now suppose that we run a breadth-first search on $G'$. Show that the order in which the breadth-first search of $G'$ colors vertices in $V$ black is the same as the order in which Dijkstra's algorithm extracts the vertices of $V$ from the priority queue when it runs on $G$.   $V + \\sum_{(u, v) \\in E} w(u, v) - E$.",
            "title": "24.3-7"
        },
        {
            "location": "/Chap24/24.3/#243-8",
            "text": "Let $G = (V, E)$ be a weighted, directed graph with nonnegative weight function $w: E \\rightarrow \\{0, 1, \\ldots, W\\}$ for some nonnegative integer $W$. Modify Dijkstra's algorithm to compute the shortest paths from a given source vertex s in $O(WV + E)$ time.   Observe that if a shortest-path estimate is not $\\infty$, then it's at most $(|V| - 1)W$. Why? In order to have $v.d < 1$, we must have relaxed an edge $(u, v)$ with $u.d < \\infty$. By induction, we can show that if we relax $(u, v)$, then $v.d$ is at most the number of edges on a path from $s$ to $v$ times the maximum edge weight. Since any acyclic path has at most $|V| - 1$ edges and the maximum edge weight is $W$, we see that $v.d \\le (|V| - 1)W$. Note also that $v.d$ must also be an integer, unless it is $\\infty$.  We also observe that in Dijkstra's algorithm, the values returned by the $\\text{EXTRACT-MIN}$ calls are monotonically increasing over time. Why? After we do our initial $|V|$ $\\text{INSERT}$ operations, we never do another. The only other way that a key value can change is by a $\\text{DECREASE-KEY}$ operation. Since edge weights are nonnegative, when we relax an edge $(u, v)$, we have that $u.d \\le v.d$. Since $u$ is the minimum vertex that we just extracted, we know that any other vertex we extract later has a key value that is at least $u.d$.  When keys are known to be integers in the range $0$ to $k$ and the key values extracted are monotonically increasing over time, we can implement a min-priority queue so that any sequence of $m$ $\\text{INSERT}$, $\\text{EXTRACT-MIN}$, and $\\text{DECREASE-KEY}$ operations takes $O(m + k)$ time. Here's how. We use an array, say $A[0..k]$\u008d, where $A[j]$ is a linked list of each element whose key is $j$. Think of $A[j]$ as a bucket for all elements with key $j$. We implement each bucket by a circular, doubly linked list with a sentinel, so that we can insert into or delete from each bucket in $O(1)$ time. We perform the min-priority queue operations as follows:   $\\text{INSERT}$: To insert an element with key $j$, just insert it into the linked list in $A[j]$\u008d. Time: $O(1)$ per $\\text{INSERT}$.  $\\text{EXTRACT-MIN}$: We maintain an index $min$ of the value of the smallest key extracted. Initially, $min$ is $0$. To find the smallest key, look in $A[min]$ and, if this list is nonempty, use any element in it, removing the element from the list and returning it to the caller. Otherwise, we rely on the monotonicity property and increment $min$ until we either find a list $A[min]$ that is nonempty (using any element in $A[min]$ as before) or we run off the end of the array $A$ (in which case the min-priority queue is empty). \n    Since there are at most $m$ $\\text{INSERT}$ operations, there are at most $m$ elements in the min-priority queue. We increment $min$ at most $k$ times, and we remove and return some element at most $m$ times. Thus, the total time over all $\\text{EXTRACT-MIN}$ operations is $O(m + k)$.    $\\text{DECREASE-KEY}$: To decrease the key of an element from $j$ to $i$, first check whether $i \\le j$, \ufb02agging an error if not. Otherwise, we remove the element from its list $A[j]$ in $O(1)$ time and insert it into the list $A[i]$ in $O(1)$ time. Time: $O(1)$ per $\\text{DECREASE-KEY}$. \n    To apply this kind of min-priority queue to Dijkstra's algorithm, we need to let $k = (|V| - 1)W$, and we also need a separate list for keys with value $\\infty$. The number of operations $m$ is $O(V + E)$ (since there are $|V|$ $\\text{INSERT}$ and $|V|$ $\\text{EXTRACT-MIN}$ operations and at most $|E|$ $\\text{DECREASE-KEY}$ operations), and so the total time is $O(V + E + VW) = O(VW + E)$.",
            "title": "24.3-8"
        },
        {
            "location": "/Chap24/24.3/#243-9",
            "text": "Modify your algorithm from Exercise 24.3-8 to run in $O((V + E) \\lg W)$ time. ($\\textit{Hint:}$ How many distinct shortest-path estimates can there be in $V - S$ at any point in time?)   First, observe that at any time, there are at most $W + 2$ distinct key values in the priority queue. Why? A key value is either $1$ or it is not. Consider what happens whenever a key value $v.d$ becomes finite. It must have occurred due to the relaxation of an edge $(u, v)$. At that time, $u$ was being placed into $S$, and $u.d \\le y.d$ for all vertices $y \\in V - S$. After relaxing edge $(u, v)$, we have $v.d \\le u.d + W$. Since any other vertex $y \\in V - S$ with $y.d < \\infty$ also had its estimate changed by a relaxation of some edge $x$ with $x.d \\le u.d$, we must have $y.d \\le x.d + W \\le u.d + W$. Thus, at the time that we are relaxing edges from a vertex $u$, we must have, for all vertices $v \\in V - S$, that $u.d \\le v.d \\le u.d + W$ or $v.d = \\infty$. Since shortest-path estimates are integer values (except for $\\infty$), at any given moment we have at most $W + 2$ different ones: $u.d$, $u.d + 1$, $u.d + 2$, $\\ldots$, $u.d + W$ and $\\infty$.  Therefore, we can maintain the min-priorty queue as a binary min-heap in which each node points to a doubly linked list of all vertices with a given key value. There are at most $W + 2$ nodes in the heap, and so $\\text{EXTRACT-MIN}$ runs in $O(\\lg W)$ time. To perform $\\text{DECREASE-KEY}$, we need to be able to find the heap node corresponding to a given key in $O(\\lg W)$ time. We can do so in $O(1)$ time as follows. First, keep a pointer $inf$ to the node containing all the $\\infty$ keys. Second, maintain an array $loc[0..W]$\u008d, where $loc[i]$ points to the unique heap entry whose key value is congruent to $i(\\mod(W + 1))$. As keys move around in the heap, we can update this array in $O(1)$ time per movement.  Alternatively, instead of using a binary min-heap, we could use a red-black tree. Now $\\text{INSERT}$, $\\text{DELETE}$, $\\text{MINIMUM}$, and $\\text{SEARCH}$\u2014from which we can construct the priority-queue operations\u2014each run in $O(\\lg W)$ time.",
            "title": "24.3-9"
        },
        {
            "location": "/Chap24/24.3/#243-10",
            "text": "Suppose that we are given a weighted, directed graph $G = (V, E)$ in which edges that leave the source vertex $s$ may have negative weights, all other edge weights are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra's algorithm correctly finds shortest paths from $s$ in this graph.   The proof of correctness, Theorem 24.6, goes through exactly as stated in the text. The key fact was that $\\delta(s, y) \\le \\delta(s, u)$. It is claimed that this holds because there are no negative edge weights, but in fact that is stronger than is needed. This always holds if $y$ occurs on a shortest path from $s$ to $u$ and $y \\ne s$ because all edges on the path from $y$ to $u$ have nonnegative weight. If any had negative weight, this would imply that we had ''gone back'' to an edge incident with $s$, which implies that a cycle is involved in the path, which would only be the case if it were a negative-weight cycle. However, these are still forbidden.",
            "title": "24.3-10"
        },
        {
            "location": "/Chap24/24.4/",
            "text": "24.4-1\n\n\n\n\nFind a feasible solution or determine that no feasible solution exists for the following system of difference constraints:\n\n\n\\begin{align}\nx_1 - x_2 & \\le & 1,  \\\\\nx_1 - x_4 & \\le & -4, \\\\\nx_2 - x_3 & \\le & 2,  \\\\\nx_2 - x_5 & \\le & 7,  \\\\\nx_2 - x_6 & \\le & 5,  \\\\\nx_3 - x_6 & \\le & 10, \\\\\nx_4 - x_2 & \\le & 2,  \\\\\nx_5 - x_1 & \\le & -1, \\\\\nx_5 - x_4 & \\le & 3,  \\\\\nx_6 - x_3 & \\le & 8\n\\end{align}\n\n\n\n\nOur vertices of the constraint graph will be \n\n\n$$\\{v_0, v_1, v_2, v_3, v_4, v_5, v_6\\}.$$\n\n\nThe edges will be\n\n\n$$(v_0, v_1), (v_0, v_2), (v_0, v_3), (v_0, v_4), (v_0, v_5), (v_0, v_6), (v_2, v_1), (v_4, v_1), (v_3, v_2), (v_5, v_2), (v_6, v_2), (v_6, v_3),$$\n\n\nwith edge weights \n\n\n$$0, 0, 0, 0, 0, 0, 1, -4, 2, 7, 5, 10, 2, -1, 3, -8$$\n\n\nrespectively. Then, computing \n\n\n$$(\\delta(v_0, v_1), \\delta(v_0, v_2), \\delta(v_0, v_3), \\delta(v_0, v_4), \\delta(v_0, v_5), \\delta(v_0, v_6)),$$\n\n\nwe get \n\n\n$$(-5, -3, 0, -1, -6, -8),$$\n\n\nwhich is a feasible solution by Theorem 24.9.\n\n\n24.4-2\n\n\n\n\nFind a feasible solution or determine that no feasible solution exists for the following system of difference constraints:\n\n\n\\begin{align}\nx_1 - x_2 & \\le &4, \\\\\nx_1 - x_5 & \\le &5, \\\\\nx_2 - x_4 & \\le &-6, \\\\\nx_3 - x_2 & \\le &1, \\\\\nx_4 - x_1 & \\le &3, \\\\\nx_4 - x_3 & \\le &5, \\\\\nx_4 - x_5 & \\le &10, \\\\\nx_5 - x_3 & \\le &-4, \\\\\nx_5 - x_4 & \\le &-8.\n\\end{align}\n\n\n\n\nThere is no feasible solution because the constraint graph contains a negative-weight cycle: $(v_1, v_4, v_2, v_3, v_5, v_1)$ has weight $-1$.\n\n\n24.4-3\n\n\n\n\nCan any shortest-path weight from the new vertex $v_0$ in a constraint graph be positive? Explain.\n\n\n\n\nNo, it cannot be positive. This is because for every vertex $v \\ne v_0$, there is an edge $(v_0, v)$ with weight zero. So, there is some path from the new vertex to every other of weight zero. Since $\\delta(v_0, v)$ is a minimum weight of all paths, it cannot be greater than the weight of this weight zero path that consists of a single edge.\n\n\n24.4-4\n\n\n\n\nExpress the single-pair shortest-path problem as a linear program.\n\n\n\n\nLet $\\delta(u)$ be the shortest-path weight from $s$ to $u$. Then we want to find $\\delta(t)$. $\\delta$ must satisfy\n\n\n\\begin{align}\n            \\delta(s) & =   0 \\\\\n\\delta(v) - \\delta(u) & \\le w(u, v) \\text{ for all $(u, v) \\in E$} & \\text{(Lemma 24.10)},\n\\end{align}\n\n\nwhere $w(u, v)$ is the weight of edge $(u, v)$.\n\n\nThus $x_v = \\delta(v)$ is a solution to\n\n\n\\begin{align}\n      x_s & = 0 \\\\\nx_v - x_u & \\le w(u, v).\n\\end{align}\n\n\nTo turn this into a set of inequalities of the required form, replace $x_s = 0$ by $x_s \\le 0$ and $-x_s \\le 0$ (i.e., $x_s \\ge$). The constraints are now\n\n\n\\begin{align}\n      x_s & \\le 0, \\\\\n     -x_s & \\le 0. \\\\\nx_v - x_u & \\le w(u, v),\n\\end{align}\n\n\nwhich still has $x_v = \\delta(v)$ as a solution.\n\n\nHowever, $\\delta$ isn't the only solution to this set of inequalities. (For example, if all edge weights are nonnegative, all $x_i = 0$ is a solution.) To force $x_t = \\delta(t)$ as required by the shortest-path problem, add the requirement to maximize (the objective function) $x_t$. This is correct because\n\n\n\n\n$\\max(x_t) \\ge \\delta(t)$ because $x_t = \\delta(t)$ is part of one solution to the set of inequalities,\n\n\n\n\n$\\max(x_t) \\le \\delta(t)$ can be demonstrated by a technique similar to the proof of Theorem 24.9:\n\n    Let $p$ be a shortest path from $s$ to $t$. Then by definition, \n\n\n$$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v).$$\n\n\nBut for each edge $(u, v)$ we have the inequality $x_v - x_u \\le w(u, v)$, so \n\n\n$$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v) \\ge \\sum_{(u, v) \\in p} (x_v - x_u) = x_t - x_s.$$\n\n\nBut $x_s = 0$, so $x_t \\le \\delta(t)$.\n\n\n\n\n\n\nNote: Maximizing $x_t$ subject to the above inequalities solves the single-pair shortest-path problem when $t$ is reachable from $s$ and there are no negative-weight cycles. But if there's a negative-weight cycle, the inequalities have no feasible solution (as demonstrated in the proof of Theorem 24.9); and if $t$ is not reachable from $s$, then $x_t$ is unbounded.\n\n\n24.4-5\n\n\n\n\nShow how to modify the Bellman-Ford algorithm slightly so that when we use it to solve a system of difference constraints with $m$ inequalities on $n$ unknowns, the running time is $O(nm)$.\n\n\n\n\nWe can follow the advice of problem 14.4-7 and solve the system of constraints on a modified constraint graph in which there is no new vertex $v_0$. This is simply done by initializing all of the vertices to have a $d$ value of $0$ before running the iterated relaxations of Bellman Ford. Since we don't add a new vertex and the $n$ edges going from it to to vertex corresponding to each variable, we are just running Bellman Ford on a graph with $n$ vertices and $m$ edges, and so it will have a runtime of $O(mn)$.\n\n\n24.4-6\n\n\n\n\nSuppose that in addition to a system of difference constraints, we want to handle \nequality constraints\n of the form $x_i = x_j + b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system.\n\n\n\n\nTo obtain the equality constraint $x_i = x_j + b_k$ we simply use the inequalities $x_i - x_j \\le b_k$ and $x_j - x_i \\le -bk$, then solve the problem as usual.\n\n\n24.4-7\n\n\n\n\nShow how to solve a system of difference constraints by a Bellman-Ford-like algorithm that runs on a constraint graph without the extra vertex $v_0$.\n\n\n\n\nObserve that after the first pass, all $d$ values are at most $0$, and that relaxing edges $(v_0, v_i)$ will never again change a $d$ value. Therefore, we can eliminate $v_0$ by running the Bellman-Ford algorithm on the constraint graph without the $v_0$ vertex but initializing all shortest path estimates to $0$ instead of $\\infty$.\n\n\n24.4-8 $\\star$\n\n\n\n\nLet $Ax \\le b$ be a system of $m$ difference constraints in $n$ unknowns. Show that the Bellman-Ford algorithm, when run on the corresponding constraint graph, maximizes $\\sum_{i = 1}^n x_i$ subject to $Ax \\le b$ and $x_i \\le 0$ for all $x_i$.\n\n\n\n\nBellman-Ford correctly solves the system of difference constraints so $Ax \\le b$ is always satisfied. We also have that $x_i = \\delta(v_0, v_i) \\le w(v_0, v_i) = 0$ so $x_i \\le 0$ for all $i$. To show that $\\sum x_i$ is maximized, we'll show that for any feasible solution $(y_1, y_2, \\ldots, y_n)$ which satisfies the constraints we have $yi \\le \\delta(v_0, v_i) = x_i$. Let $v_0, v_{i_1}, \\ldots, v_{i_k}$ be a shortest path from $v_0$ to $v_i$ in the constraint graph. Then we must have the constraints $y_{i_2} - y_{i_1} \\le w(v_{i_1}, v_{i_2}), \\ldots, y_{i_k} - y_{i_{k - 1}} \\le w(v_{i_{k - 1}},v_{i_k})$. Summing these up we have\n\n\n$$y_i \\le y_i - y_1 \\le \\sum_{m = 2}^k w(v_{i_m}, v_{i_{m - 1}}) = \\delta(v_0, v_i) = x_i.$$\n\n\n24.4-9 $\\star$\n\n\n\n\nShow that the Bellman-Ford algorithm, when run on the constraint graph for a system $Ax \\le b$ of difference constraints, minimizes the quantity $(\\max{x_i} - \\min{x_i})$ subject to $Ax \\le b$. Explain how this fact might come in handy if the algorithm is used to schedule construction jobs.\n\n\n\n\nWe can see that the Bellman-Ford algorithm run on the graph whose construction is described in this section causes the quantity $\\max{x_i} - \\min{x_i}$ to be minimized. We know that the largest value assigned to any of the vertices in the constraint graph is a $0$. It is clear that it won't be greater than zero, since just the single edge path to each of the vertices has cost zero. We also know that we cannot have every vertex having a shortest path with negative weight. To see this, notice that this would mean that the pointer for each vertex has it's $p$ value going to some other vertex that is not the source. This means that if we follow the procedure for reconstructing the shortest path for any of the vertices, we have that it can never get back to the source, a contradiction to the fact that it is a shortest path from the source to that vertex.\n\n\nNext, we note that when we run Bellman-Ford, we are maximizing $\\min{x_i}$. The shortest distance in the constraint graphs is the bare minimum of what is required in order to have all the constraints satisfied, if we were to increase any of the values we would be violating a constraint.\n\n\nThis could be in handy when scheduling construction jobs because the quantity $\\max{x_i} - \\min{x_i}$ is equal to the difference in time between the last task and the first task. Therefore, it means that minimizing it would mean that the total time that all the jobs takes is also minimized. And, most people want the entire process of construction to take as short of a time as possible.\n\n\n24.4-10\n\n\n\n\nSuppose that every row in the matrix $A$ of a linear program $Ax \\le b$ corresponds to a difference constraint, a single-variable constraint of the form $x_i \\le b_k$, or a singlevariable constraint of the form $-x_i \\le b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system.\n\n\n\n\nTo allow for single-variable constraints, we add the variable $x_0$ and let it correspond to the source vertex $v_0$ of the constraint graph. The idea is that, if there are no negative-weight cycles containing $v_0$, we will find that $\\delta(v_0, v_0) = 0$. In this case, we set $x_0 = 0$, and so we can treat any single-variable constraint using $x_i$ as if it were a $2$-variable constraint with $x_0$ as the other variable.\n\n\nSpecifically, we treat the constraint $x_i \\le b_k$ as if it were $x_i - x_0 \\le b_k$, and we add the edge $(v_0, v_i)$ with weight $b_k$ to the constraint graph. We treat the constraint $-x_i \\le b_k$ as if it were $x_0 - x_i \\le b_k$, and we add the edge $(v_i, v_0)$ with weight $b_k$ to the constraint graph.\n\n\nOnce we find shortest-path weights from $v_0$, we set $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$; that is, we do as before but also include $x_0$ as one of the variables that we set to a shortest-path weight. Since $v_0$ is the source vertex, either $x_0 = 0$ or $x_0 < 0$.\n\n\nIf $\\delta(v_0, v_0) = 0$, so that $x_0 = 0$, then setting $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$ gives a feasible solution for the system. The only new constraints beyond those in the text are those involving $x_0$. For constraints $x_i \\le b_k$, we use $x_i - x_0 \\le b_k$. By the triangle inequality, $\\delta(v_0, v_i) \\le \\delta(v_0, v_0) + w(v_0, v_i) = b_k$, and so $x_i \\le b_k$. For constraints $x_i \\le b_k$, we use $x_0 - x_i \\le b_k$. By the triangle inequality, $0 = \\delta(v_0, v_0) \\le \\delta(v_0, v_i) + w(v_i, v_0)$; thus, $0 \\le x_i + b_k$ or, equivalently, $-x_i \\le b_k$.\n\n\nIf $\\delta(v_0, v_0) < 0$, so that $x_0 < 0$, then there is a negative-weight cycle containing $v_0$. The portion of the proof of Theorem 24.9 that deals with negative-weight cycles carries through but with $v_0$ on the negative-weight cycle, and we see that there is no feasible solution.\n\n\n24.4-11\n\n\n\n\nGive an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and all of the unknowns $x_i$ must be integers.\n\n\n\n\nTo do this, just take the floor of (largest integer that is less than or equal to) each of the $b$ values and solve the resulting integer difference problem. These modified constraints will be admitting exactly the same set of assignments since we required that the solution have integer values assigned to the variables. This is because since the variables are integers, all of their differences will also be integers. For an integer to be less than or equal to a real number, it is necessary and sufficient for it to be less than or equal to the floor of that real number.\n\n\n24.4-12 $\\star$\n\n\n\n\nGive an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and a specified subset of some, but not necessarily all, of the unknowns $x_i$ must be integers.\n\n\n\n\nTo solve the problem of $Ax \\le b$ where the elements of $b$ are real-valued we carry out the same procedure as before, running Bellman-Ford, but allowing our edge weights to be real-valued. To impose the integer condition on the $x_i$'s, we modify the $\\text{RELAX}$ procedure. Suppose we call $\\text{RELAX}(v_i, v_j, w)$ where $v_j$ is required to be integral valued. If $v_j.d > \\lfloor v_i.d + w(v_i, v_j) \\rfloor$, set $v_j.d = \\lfloor v_i.d + w(v_i, v_j) \\rfloor$. This guarantees that the condition that $v_j.d - v_i.d \\le w(v_i, v_j)$ as desired. It also ensures that $v_j$ is integer valued. Since the triangle inequality still holds, $x = (v_1.d, v_2.d, \\ldots, v_n.d)$ is a feasible solution for the system, provided that $G$ contains no negative weight cycles.",
            "title": "24.4 Difference constraints and shortest paths"
        },
        {
            "location": "/Chap24/24.4/#244-1",
            "text": "Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints:  \\begin{align}\nx_1 - x_2 & \\le & 1,  \\\\\nx_1 - x_4 & \\le & -4, \\\\\nx_2 - x_3 & \\le & 2,  \\\\\nx_2 - x_5 & \\le & 7,  \\\\\nx_2 - x_6 & \\le & 5,  \\\\\nx_3 - x_6 & \\le & 10, \\\\\nx_4 - x_2 & \\le & 2,  \\\\\nx_5 - x_1 & \\le & -1, \\\\\nx_5 - x_4 & \\le & 3,  \\\\\nx_6 - x_3 & \\le & 8\n\\end{align}   Our vertices of the constraint graph will be   $$\\{v_0, v_1, v_2, v_3, v_4, v_5, v_6\\}.$$  The edges will be  $$(v_0, v_1), (v_0, v_2), (v_0, v_3), (v_0, v_4), (v_0, v_5), (v_0, v_6), (v_2, v_1), (v_4, v_1), (v_3, v_2), (v_5, v_2), (v_6, v_2), (v_6, v_3),$$  with edge weights   $$0, 0, 0, 0, 0, 0, 1, -4, 2, 7, 5, 10, 2, -1, 3, -8$$  respectively. Then, computing   $$(\\delta(v_0, v_1), \\delta(v_0, v_2), \\delta(v_0, v_3), \\delta(v_0, v_4), \\delta(v_0, v_5), \\delta(v_0, v_6)),$$  we get   $$(-5, -3, 0, -1, -6, -8),$$  which is a feasible solution by Theorem 24.9.",
            "title": "24.4-1"
        },
        {
            "location": "/Chap24/24.4/#244-2",
            "text": "Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints:  \\begin{align}\nx_1 - x_2 & \\le &4, \\\\\nx_1 - x_5 & \\le &5, \\\\\nx_2 - x_4 & \\le &-6, \\\\\nx_3 - x_2 & \\le &1, \\\\\nx_4 - x_1 & \\le &3, \\\\\nx_4 - x_3 & \\le &5, \\\\\nx_4 - x_5 & \\le &10, \\\\\nx_5 - x_3 & \\le &-4, \\\\\nx_5 - x_4 & \\le &-8.\n\\end{align}   There is no feasible solution because the constraint graph contains a negative-weight cycle: $(v_1, v_4, v_2, v_3, v_5, v_1)$ has weight $-1$.",
            "title": "24.4-2"
        },
        {
            "location": "/Chap24/24.4/#244-3",
            "text": "Can any shortest-path weight from the new vertex $v_0$ in a constraint graph be positive? Explain.   No, it cannot be positive. This is because for every vertex $v \\ne v_0$, there is an edge $(v_0, v)$ with weight zero. So, there is some path from the new vertex to every other of weight zero. Since $\\delta(v_0, v)$ is a minimum weight of all paths, it cannot be greater than the weight of this weight zero path that consists of a single edge.",
            "title": "24.4-3"
        },
        {
            "location": "/Chap24/24.4/#244-4",
            "text": "Express the single-pair shortest-path problem as a linear program.   Let $\\delta(u)$ be the shortest-path weight from $s$ to $u$. Then we want to find $\\delta(t)$. $\\delta$ must satisfy  \\begin{align}\n            \\delta(s) & =   0 \\\\\n\\delta(v) - \\delta(u) & \\le w(u, v) \\text{ for all $(u, v) \\in E$} & \\text{(Lemma 24.10)},\n\\end{align}  where $w(u, v)$ is the weight of edge $(u, v)$.  Thus $x_v = \\delta(v)$ is a solution to  \\begin{align}\n      x_s & = 0 \\\\\nx_v - x_u & \\le w(u, v).\n\\end{align}  To turn this into a set of inequalities of the required form, replace $x_s = 0$ by $x_s \\le 0$ and $-x_s \\le 0$ (i.e., $x_s \\ge$). The constraints are now  \\begin{align}\n      x_s & \\le 0, \\\\\n     -x_s & \\le 0. \\\\\nx_v - x_u & \\le w(u, v),\n\\end{align}  which still has $x_v = \\delta(v)$ as a solution.  However, $\\delta$ isn't the only solution to this set of inequalities. (For example, if all edge weights are nonnegative, all $x_i = 0$ is a solution.) To force $x_t = \\delta(t)$ as required by the shortest-path problem, add the requirement to maximize (the objective function) $x_t$. This is correct because   $\\max(x_t) \\ge \\delta(t)$ because $x_t = \\delta(t)$ is part of one solution to the set of inequalities,   $\\max(x_t) \\le \\delta(t)$ can be demonstrated by a technique similar to the proof of Theorem 24.9: \n    Let $p$ be a shortest path from $s$ to $t$. Then by definition,   $$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v).$$  But for each edge $(u, v)$ we have the inequality $x_v - x_u \\le w(u, v)$, so   $$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v) \\ge \\sum_{(u, v) \\in p} (x_v - x_u) = x_t - x_s.$$  But $x_s = 0$, so $x_t \\le \\delta(t)$.    Note: Maximizing $x_t$ subject to the above inequalities solves the single-pair shortest-path problem when $t$ is reachable from $s$ and there are no negative-weight cycles. But if there's a negative-weight cycle, the inequalities have no feasible solution (as demonstrated in the proof of Theorem 24.9); and if $t$ is not reachable from $s$, then $x_t$ is unbounded.",
            "title": "24.4-4"
        },
        {
            "location": "/Chap24/24.4/#244-5",
            "text": "Show how to modify the Bellman-Ford algorithm slightly so that when we use it to solve a system of difference constraints with $m$ inequalities on $n$ unknowns, the running time is $O(nm)$.   We can follow the advice of problem 14.4-7 and solve the system of constraints on a modified constraint graph in which there is no new vertex $v_0$. This is simply done by initializing all of the vertices to have a $d$ value of $0$ before running the iterated relaxations of Bellman Ford. Since we don't add a new vertex and the $n$ edges going from it to to vertex corresponding to each variable, we are just running Bellman Ford on a graph with $n$ vertices and $m$ edges, and so it will have a runtime of $O(mn)$.",
            "title": "24.4-5"
        },
        {
            "location": "/Chap24/24.4/#244-6",
            "text": "Suppose that in addition to a system of difference constraints, we want to handle  equality constraints  of the form $x_i = x_j + b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system.   To obtain the equality constraint $x_i = x_j + b_k$ we simply use the inequalities $x_i - x_j \\le b_k$ and $x_j - x_i \\le -bk$, then solve the problem as usual.",
            "title": "24.4-6"
        },
        {
            "location": "/Chap24/24.4/#244-7",
            "text": "Show how to solve a system of difference constraints by a Bellman-Ford-like algorithm that runs on a constraint graph without the extra vertex $v_0$.   Observe that after the first pass, all $d$ values are at most $0$, and that relaxing edges $(v_0, v_i)$ will never again change a $d$ value. Therefore, we can eliminate $v_0$ by running the Bellman-Ford algorithm on the constraint graph without the $v_0$ vertex but initializing all shortest path estimates to $0$ instead of $\\infty$.",
            "title": "24.4-7"
        },
        {
            "location": "/Chap24/24.4/#244-8-star",
            "text": "Let $Ax \\le b$ be a system of $m$ difference constraints in $n$ unknowns. Show that the Bellman-Ford algorithm, when run on the corresponding constraint graph, maximizes $\\sum_{i = 1}^n x_i$ subject to $Ax \\le b$ and $x_i \\le 0$ for all $x_i$.   Bellman-Ford correctly solves the system of difference constraints so $Ax \\le b$ is always satisfied. We also have that $x_i = \\delta(v_0, v_i) \\le w(v_0, v_i) = 0$ so $x_i \\le 0$ for all $i$. To show that $\\sum x_i$ is maximized, we'll show that for any feasible solution $(y_1, y_2, \\ldots, y_n)$ which satisfies the constraints we have $yi \\le \\delta(v_0, v_i) = x_i$. Let $v_0, v_{i_1}, \\ldots, v_{i_k}$ be a shortest path from $v_0$ to $v_i$ in the constraint graph. Then we must have the constraints $y_{i_2} - y_{i_1} \\le w(v_{i_1}, v_{i_2}), \\ldots, y_{i_k} - y_{i_{k - 1}} \\le w(v_{i_{k - 1}},v_{i_k})$. Summing these up we have  $$y_i \\le y_i - y_1 \\le \\sum_{m = 2}^k w(v_{i_m}, v_{i_{m - 1}}) = \\delta(v_0, v_i) = x_i.$$",
            "title": "24.4-8 $\\star$"
        },
        {
            "location": "/Chap24/24.4/#244-9-star",
            "text": "Show that the Bellman-Ford algorithm, when run on the constraint graph for a system $Ax \\le b$ of difference constraints, minimizes the quantity $(\\max{x_i} - \\min{x_i})$ subject to $Ax \\le b$. Explain how this fact might come in handy if the algorithm is used to schedule construction jobs.   We can see that the Bellman-Ford algorithm run on the graph whose construction is described in this section causes the quantity $\\max{x_i} - \\min{x_i}$ to be minimized. We know that the largest value assigned to any of the vertices in the constraint graph is a $0$. It is clear that it won't be greater than zero, since just the single edge path to each of the vertices has cost zero. We also know that we cannot have every vertex having a shortest path with negative weight. To see this, notice that this would mean that the pointer for each vertex has it's $p$ value going to some other vertex that is not the source. This means that if we follow the procedure for reconstructing the shortest path for any of the vertices, we have that it can never get back to the source, a contradiction to the fact that it is a shortest path from the source to that vertex.  Next, we note that when we run Bellman-Ford, we are maximizing $\\min{x_i}$. The shortest distance in the constraint graphs is the bare minimum of what is required in order to have all the constraints satisfied, if we were to increase any of the values we would be violating a constraint.  This could be in handy when scheduling construction jobs because the quantity $\\max{x_i} - \\min{x_i}$ is equal to the difference in time between the last task and the first task. Therefore, it means that minimizing it would mean that the total time that all the jobs takes is also minimized. And, most people want the entire process of construction to take as short of a time as possible.",
            "title": "24.4-9 $\\star$"
        },
        {
            "location": "/Chap24/24.4/#244-10",
            "text": "Suppose that every row in the matrix $A$ of a linear program $Ax \\le b$ corresponds to a difference constraint, a single-variable constraint of the form $x_i \\le b_k$, or a singlevariable constraint of the form $-x_i \\le b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system.   To allow for single-variable constraints, we add the variable $x_0$ and let it correspond to the source vertex $v_0$ of the constraint graph. The idea is that, if there are no negative-weight cycles containing $v_0$, we will find that $\\delta(v_0, v_0) = 0$. In this case, we set $x_0 = 0$, and so we can treat any single-variable constraint using $x_i$ as if it were a $2$-variable constraint with $x_0$ as the other variable.  Specifically, we treat the constraint $x_i \\le b_k$ as if it were $x_i - x_0 \\le b_k$, and we add the edge $(v_0, v_i)$ with weight $b_k$ to the constraint graph. We treat the constraint $-x_i \\le b_k$ as if it were $x_0 - x_i \\le b_k$, and we add the edge $(v_i, v_0)$ with weight $b_k$ to the constraint graph.  Once we find shortest-path weights from $v_0$, we set $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$; that is, we do as before but also include $x_0$ as one of the variables that we set to a shortest-path weight. Since $v_0$ is the source vertex, either $x_0 = 0$ or $x_0 < 0$.  If $\\delta(v_0, v_0) = 0$, so that $x_0 = 0$, then setting $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$ gives a feasible solution for the system. The only new constraints beyond those in the text are those involving $x_0$. For constraints $x_i \\le b_k$, we use $x_i - x_0 \\le b_k$. By the triangle inequality, $\\delta(v_0, v_i) \\le \\delta(v_0, v_0) + w(v_0, v_i) = b_k$, and so $x_i \\le b_k$. For constraints $x_i \\le b_k$, we use $x_0 - x_i \\le b_k$. By the triangle inequality, $0 = \\delta(v_0, v_0) \\le \\delta(v_0, v_i) + w(v_i, v_0)$; thus, $0 \\le x_i + b_k$ or, equivalently, $-x_i \\le b_k$.  If $\\delta(v_0, v_0) < 0$, so that $x_0 < 0$, then there is a negative-weight cycle containing $v_0$. The portion of the proof of Theorem 24.9 that deals with negative-weight cycles carries through but with $v_0$ on the negative-weight cycle, and we see that there is no feasible solution.",
            "title": "24.4-10"
        },
        {
            "location": "/Chap24/24.4/#244-11",
            "text": "Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and all of the unknowns $x_i$ must be integers.   To do this, just take the floor of (largest integer that is less than or equal to) each of the $b$ values and solve the resulting integer difference problem. These modified constraints will be admitting exactly the same set of assignments since we required that the solution have integer values assigned to the variables. This is because since the variables are integers, all of their differences will also be integers. For an integer to be less than or equal to a real number, it is necessary and sufficient for it to be less than or equal to the floor of that real number.",
            "title": "24.4-11"
        },
        {
            "location": "/Chap24/24.4/#244-12-star",
            "text": "Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and a specified subset of some, but not necessarily all, of the unknowns $x_i$ must be integers.   To solve the problem of $Ax \\le b$ where the elements of $b$ are real-valued we carry out the same procedure as before, running Bellman-Ford, but allowing our edge weights to be real-valued. To impose the integer condition on the $x_i$'s, we modify the $\\text{RELAX}$ procedure. Suppose we call $\\text{RELAX}(v_i, v_j, w)$ where $v_j$ is required to be integral valued. If $v_j.d > \\lfloor v_i.d + w(v_i, v_j) \\rfloor$, set $v_j.d = \\lfloor v_i.d + w(v_i, v_j) \\rfloor$. This guarantees that the condition that $v_j.d - v_i.d \\le w(v_i, v_j)$ as desired. It also ensures that $v_j$ is integer valued. Since the triangle inequality still holds, $x = (v_1.d, v_2.d, \\ldots, v_n.d)$ is a feasible solution for the system, provided that $G$ contains no negative weight cycles.",
            "title": "24.4-12 $\\star$"
        },
        {
            "location": "/Chap24/24.5/",
            "text": "24.5-1\n\n\n\n\nGive two shortest-paths trees for the directed graph of Figure 24.2 (on page 648) other than the two shown.\n\n\n\n\nSince the induced shortest path trees on $\\{s, t, y\\}$ and on $\\{t, x, y, z\\}$ are independent and have to possible configurations each, there are four total arising from that. So, we have the two not shown in the figure are the one consisting of the edges $\\{(s, t), (s, y), (y, x), (x, z)\\}$ and the one consisting of the edges $\\{(s, t), (t, y), (t, x), (y, z)\\}$.\n\n\n24.5-2\n\n\n\n\nGive an example of a weighted, directed graph $G = (V, E)$ with weight function $w: E \\rightarrow \\mathbb R$ and source vertex $s$ such that $G$ satisfies the following property: For every edge $(u, v) \\in E$, there is a shortest-paths tree rooted at $s$ that contains $(u, v)$ and another shortest-paths tree rooted at $s$ that does not contain $(u, v)$.\n\n\n\n\nLet $G$ have $3$ vertices $s$, $x$, and $y$. Let the edges be $(s, x)$, $(s, y)$, and $(x, y)$ with weights $1$, $1$, and $0$ respectively. There are $3$ possible trees on these vertices rooted at $s$, and each is a shortest paths tree which gives $\\delta(s, x) = \\delta(s, y) = 1$.\n\n\n24.5-3\n\n\n\n\nEmbellish the proof of Lemma 24.10 to handle cases in which shortest-path weights are $\\infty$ or $-\\infty$.\n\n\n\n\nTo modify Lemma 24.10 to allow for possible shortest path weights of $\\infty$ and $-\\infty$, we need to define our addition as $\\infty + c = \\infty$, and $-\\infty + c = -\\infty$. This will make the statement behave correctly, that is, we can take the shortest path from $s$ to $u$ and tack on the edge $(u, v)$ to the end. That is, if there is a negative weight cycle on your way to $u$ and there is an edge from $u$ to $v$, there is a negative weight cycle on our way to $v$. Similarly, if we cannot reach $v$ and there is an edge from $u$ to $v$, we cannot reach $u$.\n\n\n24.5-4\n\n\n\n\nLet $G = (V, E)$ be a weighted, directed graph with source vertex $s$, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that if a sequence of relaxation steps sets $s.\\pi$ to a non-$\\text{NIL}$ value, then $G$ contains a negative-weight cycle.\n\n\n\n\nWhenever $\\text{RELAX}$ sets $\\pi$ for some vertex, it also reduces the vertex's $d$ value. Thus if $s.\\pi$ gets set to a non-$\\text{NIL}$ value, $s.d$ is reduced from its initial value of $0$ to a negative number. But $s.d$ is the weight of some path from $s$ to $s$, which is a cycle including $s$. Thus, there is a negative-weight cycle.\n\n\n24.5-5\n\n\n\n\nLet $G = (V, E)$ be a weighted, directed graph with no negative-weight edges. Let $s \\in V$ be the source vertex, and suppose that we allow $v.\\pi$ to be the predecessor of $v$ on any shortest path to $v$ from source $s$ if $v \\in V - \\{s\\}$ is reachable from $s$, and $\\text{NIL}$ otherwise. Give an example of such a graph $G$ and an assignment of $\\pi$ values that produces a cycle in $G_\\pi$. (By Lemma 24.16, such an assignment cannot be produced by a sequence of relaxation steps.)\n\n\n\n\nSuppose that we have a grap hon three vertices $\\{s, u, v\\}$ and containing edges $(s, u), (s, v), (u, v), (v, u)$ all with weight $0$. Then, there is a shortest path from $s$ to $v$ of $s$, $u$, $v$ and a shortest path from $s$ to $u$ of $s$ $v$, $u$. Based off of these, we could set $v.\\pi = u$ and $u.\\pi = v$. This then means that there is a cycle consisting of $u, v$ in $G_\\pi$.\n\n\n24.5-6\n\n\n\n\nLet $G = (V, E)$ be a weighted, directed graph with weight function $w: E \\rightarrow \\mathbb R$ and no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that for every vertex $v \\in V_\\pi$, there exists a path from $s$ to $v$ in $G_\\pi$ and that this property is maintained as an invariant over any sequence of relaxations.\n\n\n\n\nWe will prove this by induction on the number of relaxations performed. For the base-case, we have just called $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. The only vertex in $V_\\pi$ is $s$, and there is trivially a path from $s$ to itself. Now suppose that after any sequence of $n$ relaxations, for every vertex $v \\in V_\\pi$ there exists a path from $s$ to $v$ in $G_\\pi$. Consider the $(n + 1)$th relaxation. Suppose it is such that $v.d > u.d + w(u, v)$. When we relax $v$, we update $v.\\pi = u.\\pi$. By the induction hypothesis, there was a path from $s$ to $u$ in $G_\\pi$. Now $v$ is in $V_\\pi$, and the path from $s$ to $u$, followed by the edge $(u,v) = (v.\\pi, v)$ is a path from s to $v$ in $G_\\pi$, so the claim holds.\n\n\n24.5-7\n\n\n\n\nLet $G = (V, E)$ be a weighted, directed graph that contains no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that there exists a sequence of $|V| - 1$ relaxation steps that produces $v.d = \\delta(s, v)$ for all $v \\in V$.\n\n\n\n\nSuppose we have a shortest-paths tree $G_\\pi$. Relax edges in $G_\\pi$ according to the order in which a BFS would visit them. Then we are guaranteed that the edges along each shortest path are relaxed in order. By the path-relaxation property, we would then have $v.d = \\delta(s, v)$ for all $v \\in V$. Since $G_\\pi$ contains at most $|V| - 1$ edges, we need to relax only $|V| - 1$ edges to get $v.d = \\delta(s, v)$ for all $v \\in V$.\n\n\n24.5-8\n\n\n\n\nLet $G$ be an arbitrary weighted, directed graph with a negative-weight cycle reachable from the source vertex $s$. Show how to construct an infinite sequence of relaxations of the edges of $G$ such that every relaxation causes a shortest-path estimate to change.\n\n\n\n\nSuppose that there is a negative-weight cycle $c = \\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = v_k$, that is reachable from the source vertex $s$; thus, $w(c) < 0$. Without loss of generality, $c$ is simple. There must be an acyclic path from $s$ to some vertex of $c$ that uses no other vertices in $c$. Without loss of generality let this vertex of $c$ be $v_0$, and let this path from $s$ to $v_0$ be $p = \\langle u_0, u_1, \\ldots, u_l \\rangle$, where $u_0 = s$ and $u_l = v_0 = v_k$. (It may be the case that $u_l = s$, in which case path $p$ has no edges.) \n\n\nAfter the call to $\\text{INITIALIZE-SINGLE-SOURCE}$ sets $v.d = \\infty$ for all $v \\in V - \\{s\\}$, perform the following sequence of relaxations. First, relax every edge in path $p$, in order. Then relax every edge in cycle $c$, in order, and repeatedly relax the cycle. That is, we relax the edges $(u_0, u_1)$, $(u_1, u_2)$, $\\ldots$, $(u_{l - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $\\ldots$\n\n\nWe claim that every edge relaxation in this sequence reduces a shortest-path estimate. Clearly, the first time we relax an edge $(u_{i - 1}, u_i)$ or $(v_{j - 1}, v_j)$, for $i = 1, 2, \\ldots, l$ and $j = 1, 2, \\ldots, k - 1$ (note that we have not yet relaxed the last edge of cycle $c$), we reduce $u_i.d$ or $v_j.d$ from $\\infty$ to a finite value. Now consider the relaxation of any edge $(v_{j - 1}, v_j)$ after this opening sequence of relaxations. We use induction on the number of edge relaxations to show that this relaxation reduces $v_j.d$.\n\n\nBasis:\n The next edge relaxed after the opening sequence is $(v_{k - 1}, v_k)$. Before relaxation, $v_k.d = w(p)$, and after relaxation, $v_k.d = w(p) + w(c) < w(p)$, since $w(c) < 0$.\n\n\nInductive step:\n Consider the relaxation of edge $(v_{j - 1}, v_j)$. Since $c$ is a simple cycle, the last time $v_j.d$ was updated was by a relaxation of this same edge. By the inductive hypothesis, $v_{j - 1}.d$ has just been reduced. Thus, $v_{j - 1}.d + w(v_{j - 1}, v_j) < v_j.d$, and so the relaxation will reduce the value of $v_j.d$.",
            "title": "24.5 Proofs of shortest-paths properties"
        },
        {
            "location": "/Chap24/24.5/#245-1",
            "text": "Give two shortest-paths trees for the directed graph of Figure 24.2 (on page 648) other than the two shown.   Since the induced shortest path trees on $\\{s, t, y\\}$ and on $\\{t, x, y, z\\}$ are independent and have to possible configurations each, there are four total arising from that. So, we have the two not shown in the figure are the one consisting of the edges $\\{(s, t), (s, y), (y, x), (x, z)\\}$ and the one consisting of the edges $\\{(s, t), (t, y), (t, x), (y, z)\\}$.",
            "title": "24.5-1"
        },
        {
            "location": "/Chap24/24.5/#245-2",
            "text": "Give an example of a weighted, directed graph $G = (V, E)$ with weight function $w: E \\rightarrow \\mathbb R$ and source vertex $s$ such that $G$ satisfies the following property: For every edge $(u, v) \\in E$, there is a shortest-paths tree rooted at $s$ that contains $(u, v)$ and another shortest-paths tree rooted at $s$ that does not contain $(u, v)$.   Let $G$ have $3$ vertices $s$, $x$, and $y$. Let the edges be $(s, x)$, $(s, y)$, and $(x, y)$ with weights $1$, $1$, and $0$ respectively. There are $3$ possible trees on these vertices rooted at $s$, and each is a shortest paths tree which gives $\\delta(s, x) = \\delta(s, y) = 1$.",
            "title": "24.5-2"
        },
        {
            "location": "/Chap24/24.5/#245-3",
            "text": "Embellish the proof of Lemma 24.10 to handle cases in which shortest-path weights are $\\infty$ or $-\\infty$.   To modify Lemma 24.10 to allow for possible shortest path weights of $\\infty$ and $-\\infty$, we need to define our addition as $\\infty + c = \\infty$, and $-\\infty + c = -\\infty$. This will make the statement behave correctly, that is, we can take the shortest path from $s$ to $u$ and tack on the edge $(u, v)$ to the end. That is, if there is a negative weight cycle on your way to $u$ and there is an edge from $u$ to $v$, there is a negative weight cycle on our way to $v$. Similarly, if we cannot reach $v$ and there is an edge from $u$ to $v$, we cannot reach $u$.",
            "title": "24.5-3"
        },
        {
            "location": "/Chap24/24.5/#245-4",
            "text": "Let $G = (V, E)$ be a weighted, directed graph with source vertex $s$, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that if a sequence of relaxation steps sets $s.\\pi$ to a non-$\\text{NIL}$ value, then $G$ contains a negative-weight cycle.   Whenever $\\text{RELAX}$ sets $\\pi$ for some vertex, it also reduces the vertex's $d$ value. Thus if $s.\\pi$ gets set to a non-$\\text{NIL}$ value, $s.d$ is reduced from its initial value of $0$ to a negative number. But $s.d$ is the weight of some path from $s$ to $s$, which is a cycle including $s$. Thus, there is a negative-weight cycle.",
            "title": "24.5-4"
        },
        {
            "location": "/Chap24/24.5/#245-5",
            "text": "Let $G = (V, E)$ be a weighted, directed graph with no negative-weight edges. Let $s \\in V$ be the source vertex, and suppose that we allow $v.\\pi$ to be the predecessor of $v$ on any shortest path to $v$ from source $s$ if $v \\in V - \\{s\\}$ is reachable from $s$, and $\\text{NIL}$ otherwise. Give an example of such a graph $G$ and an assignment of $\\pi$ values that produces a cycle in $G_\\pi$. (By Lemma 24.16, such an assignment cannot be produced by a sequence of relaxation steps.)   Suppose that we have a grap hon three vertices $\\{s, u, v\\}$ and containing edges $(s, u), (s, v), (u, v), (v, u)$ all with weight $0$. Then, there is a shortest path from $s$ to $v$ of $s$, $u$, $v$ and a shortest path from $s$ to $u$ of $s$ $v$, $u$. Based off of these, we could set $v.\\pi = u$ and $u.\\pi = v$. This then means that there is a cycle consisting of $u, v$ in $G_\\pi$.",
            "title": "24.5-5"
        },
        {
            "location": "/Chap24/24.5/#245-6",
            "text": "Let $G = (V, E)$ be a weighted, directed graph with weight function $w: E \\rightarrow \\mathbb R$ and no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that for every vertex $v \\in V_\\pi$, there exists a path from $s$ to $v$ in $G_\\pi$ and that this property is maintained as an invariant over any sequence of relaxations.   We will prove this by induction on the number of relaxations performed. For the base-case, we have just called $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. The only vertex in $V_\\pi$ is $s$, and there is trivially a path from $s$ to itself. Now suppose that after any sequence of $n$ relaxations, for every vertex $v \\in V_\\pi$ there exists a path from $s$ to $v$ in $G_\\pi$. Consider the $(n + 1)$th relaxation. Suppose it is such that $v.d > u.d + w(u, v)$. When we relax $v$, we update $v.\\pi = u.\\pi$. By the induction hypothesis, there was a path from $s$ to $u$ in $G_\\pi$. Now $v$ is in $V_\\pi$, and the path from $s$ to $u$, followed by the edge $(u,v) = (v.\\pi, v)$ is a path from s to $v$ in $G_\\pi$, so the claim holds.",
            "title": "24.5-6"
        },
        {
            "location": "/Chap24/24.5/#245-7",
            "text": "Let $G = (V, E)$ be a weighted, directed graph that contains no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that there exists a sequence of $|V| - 1$ relaxation steps that produces $v.d = \\delta(s, v)$ for all $v \\in V$.   Suppose we have a shortest-paths tree $G_\\pi$. Relax edges in $G_\\pi$ according to the order in which a BFS would visit them. Then we are guaranteed that the edges along each shortest path are relaxed in order. By the path-relaxation property, we would then have $v.d = \\delta(s, v)$ for all $v \\in V$. Since $G_\\pi$ contains at most $|V| - 1$ edges, we need to relax only $|V| - 1$ edges to get $v.d = \\delta(s, v)$ for all $v \\in V$.",
            "title": "24.5-7"
        },
        {
            "location": "/Chap24/24.5/#245-8",
            "text": "Let $G$ be an arbitrary weighted, directed graph with a negative-weight cycle reachable from the source vertex $s$. Show how to construct an infinite sequence of relaxations of the edges of $G$ such that every relaxation causes a shortest-path estimate to change.   Suppose that there is a negative-weight cycle $c = \\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = v_k$, that is reachable from the source vertex $s$; thus, $w(c) < 0$. Without loss of generality, $c$ is simple. There must be an acyclic path from $s$ to some vertex of $c$ that uses no other vertices in $c$. Without loss of generality let this vertex of $c$ be $v_0$, and let this path from $s$ to $v_0$ be $p = \\langle u_0, u_1, \\ldots, u_l \\rangle$, where $u_0 = s$ and $u_l = v_0 = v_k$. (It may be the case that $u_l = s$, in which case path $p$ has no edges.)   After the call to $\\text{INITIALIZE-SINGLE-SOURCE}$ sets $v.d = \\infty$ for all $v \\in V - \\{s\\}$, perform the following sequence of relaxations. First, relax every edge in path $p$, in order. Then relax every edge in cycle $c$, in order, and repeatedly relax the cycle. That is, we relax the edges $(u_0, u_1)$, $(u_1, u_2)$, $\\ldots$, $(u_{l - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $\\ldots$  We claim that every edge relaxation in this sequence reduces a shortest-path estimate. Clearly, the first time we relax an edge $(u_{i - 1}, u_i)$ or $(v_{j - 1}, v_j)$, for $i = 1, 2, \\ldots, l$ and $j = 1, 2, \\ldots, k - 1$ (note that we have not yet relaxed the last edge of cycle $c$), we reduce $u_i.d$ or $v_j.d$ from $\\infty$ to a finite value. Now consider the relaxation of any edge $(v_{j - 1}, v_j)$ after this opening sequence of relaxations. We use induction on the number of edge relaxations to show that this relaxation reduces $v_j.d$.  Basis:  The next edge relaxed after the opening sequence is $(v_{k - 1}, v_k)$. Before relaxation, $v_k.d = w(p)$, and after relaxation, $v_k.d = w(p) + w(c) < w(p)$, since $w(c) < 0$.  Inductive step:  Consider the relaxation of edge $(v_{j - 1}, v_j)$. Since $c$ is a simple cycle, the last time $v_j.d$ was updated was by a relaxation of this same edge. By the inductive hypothesis, $v_{j - 1}.d$ has just been reduced. Thus, $v_{j - 1}.d + w(v_{j - 1}, v_j) < v_j.d$, and so the relaxation will reduce the value of $v_j.d$.",
            "title": "24.5-8"
        },
        {
            "location": "/Chap24/Problems/24-1/",
            "text": "Suppose that we order the edge relaxations in each pass of the Bellman-Ford algorithm as follows. Before the first pass, we assign an arbitrary linear order $v_1, v_2, \\ldots, v_{|V|}$ to the vertices of the input graph $G = (V, E)$. Then, we partition the edge set $E$ into $E_f \\cup E_b$, where $E_f = {(v_i, v_j) \\in E: i < j}$ and $E_b = \\{(v_i, v_j) \\in E: i > j\\}$. (Assume that $G$ contains no self-loops, so that every edge is in either $E_f$ or $E_b$.) Define $G_f = (V, E_f)$ and $G_b = (V, E_b)$.\n\n\na.\n Prove that $G_f$ is acyclic with topological sort $\\langle v_1, v_2, \\ldots, v_{|V|} \\rangle$ and that $G_b$ is acyclic with topological sort $\\langle v_{|V|}, v_{|V| - 1}, \\ldots, v_1 \\rangle$.\n\n\nSuppose that we implement each pass of the Bellman-Ford algorithm in the following way. We visit each vertex in the order $v_1, v_2, \\ldots, v_{|V|}$, relaxing edges of $E_f$ that leave the vertex. We then visit each vertex in the order $v_{|V|}, v_{|V| - 1}, \\ldots, v_1$, relaxing edges of $E_b$ that leave the vertex.\n\n\nb.\n Prove that with this scheme, if $G$ contains no negative-weight cycles that are reachable from the source vertex $s$, then after only $\\lceil |V| / 2 \\rceil$ passes over the edges, $v.d = \\delta(s, v)$ for all vertices $v \\in V$.\n\n\nc.\n Does this scheme improve the asymptotic running time of the Bellman-Ford algorithm?\n\n\n\n\na.\n Assume for the purpose contradiction that $G_f$ is not acyclic; thus $G_f$ has a cycle. A cycle must have at least one edge $(u, v)$ in which $u$ has higher index than $v$. This edge is not in $E_f$ (by the definition of $E_f$), in contradition to the assumption that $G_f$ has a cycle. Thus $G_f$ is acyclic.\n\n\nThe sequence $\\langle v_1, v_2, \\ldots, v_{|V|} \\rangle$ is a topological sort for $G_f$, because from the definition of $E_f$ we know that all edges are directed from smaller indices to larger indices.\n\n\nThe proof for $E_b$ is similar.\n\n\nb.\n For all vertices $v \\in V$, we know that either $\\delta(s, v) = \\infty$ or $\\delta(s, v)$ is finite. If $\\delta(s, v) = \\infty$, then $v.d$ will be $\\infty$. Thus, we need to consider only the case where $v.d$ is finite. There must be some shortest path from $s$ to $v$. Let $p = \\langle v_0, v_1, \\ldots, v_{k - 1}, v_k \\rangle$ be that path, where $v_0 = s$ and $v_k = v$. Let us now consider how many times there is a change in direction in $p$, that is, a situation in which $(v_{i - 1}, v_i) \\in E_f$ and $(v_i, v_{i + 1} \\in E_b$ or vice versa. There can be at most $|V| - 1$ edges in $p$, so there can be at most $|V| - 2$ changes in direction. Any portion of the path where there is no change in direction is computed with the correct $d$ values in the first or second half of a single pass once the vertex that begins the no-change-in-direction sequence has the correct $d$ value, because the edges are relaxed in the order of the direction of the sequence. Each change in direction requires a half pass in the new direction of the path. The following table shows the maximum number of passes needed depending on the parity of $|V| - 1$ and the direction of the first edge:\n\n\n\\begin{array}{lll}\n|V| - 1 & \\text{first edge direction} & \\text{passes} \\\\\n\\hline\n\\text{even} & \\text{forward}  & (|V| - 1) / 2       \\\\\n\\text{even} & \\text{backward} & (|V| - 1) / 2 + 1   \\\\\n\\text{odd}  & \\text{forward}  & |V| / 2             \\\\\n\\text{odd}  & \\text{backward} & |V| / 2\n\\end{array}\n\n\nIn any case, the maximum number of passes that we will need is $\\lceil |V| / 2 \\rceil$.\n\n\nc.\n This scheme does not affect the asymptotic running time of the algorithm because even though we perform only $\\lceil |V| / 2 \\rceil$ passes instead of $|V| - 1$ passes, it is still $O(V)$ passes. Each pass still takes $\\Theta(E)$ time, so the running time remains $O(VE)$.",
            "title": "24-1 Yen's improvement to Bellman-Ford"
        },
        {
            "location": "/Chap24/Problems/24-2/",
            "text": "A $d$-dimensional box with dimensions $(x_1, x_2, \\ldots, x_d)$ \nnests\n within another box with dimensions $(y_1, y_2, \\ldots, y_d)$ if there exists a permutation $\\pi$ on $\\{1, 2, \\ldots, d\\}$ such that $x_{\\pi(1)} < y_1$, $x_{\\pi(2)} < y_2$, $\\ldots$, $x_{\\pi(d)} < y_d$.\n\n\na.\n Argue that the nesting relation is transitive.\n\n\nb.\n Describe an efficient method to determine whether or not one $d$-dimensional box nests inside another.\n\n\nc.\n Suppose that you are given a set of $n$ $d$-dimensional boxes $\\{B_1, B_2, \\ldots, B_n\\}$. Give an efficient algorithm to find the longest sequence $\\langle B_{i_1}, B_{i_2}, \\ldots, B_{i_k} \\rangle$ of boxes such that $B_{i_j}$ nests within $B_{i_{j + 1}}$ for $j = 1, 2, \\ldots, k - 1$. Express the running time of your algorithm in terms of $n$ and $d$.\n\n\n\n\na.\n Consider boxes with dimensions $x = (x_1, \\ldots, x_d)$, $y = (y_1, \\ldots, y_d)$, and $z = (z_1, \\ldots, z_d)$. Suppose there exists a permutation $\\pi$ such that $x_{\\pi(i)} < y_i$ for $i = 1, \\ldots, d$ and there exists a permutation $\\pi'$ such that $y_{\\pi'(i)} < z_i$ for $i = 1, \\ldots, d$, so that $x$ nests inside $y$ and $y$ nests inside $z$. Construct a permutation $\\pi''$, where $\\pi''(i) = \\pi'(\\pi(i))$. Then for $i = 1, \\ldots, d$, we have $x_{\\pi''(i)} = x_{\\pi'(\\pi(i))} < y_{\\pi'(i)} < z_i$, and so $x$ nests inside $z$.\n\n\nb.\n Sort the dimensions of each box from longest to shortest. A box $X$ with sorted dimensions $(x_1, x_2, \\ldots, x_d)$ nests inside a box $Y$ with sorted dimensions $(y_1, y_2, \\ldots, y_d)$ if and only if $x_i < y_i$ for $i = 1, 2, \\ldots, d$. The sorting can be done in $O(d\\lg d)$ time, and the test for nesting can be done in $O(d)$ time, and so the algorithm runs in $O(d\\lg d)$ time. This algorithm works because a $d$-dimensional box can be oriented so that every permutation of its dimensions is possible. (Experiment with a $3$-dimensional box if you are unsure of this).\n\n\nc.\n Construct a dag $G = (V, E)$, where each vertex $v_i$ corresponds to box $B_i$, and $(v_i, v_j) \\in E$ if and only if box $B_i$ nests inside box $B_j$. Graph $G$ is indeed a dag, because nesting is transitive and antireflexive (i.e., no box nests inside itself). The time to construct the dag is $O(dn^2 + dn\\lg d)$, from comparing each of the $\\binom{n}{2}$ pairs of boxes after sorting the dimensions of each.\n\n\nAdd a supersource vertex $s$ and a supersink vertex $t$ to $G$, and add edges $(s, v_i)$ for all vertices $v_i$ with $in\\text-degree$ $0$ and $(v_j, t)$ for all vertices $v_j$ with outdegree $0$. Call the resulting dag $G'$. The time to do so is $O(n)$.\n\n\nFind a longest path from $s$ to $t$ in $G'$. (Section 24.2 discusses how to find a longest path in a dag.) This path corresponds to a longest sequence of nesting boxes. The time to find a longest path is $O(n^2)$, since $G'$ has $n + 2$ vertices and $O(n^2)$ edges.\n\n\nOverall, this algorithm runs in $O(dn^2 + dn\\lg d)$ time.",
            "title": "24-2 Nesting boxes"
        },
        {
            "location": "/Chap24/Problems/24-3/",
            "text": "Arbitrage\n is the use of discrepancies in currency exchange rates to transform one unit of a currency into more than one unit of the same currency. For example, suppose that $1$ U.S. dollar buys $49$ Indian rupees, $1$ Indian rupee buys $2$ Japanese yen, and $1$ Japanese yen buys $0.0107$ U.S. dollars. Then, by converting currencies, a trader can start with $1$ U.S. dollar and buy $49 \\times 2 \\times 0.0107 = 1.0486$ U.S. dollars, thus turning a profit of $4.86$ percent.\n\n\nSuppose that we are given $n$ currencies $c_1, c_2, \\ldots, c_n$ and an $n \\times n$ table $R$ of exchange rates, such that one unit of currency $c_i$ buys $R[i, j]$ units of currency $c_j$.\n\n\na.\n Give an efficient algorithm to determine whether or not there exists a sequence of currencies $\\langle c_{i_1}, c_{i_2}, \\ldots, c_{i_k} \\rangle$ such that\n\n\n$$R[i_1, i_2] \\cdot R[i_2, i_3] \\ldots R[i_{k - 1}, i_k] \\cdot R[i_k, i_1] > 1.$$\n\n\nAnalyze the running time of your algorithm.\n\n\nb.\n Give an efficient algorithm to print out such a sequence if one exists. Analyze the running time of your algorithm.\n\n\n\n\na.\n We can use the Bellman-Ford algorithm on a suitable weighted, directed graph $G = (V, E)$, which we form as follows. There is one vertex in $V$ for each currency, and for each pair of currencies $c_i$ and $c_j$, there are directed edges $(v_i, v_j)$ and $(v_j , v_i)$. (Thus, $|V| = n$ and $|E| = n(n - 1)$.)\n\n\nWe are looking for a cycle $\\langle i_1, i_2, i_3, \\ldots, i_k, i_1 \\rangle$ such that\n\n\n$$R[i_1, i_2] \\cdot R[i_2, i_3] \\ldots R[i_{k - 1}, i_k] \\cdot R[i_k, i_1] > 1.$$\n\n\nTaking logarithms of both sides of this inequality gives\n\n\n$$\\lg R[i_1, i_2] + \\lg R[i_2, i_3] + \\cdots + \\lg R[i_{k - 1}, i_k] + \\lg R[i_k, i_1] > 0.$$\n\n\nIf we negate both sides, we get\n\n\n$$(-\\lg R[i_1, i_2]) + (-\\lg R[i_2, i_3]) + \\cdots + (-\\lg R[i_{k - 1}, i_k]) + (-\\lg R[i_k, i_1]) < 0,$$\n\n\nand so we want to determine whether $G$ contains a negative-weight cycle with these edge weights.\n\n\nWe can determine whether there exists a negative-weight cycle in $G$ by adding an extra vertex $v_0$ with $0$-weight edges $(v_0, v_i)$ for all $v_i \\in V$, running $\\text{BELLMAN-FORD}$ from $v_0$, and using the boolean result of $\\text{BELLMAN-FORD}$ (which is $\\text{TRUE}$ if there are no negative-weight cycles and $\\text{FALSE}$ if there is a negative-weight cycle) to guide our answer. That is, we invert the boolean result of $\\text{BELLMAN-FORD}$.\n\n\nThis method works because adding the new vertex $v_0$ with $0$-weight edges from $v_0$ to all other vertices cannot introduce any new cycles, yet it ensures that all negative-weight cycles are reachable from $v_0$ .\n\n\nIt takes $\\Theta(n^2)$ time to create $G$, which has $\\Theta(n^2)$ edges. Then it takes $O(n^3)$ time to run $\\text{BELLMAN-FORD}$. Thus, the total time is $O(n^3)$.\n\n\nAnother way to determine whether a negative-weight cycle exists is to create $G$ and, without adding $v_0$ and its incident edges, run either of the all-pairs shortestpaths algorithms. If the resulting shortest-path distance matrix has any negative values on the diagonal, then there is a negative-weight cycle.\n\n\nb.\n Note: The solution to this part also serves as a solution to Exercise 24.1-6.\n\n\nAssuming that we ran $\\text{BELLMAN-FORD}$ to solve part (a), we only need to find the vertices of a negative-weight cycle. We can do so as follows. Go through the edges once again. Once we find an edge $(u, v)$ for which $u.d + w(u, v) < v.d$, then we know that either vertex $v$ is on a negative-weight cycle or is reachable from one. We can find a vertex on the negative-weight cycle by tracing back the $v$ values from $v$, keeping track of which vertices we've visited until we reach a vertex $x$ that we've visited before. Then we can trace back $v$ values from $x$ until we get back to $x$, and all vertices in between, along with $x$, will constitute a negative-weight cycle. We can use the recursive method given by the $\\text{PRINTPATH}$ procedure of Section 22.2, but stop it when it returns to vertex $x$.\n\n\nThe running time is $O(n^3)$ to run $\\text{BELLMAN-FORD}$, plus $O(m)$ to check all the edges and $O(n)$ to print the vertices of the cycle, for a total of $O(n^3)$ time.",
            "title": "24-3 Arbitrage"
        },
        {
            "location": "/Chap24/Problems/24-4/",
            "text": "A \nscaling\n algorithm solves a problem by initially considering only the highestorder bit of each relevant input value (such as an edge weight). It then refines the initial solution by looking at the two highest-order bits. It progressively looks at more and more high-order bits, refining the solution each time, until it has examined all bits and computed the correct solution.\n\n\nIn this problem, we examine an algorithm for computing the shortest paths from a single source by scaling edge weights. We are given a directed graph $G = (V, E)$ with nonnegative integer edge weights $w$. Let $W = \\max_{(u, v) \\in E} \\{w(u, v)\\}$. Our goal is to develop an algorithm that runs in $O(E\\lg W)$ time. We assume that all vertices are reachable from the source.\n\n\nThe algorithm uncovers the bits in the binary representation of the edge weights one at a time, from the most significant bit to the least significant bit. Specifically, let $k = \\lceil \\lg(W + 1) \\rceil$ be the number of bits in the binary representation of $W$, and for $i = 1, 2, \\ldots, k$, let $w_i(u, v) = \\lfloor w(u, v) / 2^{k - i} \\rfloor$. That is, $w_i(u, v)$ is the ''scaled-down'' version of $w(u, v)$ given by the $i$ most significant bits of $w(u, v)$. (Thus, $w_k(u, v) = w(u, v)$ for all $(u, v) \\in E$.) For example, if $k = 5$ and $w(u, v) = 25$, which has the binary representation $\\langle 11001 \\rangle$, then $w_3(u, v) = \\langle 110 \\rangle = 6$. As another example with $k = 5$, if $w(u, v) = \\langle 00100 \\rangle = 4$, then $w_3(u, v) = \\langle 001 \\rangle = 1$. Let us define $\\delta_i(u, v)$ as the shortest-path weight from vertex $u$ to vertex $v$ using weight function $w_i$. Thus, $\\delta_k(u, v) = \\delta(u, v)$ for all $u, v \\in V$. For a given source vertex $s$, the scaling algorithm first computes the shortest-path weights $\\delta_1(s, v)$ for all $v \\in V$, then computes $\\delta_2(s, v)$ for all $v \\in V$, and so on, until it computes $\\delta_k(s, v)$ for all $v \\in V$. We assume throughout that $|E| \\ge |V| - 1$, and we shall see that computing $\\delta_i$ from $\\delta_{i - 1}$ takes $O(E)$ time, so that the entire algorithm takes $O(kE) = O(E\\lg W)$ time.\n\n\na.\n Suppose that for all vertices $v \\in V$, we have $\\delta(s, v) \\le |E|$. Show that we can compute $\\delta(s, v)$ for all $v \\in V$ in $O(E)$ time.\n\n\nb.\n Show that we can compute $\\delta_1(s, v)$ for all $v \\in V$ in $O(E)$ time. \n\n\nLet us now focus on computing $\\delta_i$ from $\\delta_{i - 1}$.\n\n\nc.\n Prove that for $i = 2, 3, \\ldots, k$, we have either $w_i(u, v) = 2w_{i - 1}(u, v)$ or $w_i(u, v) = 2w_{i - 1}(u, v) + 1$. Then, prove that \n\n\n$$2\\delta_{i - 1}(s, v) \\le \\delta_i(s, v) \\le 2\\delta_{i - 1}(s, v) + |V| - 1$$\n\n\nfor all $v \\in V$.\n\n\nd.\n Define for $i = 2, 3, \\ldots, k$ and all $(u, v) \\in E$, \n\n\n$$\\hat w_i = w_i(u, v) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v).$$\n\n\nProve that for $i = 2, 3, \\ldots, k$ and all $u, v \\in V$, the ''reweighted'' value $\\hat w_i(u, v)$ of edge $(u, v)$ is a nonnegative integer.\n\n\ne.\n Now, define $\\delta_i(s, v)$ as the shortest-path weight from $s$ to $v$ using the weight function $\\hat w_i$. Prove that for $i = 2, 3, \\ldots, k$ and all $v \\in V$, \n\n\n$$\\delta_i(s, v) = \\hat\\delta_i(s, v) + 2\\delta_{i - 1}(s, v)$$\n\n\nand that $\\hat\\delta_i(s, v) \\le |E|$.\n\n\nf.\n Show how to compute $\\delta_i(s, v)$ from $\\delta_{i - 1}(s, v)$ for all $v \\in V$ in $O(E)$ time, and conclude that we can compute $\\delta(s, v)$ for all $v \\in V$ in $O(E\\lg W)$ time.\n\n\n\n\na.\n Since all weights are nonnegative, use Dijkstra's algorithm. Implement the priority queue as an array $Q[0..|E| + 1]$, where $Q[i]$ is a list of vertices $v$ for which $v.d = i$. Initialize $v.d$ for $v \\ne s$ to $|E| + 1$ instead of to $\\infty$, so that all vertices have a place in $Q$. (Any initial $v.d > \\delta(s, v)$ works in the algorithm, since $v.d$ decreases until it reaches $\\delta(s, v)$.)\n\n\nThe $|V|$ $\\text{EXTRACT-MIN}$s can be done in $O(E)$ total time, and decreasing a $d$ value during relaxation can be done in $O(1)$ time, for a total running time of $O(E)$.\n\n\n\n\nWhen $v.d$ decreases, just add $v$ to the front of the list in $Q[v.d]$.\n\n\n$\\text{EXTRACT-MIN}$ removes the head of the list in the first nonempty slot of $Q$. To do $\\text{EXTRACT-MIN}$ without scanning all of $Q$, keep track of the smallest $i$ for which $Q[i]$ is not empty. The key point is that when $v.d$ decreases due to relaxation of edge $(u, v)$, $v.d$ remains $u.d$, so it never moves to an earlier slot of $Q$ than the one that had $u$, the previous minimum. Thus $\\text{EXTRACT-MIN}$ can always scan upward in the array, taking a total of $O(E)$ time for all $\\text{EXTRACT-MIN}$s.\n\n\n\n\nb.\n For all $(u, v) \\in E$, we have $w_1(u, v) \\in \\{0, 1\\}$, so $\\delta_1(s, v) \\le |V| - 1 \\le |E|$. Use part (a) to get the $O(E)$ time bound.\n\n\nc.\n To show that $w_i(u, v) = 2w_{i - 1}(u, v)$ or $w_i(u, v) = 2w_{i - 1}(u, v) + 1$, observe that the $i$ bits of $w_i(u, v)$ consist of the $i - 1$ bits of $w_{i - 1}(u, v)$ followed by one more bit. If that low-order bit is $0$, then $w_i(u, v) = 2w_{i - 1}(u, v)$; if it is $1$, then $w_i(u, v) = 2w_{i - 1}(u, v) + 1$.\n\n\nNotice the following two properties of shortest paths:\n\n\n\n\nIf all edge weights are multiplied by a factor of $c$, then all shortest-path weights are multiplied by $c$.\n\n\nIf all edge weights are increased by at most $c$, then all shortest-path weights are increased by at most $c(|V| - 1)$, since all shortest paths have at most $|V| - 1$ edges.\n\n\n\n\nThe lowest possible value for $w_i(u, v)$ is $2w_{i - 1}(u, v)$, so by the first observation, the lowest possible value for $\\delta_i(s, v)$ is $2\\delta_{i - 1}(s, v)$.\n\n\nThe highest possible value for $w_i(u, v)$ is $2w_{i - 1}(u, v) + 1$. Therefore, using the two observations together, the highest possible value for $\\delta_i(s, v)$ is $2\\delta_{i - 1}(s, v) + |V| - 1$\n\n\nd.\n We have \n\n\n\\begin{align}\n\\hat w_i(u, v)\n    & =   w_i(u, v) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v) \\\\\n    & \\ge 2w_{i - 1}(u, v) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v) \\\\\n    & \\ge 0.\n\\end{align}\n\n\nThe second line follows from part (c), and the third line follows from Lemma 24.10: $\\delta_{i - 1}(s, v) \\le \\delta_{i - 1}(s, u) + w_{i - 1}(u, v)$.\n\n\ne.\n Observe that if we compute $\\hat w_i(p)$ for any path $p:u \\leadsto v$, the terms $\\delta_{i - 1}(s, t)$ cancel for every intermediate vertex $t$ on the path. Thus,\n\n\n$$\\hat w_i(p) = w_i(p) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v).$$\n\n\n(This relationship will be shown in detail in equation ($\\text{25.10}$) within the proof of Lemma 25.1.) The $\\delta_{i - 1}$ terms depend only on $u$, $v$, and $s$, but not on the path $p$; therefore the same paths will be of minimum $w_i$ weight and of minimum $\\hat w_i$ weight between $u$ and $v$. Letting $u = s$, we get \n\n\n\\begin{align}\n\\hat\\delta_i(s, v)\n    & = \\delta_i(s, v) + 2\\delta_{i - 1}(s, s) - 2\\delta_{i - 1}(s, v) \\\\\n    & = \\delta_i(s, v) - 2\\delta_{i - 1}(s, v).\n\\end{align}\n\n\nRewriting this result as $\\delta_i(s, v) = \\hat\\delta_i(s, v) + 2\\delta_{i - 1}(s, v)$ and combining it with $\\delta_i(s, v) \\le 2\\delta_{i - 1}(s, v) + |V| - 1$ (from part (c)) gives us $\\hat\\delta_i(s, v) \\le |V| - 1 \\le |E|$.\n\n\nf.\n To compute $\\delta_i(s, v)$ from $\\delta_{i - 1}(s, v)$ for all $v \\in V$ in $O(E)$ time:\n\n\n\n\nCompute the weights $\\hat w_i(u, v)$ in $O(E)$ time, as shown in part (d).\n\n\nBy part (e), $\\hat\\delta_i(s, v) \\le |E|$, so use part (a) to compute all $\\hat\\delta_i(s, v)$ in $O(E)$ time.\n\n\nCompute all $\\delta_i(s, v)$ from $\\hat\\delta_i(s, v)$ and $\\delta_{i - 1}(s, v)$ as shown in part (e), in $O(V)$ time.\n\n\n\n\nTo compute all $\\delta(s, v)$ in $O(E\\lg W)$ time:\n\n\n\n\nCompute $\\delta_1(s, v)$ for all $v \\in V$. As shown in part (b), this takes $O(E)$ time.\n\n\nFor each $i = 2, 3, \\ldots, k$, compute all $\\delta_i(s, v)$ from $\\delta_{i - 1}(s, v)$ in $O(E)$ time as shown above. This procedure computes $\\delta(s, v) = \\delta_k(u, v)$ in time $O(Ek) = O(E\\lg W)$.",
            "title": "24-4 Gabow's scaling algorithm for single-source shortest paths"
        },
        {
            "location": "/Chap24/Problems/24-5/",
            "text": "Let $G = (V, E)$ be a directed graph with weight function $w: E \\to \\mathbb R$, and let $n = |V|$. We define the \nmean weight\n of a cycle $c = \\langle e_1, e_2, \\ldots, e_k \\rangle$ of edges in $E$ to be\n\n\n$$\\mu(c) = \\frac{1}{k} \\sum_{i = 1}^k w(e_i).$$\n\n\nLet $\\mu^* = \\min_c \\mu(c)$, where $c$ ranges over all directed cycles in $G$. We call a cycle $c$ for which $\\mu(c) = \\mu^*$ a  \nminimum mean-weight cycle\n. This problem investigates an efficient algorithm for computing $\\mu^*$.\n\n\nAssume without loss of generality that every vertex $v \\in V$ is reachable from a source vertex $s \\in V$. Let $\\delta(s, v)$ be the weight of a shortest path from $s$ to $v$, and let $\\delta_k(s, v)$ be the weight of a shortest path from $s$ to $v$ consisting of \nexactly\n $k$ edges. If there is no path from $s$ to $v$ with exactly $k$ edges, then $\\delta_k(s, v) = \\infty$.\n\n\na.\n Show that if $\\mu^* = 0$, then $G$ contains no negative-weight cycles and $\\delta(s, v) = \\min_{0 \\le k \\le n - 1} \\delta_k(s, v)$ for all vertices $v \\in V$.\n\n\nb.\n Show that if $\\mu^* = 0$, then\n\n\n$$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\ge 0$$\n\n\nfor all vertices $v \\in V$. ($\\textit{Hint:}$ Use both properties from part (a).)\n\n\nc.\n Let $c$ be a $0$-weight cycle, and let $u$ and $v$ be any two vertices on $c$. Suppose that $\\mu^* = 0$ and that the weight of the simple path from $u$ to $v$ along the cycle is $x$. Prove that $\\delta(s, v) = \\delta(s, u) + x$. ($\\textit{Hint:}$ The weight of the simple path from $v$ to $u$ along the cycle is $-x$.)\n\n\nd.\n Show that if $\\mu^* = 0$, then on each minimum mean-weight cycle there exists a vertex $v$ such that\n\n\n$$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$\n\n\n($\\textit{Hint:}$ Show how to extend a shortest path to any vertex on a minimum meanweight cycle along the cycle to make a shortest path to the next vertex on the cycle.)\n\n\ne.\n Show that if $\\mu^* = 0$, then\n\n\n$$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$\n\n\nf.\n Show that if we add a constant $t$ to the weight of each edge of $G$, then $\\mu^*$ increases by $t$. Use this fact to show that\n\n\n$$\\mu^* = \\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k}.$$\n\n\ng.\n Give an $O(VE)$-time algorithm to compute $\\mu^*$.\n\n\n\n\na.\n If $\\mu^* = 0$, then we have that the lowest that $\\frac{1}{k}\n{i = 1}^k w(e_i)$ can be zero. This means that the lowest $\\sum\n{i = 1}^k w(e_i)$ can be $0$. This means that no cycle can have negative weight. Also, we know that for any path from $s$ to $v$, we can make it simple by removing any cycles that occur. This means that it had a weight equal to some path that has at most $n - 1$ edges in it. Since we take the minimum over all possible number of edges, we have the minimum over all paths.\n\n\nb.\n To show that\n\n\n$$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\ge 0,$$\n\n\nwe need to show that\n\n\n$$\\max_{0 \\le k \\le n - 1} \\delta_n(s, v) - \\delta_k(s, v) \\ge 0.$$\n\n\nSince we have that $\\mu^* = 0$, there aren't any negative weight cycles. This means that we can't have the minimum cost of a path decrease as we increase the possible length of the path past $n - 1$. This means that there will be a path that at least ties for cheapest when we restrict to the path being less than length $n$. Note that there may also be cheapest path of longer length since we necessarily do have zero cost cycles. However, this isn't guaranteed since the zero cost cycle may not lie along a cheapest path from $s$ to $v$.\n\n\nc.\n Since the total cost of the cycle is $0$, and one part of it has cost $x$, in order to balance that out, the weight of the rest of the cycle has to be $-x$. So, suppose we have some shortest length path from $s$ to $u$, then, we could traverse the path from $u$ to $v$ along the cycle to get a path from $s$ to $u$ that has length $\\delta(s, u) + x$. This gets us that $\\delta(s, v) \\le \\delta(s, u) + x$. \n\n\nTo see the converse inequality, suppose that we have some shortest length path from $s$ to $v$. Then, we can traverse the cycle going from $v$ to $u$. We already said that this part of the cycle had total cost $-x$. This gets us that $\\delta(s, u) \\le \\delta(s, v) - x$. Or, rearranging, we have $\\delta(s, u) + x \\le \\delta(s, v)$. Since we have inequalities both ways, we must have equality.\n\n\nd.\n To see this, we find a vertex $v$ and natural number $k \\le n - 1$ so that $\\delta_n(s, v) - \\delta_k(s, v) = 0$. To do this, we will first take any shortest length, smallest number of edges path from $s$ to any vertex on the cycle. Then, we will just keep on walking around the cycle until we've walked along $n$ edges. Whatever vertex we end up on at that point will be our $v$. Since we did not change the $d$ value of $v$ after looking at length $n$ paths, by part (a), we know that there was some length of this path, say $k$, which had the same cost. That is, we have $\\delta_n(s, v) = \\delta_k(s,v)$.\n\n\ne.\n This is an immediate result of the previous problem and part (b). Part (a) says that the inequality holds for all $v$, so, we have\n\n\n$$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta(s, v)}{n - k} \\ge 0.$$\n\n\nThe previous part says that there is some $v$ on each minimum weight cycle so that\n\n\n$$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta(s, v)}{n - k} = 0,$$\n\n\nwhich means that\n\n\n$$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\le 0.$$\n\n\nPutting the two inequalities together, we have the desired equality.\n\n\nf.\n If we add $t$ to the weight of each edge, the mean weight of any cycle becomes\n\n\n$$\\mu(c) = \\frac{1}{k} \\sum_{i = 1}^k (w(e_i) + t) = \\frac{1}{k} \\Big(\\sum_i^k w(e_i) \\Big) + \\frac{kt}{k} = \\frac{1}{k} \\Big(\\sum_i^k w(e_i) \\Big) + t.$$\n\n\nThis is the original, unmodified mean weight cycle, plus $t$. Since this is how the mean weight of every cycle is changed, the lowest mean weight cycle stays the lowest mean weight cycle. This means that $\\mu^*$ will increase by $t$. Suppose that we first compute $\\mu^*$. Then, we subtract from every edge weight the value $\\mu^*$. This will make the new $\\mu^*$ equal zero, which by part (e) means that\n\n\n$$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$\n\n\nSince they are both equal to zero, they are both equal to each other.\n\n\ng.\n By the previous part, it suffices to compute the expression on the previ- ous line. We will start by creating a table that lists $\\delta_k(s, v)$ for every $k \\in {1, \\ldots, n}$ and $v \\in V$. This can be done in time $O(V(E + V))$ by creating a $|V|$ by $|V|$ table, where the $k$th row and vth column represent $\\delta)k(s, v)$ when wanting to compute a particular entry, we need look at a number of entries in the previous row equal to the in degree of the vertex we want to compute. \n\n\nSo, summing over the computation required for each row, we need $O(E + V)$. Note that this total runtime can be bumped down to $O(VE)$ by not including in the table any isolated vertices, this will ensure that $E \\in \\Omega(V)$. So, $O(V(E + V))$ becomes $O(VE)$. Once we have this table of values computed, it is simple to just replace each row with the last row minus what it was, and divide each entry by $n - k$, then, find the min column in each row, and take the max of those numbers.",
            "title": "24-5 Karp's minimum mean-weight cycle algorithm"
        },
        {
            "location": "/Chap24/Problems/24-6/",
            "text": "A sequence is \nbitonic\n if it monotonically increases and then monotonically decreases, or if by a circular shift it monotonically increases and then monotonically decreases. For example the sequences $\\langle 1, 4, 6, 8, 3, -2 \\rangle$, $\\langle 9, 2, -4, -10, -5 \\rangle$, and $\\langle 1, 2, 3, 4 \\rangle$ are bitonic, but $\\langle 1, 3, 12, 4, 2, 10 \\rangle$ is not bitonic. (See Problem 15-3 for the bitonic euclidean traveling-salesman problem.)\n\n\nSuppose that we are given a directed graph $G = (V, E)$ with weight function $w: E \\to \\mathbb R$, where all edge weights are unique, and we wish to find single-source shortest paths from a source vertex $s$. We are given one additional piece of information: for each vertex $v \\in V$, the weights of the edges along any shortest path from $s$ to $v$ form a bitonic sequence.\n\n\nGive the most efficient algorithm you can to solve this problem, and analyze its running time.\n\n\n\n\nObserve that a bitonic sequence can increase, then decrease, then increase, or it can decrease, then increase, then decrease. That is, there can be at most two changes of direction in a bitonic sequence. Any sequence that increases, then decreases, then increases, then decreases has a bitonic sequence as a subsequence.\n\n\nNow, let us suppose that we had an even stronger condition than the bitonic property given in the problem: for each vertex $v \\in V$, the weights of the edges along any shortest path from $s$ to $v$ are increasing. Then we could call $\\text{INITIALIZE-SINGLE-SOURCE}$ and then just relax all edges one time, going in increasing order of weight. Then the edges along every shortest path would be relaxed in order of their appearance on the path. (We rely on the uniqueness of edge weights to ensure that the ordering is correct.) The path-relaxation property (Lemma 24.15) would guarantee that we would have computed correct shortest paths from $s$ to each vertex.\n\n\nIf we weaken the condition so that the weights of the edges along any shortest path increase and then decrease, we could relax all edges one time, in increasing order of weight, and then one more time, in decreasing order of weight. That order, along with uniqueness of edge weights, would ensure that we had relaxed the edges of every shortest path in order, and again the path-relaxation property would guarantee that we would have computed correct shortest paths.\n\n\nTo make sure that we handle all bitonic sequences, we do as suggested above. That is, we perform four passes, relaxing each edge once in each pass. The first and third passes relax edges in increasing order of weight, and the second and fourth passes in decreasing order. Again, by the path-relaxation property and the uniqueness of edge weights, we have computed correct shortest paths.\n\n\nThe total time is $O(V + E\\lg V)$, as follows. The time to sort $|E|$ edges by weight is $O(E\\lg E) = O(E\\lg V)$ (since $|E| = O(V^2)$). $\\text{INITIALIZE-SINGLE-SOURCE}$ takes $O(V)$ time. Each of the four passes takes $O(E)$ time. Thus, the total time is $O(E\\lg V + V + E) = O(V + E\\lg V)$.",
            "title": "24-6 Bitonic shortest paths"
        },
        {
            "location": "/Chap25/25.1/",
            "text": "25.1-1\n\n\n\n\nRun $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the weighted, directed graph of Figure 25.2, showing the matrices that result for each iteration of the loop. Then do the same for $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$.\n\n\n\n\n\n\n\n\nInitial:\n\n\n\\begin{pmatrix}\n     0 & \\infty & \\infty & \\infty &     -1 & \\infty \\\\\n     1 &      0 & \\infty &      2 & \\infty & \\infty \\\\\n\\infty &      2 &      0 & \\infty & \\infty &     -8 \\\\\n    -4 & \\infty & \\infty &      0 &      3 & \\infty \\\\\n\\infty &      7 & \\infty & \\infty &      0 & \\infty \\\\\n\\infty &      5 &     10 & \\infty & \\infty & 0\n\\end{pmatrix}\n\n\n\n\n\n\nSlow:\n\n\n$m = 2$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty & \\infty &     -1 & \\infty \\\\\n-2 &  0 & \\infty &      2 &      0 & \\infty \\\\\n 3 & -3 &      0 &      4 & \\infty &     -8 \\\\\n-4 & 10 & \\infty &      0 &     -5 & \\infty \\\\\n 8 &  7 & \\infty &      9 &      0 & \\infty \\\\\n 6 &  5 &     10 &      7 & \\infty & 0\n\\end{pmatrix}\n\n\n$m = 3$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-2 & -3 &      0 & -1 &  2 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  5 & 0\n\\end{pmatrix}\n\n\n$m = 4$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -3 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}\n\n\n$m = 5$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 & 0  & \\infty \\\\\n 3 &  5 &     10 &  7 & 2  & 0\n\\end{pmatrix}\n\n\n\n\n\n\nFast:\n\n\n$m = 2$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty & \\infty &     -1 & \\infty \\\\\n-2 &  0 & \\infty &      2 &      0 & \\infty \\\\\n 3 & -3 &      0 &      4 & \\infty &     -8 \\\\\n-4 & 10 & \\infty &      0 &     -5 & \\infty \\\\\n 8 &  7 & \\infty &      9 &      0 & \\infty \\\\\n 6 &  5 &     10 &      7 & \\infty & 0\n\\end{pmatrix}\n\n\n$m = 4$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -3 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}\n\n\n$m = 8$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}\n\n\n\n\n\n\n25.1-2\n\n\n\n\nWhy do we require that $w_{ii} = 0$ for all $1 \\le i \\le n$?\n\n\n\n\nThis is consistent with the fact that the shortest path from a vertex to itself is the empty path of weight $0$. If there were another path of weight less than $0$ then it must be a negative-weight cycle, since it starts and ends at $v_i$.\n\n\n25.1-3\n\n\n\n\nWhat does the matrix\n\n\n$$\nL^{(0)} = \n\\begin{pmatrix}\n     0 & \\infty & \\infty & \\cdots & \\infty \\\\\n\\infty &      0 & \\infty & \\cdots & \\infty \\\\\n\\infty & \\infty &      0 & \\cdots & \\infty \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\infty & \\infty & \\infty & \\cdots & 0\n\\end{pmatrix}\n$$\n\n\nused in the shortest-paths algorithms correspond to in regular matrix multiplication?\n\n\n\n\nThe matrix $L^{(0)}$ corresponds to the identity matrix\n\n\n$$\nI =\n\\begin{pmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{pmatrix}\n$$\n\n\nof regular matrix multiplication. Substitute $0$ (the identity for $+$) for $\\infty$ (the identity for $\\min$), and $1$ (the identity for $\\cdot$) for $0$ (the identity for $+$).\n\n\n25.1-4\n\n\n\n\nShow that matrix multiplication defined by $\\text{EXTEND-SHORTEST-PATHS}$ is associative.\n\n\n\n\nTo verify associativity, we need to check that $(W^iW^j)W^p = W^i(W^jW^p)$ for all $i$, $j$ and $p$, where we use the matrix multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Consider entry $(a, b)$ of the left hand side. This is:\n\n\n\\begin{align}\n\\min_{1 \\le k \\le n} [W^iW^j]_{a, k} + W_{k, b}^p\n    & = \\min_{1 \\le k \\le n} \\min_{1 \\le q \\le n} W_{a, q}^i + W_{q, k}^j + W_{k, b}^p \\\\\n    & = \\min_{1 \\le q \\le n} W_{a, q}^i + \\min_{1 \\le k \\le n} W_{q, k}^j + W_{k, b}^p \\\\\n    & = \\min_{1 \\le q \\le n} W_{a, q}^i + [W^jW^p]_{q, b},\n\\end{align}\n\n\nwhich is precisely entry $(a, b)$ of the right hand side.\n\n\n25.1-5\n\n\n\n\nShow how to express the single-source shortest-paths problem as a product of matrices and a vector. Describe how evaluating this product corresponds to a Bellman-Ford-like algorithm (see Section 24.1).\n\n\n\n\nThe all-pairs shortest-paths algorithm in Section 25.1 computes\n\n\n$$L^{(n - 1)} = W^{n - 1} = L^{(0)} \\cdot W^{n - 1},$$\n\n\nwhere $l_{ij}^{(n - 1)} = \\delta(i, j)$ and $L^{(0)}$ is the identity matrix. That is, the entry in the $i$th row and $j$th column of the matrix ''product'' is the shortest-path distance from vertex $i$ to vertex $j$, and row $i$ of the product is the solution to the single-source shortest-paths problem for vertex $i$.\n\n\nNotice that in a matrix ''product'' $C = A \\cdot B$, the $i$th row of $C$ is the $i$th row of $A$ ''multiplied'' by $B$. Since all we want is the $i$th row of $C$, we never need more than the $i$th row of $A$.\n\n\nThus the solution to the single-source shortest-paths from vertex $i$ is $L_i^{(0)} \\cdot W^{n - 1}$, where $L_i^{(0)}$ is the $i$th row of $L^{(0)}$\u2014a vector whose $i$th entry is $0$ and whose other entries are $\\infty$.\n\n\nDoing the above ''multiplications'' starting from the left is essentially the same as the $\\text{BELLMAN-FORD}$ algorithm. The vector corresponds to the $d$ values in $\\text{BELLMAN-FORD}$\u2014the shortest-path estimates from the source to each vertex.\n\n\n\n\nThe vector is initially $0$ for the source and $\\infty$ for all other vertices, the same as the values set up for $d$ by $\\text{INITIALIZE-SINGLE-SOURCE}$.\n\n\nEach ''multiplication'' of the current vector by $W$ relaxes all edges just as $\\text{BELLMAN-FORD}$ does. That is, a distance estimate in the row, say the distance to $v$, is updated to a smaller estimate, if any, formed by adding some $w(u, v)$ to the current estimate of the distance to $u$.\n\n\nThe relaxation/multiplication is done $n - 1$ times.\n\n\n\n\n25.1-6\n\n\n\n\nSuppose we also wish to compute the vertices on shortest paths in the algorithms of this section. Show how to compute the predecessor matrix $\\prod$ from the completed matrix $L$ of shortest-path weights in $O(n^3)$ time.\n\n\n\n\nFor each source vertex $v_i$ we need to compute the shortest-paths tree for $v_i$. To do this, we need to compute the predecessor for each $j \\ne i$. For fixed $i$ and $j$, this is the value of $k$ such that $L_{i, k} + w(k, j) = L_{i, j}$. Since there are $n$ vertices whose trees need computing, $n$ vertices for each such tree whose predecessors need computing, and it takes $O(n)$ to compute this for each one (checking each possible $k$), the total time is $O(n^3)$.\n\n\n25.1-7\n\n\n\n\nWe can also compute the vertices on shortest paths as we compute the shortestpath weights. Define $\\pi_{ij}^{(m)}$ as the predecessor of vertex $j$ on any minimum-weight path from $i$ to $j$ that contains at most $m$ edges. Modify the $\\text{EXTEND-SHORTESTPATHS}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ procedures to compute the matrices$\\prod^{(1)}, \\prod^{(2)}, \\ldots, \\prod^{(n - 1)}$ as the matrices $L^{(1)}, L^{(2)}, \\ldots, L^{(n - 1)}$ are computed.\n\n\n\n\nTo have the procedure compute the predecessor along the shortest path, see the modified procedures, $\\text{EXTEND-SHORTEST-PATH-MOD}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD}$\n\n\nEXTEND\n-\nSHORTEST\n-\nPATH\n-\nMOD\n(\n\u220f\n,\n \nL\n,\n \nW\n)\n\n    \nn\n \n=\n \nL\n.\nrow\n\n    \nLet\n \nL\n'\n \n=\n \n(\nl\n'\n_\n{\ni\n,\n \nj\n})\n \nbe\n \na\n \nnew\n \nn\n \n\u00d7\n \nn\n \nmatirx\n\n    \n\u220f'\n \n=\n \n(\nPI\n'\n_\n{\ni\n,\n \nj\n})\n \nis\n \na\n \nnew\n \nn\n \n\u00d7\n \nn\n \nmatrix\n\n    \nfor\n \ni\n \n=\n \n1\n \nto\n \nn\n\n        \nfor\n \nj\n \n=\n \n1\n \nto\n \nn\n\n            \nl\n'\n_\n{\ni\n,\n \nj\n}\n \n=\n \n\u221e\n\n            \nPI_\n{\ni\n,\n \nj\n}\n \n=\n \nNIL\n\n            \nfor\n \nk\n \n=\n \n1\n \nto\n \nn\n\n                \nif\n \nl_\n{\ni\n,\n \nk\n}\n \n+\n \nl_\n{\nj\n,\n \nk\n}\n \n<\n \nl_\n{\ni\n,\n \nj\n}\n\n                    \nl_\n{\ni\n,\n \nj\n}\n \n=\n \nl_\n{\ni\n,\n \nk\n}\n \n+\n \nl_\n{\nj\n,\n \nk\n}\n\n                    \nPI\n'\n_\n{\ni\n,\n \nj\n}\n \n=\n \nPI_\n{\nk\n,\n \nj\n}\n\n    \nreturn\n \n\u220f'\n,\n \nL\n'\n\n\n\n\n\nSLOW-ALL-PAIRS-SHORTEST-PATHS-MOD(W)\n    n = W.rows\n    L^{(1)} = W\n    \u220f^{(1)} = (PI_{i, j}^{(1)}) where PI_{i, j}^{(1)} = i if there is an edge from i to j, and NIL otherwise.\n    for m = 2 to n - 1\n        \u220f^{(m)}, L^{(m)} = EXTEND-SHORTEST-PATH-MOD(\u220f^{(m - 1)}, L^{(m - 1)}, W)\n    return \u220f^{(n - 1)}, L^{(n - 1)}\n\n\n\n\n25.1-8\n\n\n\n\nThe $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ procedure, as written, requires us to store $\\lceil \\lg(n - 1) \\rceil$ matrices, each with $n^2$ elements, for a total space requirement of $\\Theta(n^2\\lg n)$. Modify the procedure to require only $\\Theta(n^2)$ space by using only two $n \\times n$ matrices.\n\n\n\n\nWe can overwrite matrices as we go. Let $A \\star B$ denote multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Then we modify $\\text{FASTER-ALL-EXTEND-SHORTEST-PATHS}(W)$. We initially create an $n$ by $n$ matrix $L$. Delete line 5 of the algorithm, and change line 6 to $L = W \\star W$, followed by $W = L$.\n\n\n25.1-9\n\n\n\n\nModify $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ so that it can determine whether the graph contains a negative-weight cycle.\n\n\n\n\nFor the modification, keep computing for one step more than the original, that is, we compute all the way up to $L^{(2k + 1)}$ where $2^k > n - 1$. Then, if there aren't any negative weight cycles, then, we will have that the two matrices should be equal since having no negative weight cycles means that between any two vertices, there is a path that is tied for shortest and contains at most $n - 1$ edges. \n\n\nHowever, if there is a cycle of negative total weight, we know that it's length is at most $n$, so, since we are allowing paths to be larger by $2k \\ge n$ between these two matrices, we have that we would need to have all of the vertices on the cycle have their distance reduce by at least the negative weight of the cycle. Since we can detect exactly when there is a negative cycle, based on when these two matrices are different. This algorithm works. It also only takes time equal to a single matrix multiplication which is littlee oh of the unmodified algorithm.\n\n\n25.1-10\n\n\n\n\nGive an efficient algorithm to find the length (number of edges) of a minimum-length negative-weight cycle in a graph.\n\n\n\n\nRun $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the graph. Look at the diagonal elements of $L^{(m)}$. Return the first value of $m$ for which one (or more) of the diagonal elements ($l_{ii}^{(m)}$) is negative. If $m$ reaches $n + 1$, then stop and declare that there are no negative-weight cycles.\n\n\nLet the number of edges in a minimum-length negative-weight cycle be $m^*$, where $m^* = \\infty$ if the graph has no negative-weight cycles.\n\n\nCorrectness\n\n\nLet's assume that for some value $m^* \\le n$ and some value of $i$, we find that $l_{ii}^{m^*} < 0$. Then the graph has a cycle with $m^*$ edges that goes from vertex $i$ to itself, and this cycle has negative weight (stored in $l_{ii}^{m^*}$). This is the minimum-length negative-weight cycle because $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ computes all paths of $1$ edge, then all paths of $2$ edges, and so on, and all cycles shorter than $m^*$ edges were checked before and did not have negative weight. Now assume that for all $m \\le n$, there is no negative $l_{ii}^{(m)}$ element. Then, there is no negativeweight cycle in the graph, because all cycles have length at most $n$.\n\n\nTime\n\n\n$O(n^4)$. More precisely, $\\Theta(n^3 \\cdot \\min(n, m^*))$.\n\n\nFaster solution\n\n\nRun $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ on the graph until the first time that the matrix $L^{(m)}$ has one or more negative values on the diagonal, or until we have computed $L^{(m)}$ for some $m > n$. If we find any negative entries on the diagonal, we know that the minimum-length negative-weight cycle has more than $m / 2$ edges and at most $m$ edges. We just need to binary search for the value of $m^*$ in the range $m / 2 < m^* \\le m$. The key observation is that on our way to computing $L^{(m)}$ , we computed $L^{(1)}, L^{(2)}, L^{(4)}, L^{(8)}, \\ldots, L^{(m / 2)}$, and these matrices suffice to compute every matrix we'll need. Here's pseudocode:\n\n\nFIND\n-\nMIN\n-\nLENGTH\n-\nNEG\n-\nWEIGHT\n-\nCYCLE\n(\nW\n)\n\n    \nn\n \n=\n \nW\n.\nrows\n\n    \nL\n(\n1\n)\n \n=\n \nW\n\n    \nm\n \n=\n \n1\n\n    \nwhiel\n \nm\n \n\u2264\n \nn\n \nand\n \nno\n \ndiagonal\n \nentries\n \nof\n \nL\n(\nm\n)\n \nare\n \nnegative\n\n        \nL\n(\n2\nm\n)\n \n=\n \nEXTEND\n-\nSHORTEST\n-\nPATHS\n(\nL\n(\nm\n),\n \nL\n(\nm\n))\n\n        \nm\n \n=\n \n2\nm\n\n    \nif\n \nm\n \n>\n \nn\n \nand\n \nno\n \ndiagonal\n \nentries\n \nof\n \nL\n(\nm\n)\n \nare\n \nnegative\n\n        \nreturn\n \n\"no negative-weight cycles\"\n\n    \neles\n \nif\n \nm\n \n\u2264\n \n2\n\n        \nreturn\n \nm\n\n    \nelse\n\n        \nlow\n \n=\n \nm\n \n/\n \n2\n\n        \nhigh\n \n=\n \nm\n\n        \nd\n \n=\n \nm\n \n/\n \n4\n\n        \nwhile\n \nd\n \n\u2265\n \n1\n\n            \ns\n \n=\n \nlow\n \n+\n \nd\n\n            \nL\n(\ns\n)\n \n=\n \nEXTEND\n-\nSHORTEST\n-\nPATHS\n(\nL\n(\nlow\n),\n \nL\n(\nd\n))\n\n            \nif\n \nL\n(\ns\n)\n \nhas\n \nany\n \nnegative\n \nentries\n \non\n \nthe\n \ndiagonal\n\n                \nhigh\n \n=\n \ns\n\n            \nelse\n \nlow\n \n=\n \ns\n\n            \nd\n \n=\n \nd\n \n/\n \n2\n\n        \nreturn\n \nhigh\n\n\n\n\n\nCorrectness\n \n\n\nIf, after the first \nwhile\n loop, $m > n$ and no diagonal entries of $L^{(m)}$ are negative, then there is no negative-weight cycle. Otherwise, if $m \\le 2$, then either $m = 1$ or $m = 2$, and $L^{(m)}$ is the first matrix with a negative entry on the diagonal. Thus, the correct value to return is $m$.\n\n\nIf $m > 2$, then we maintain an interval bracketed by the values $low$ and $high$, such that the correct value $m^*$ is in the range $low < m^* \\le high$. We use the following loop invariant:\n\n\nLoop invariant:\n At the start of each iteration of the ''\nwhile\n $d \\ge 1$'' loop,\n\n\n\n\n$d = 2^p$ for some integer $p \\ge -1$,\n\n\n$d = (high - low) / 2$,\n\n\n$low < m^* \\le high$.\n\n\n\n\nInitialization:\n Initially, $m$ is an integer power of $2$ and $m > 2$. Since $d = m / 4$, we have that $d$ is an integer power of $2$ and $d > 1 / 2$, so that $d = 2^p$ for some integer $p \\ge 0$. We also have \n\n\n(high - low) / 2 = (m - (m / 2)) / 2 = m / 4 = d.\n\n\nFinally, $L^{(m)}$ has a negative entry on the diagonal and $L^{(m / 2)}$ does not. Since $low = m / 2$ and $high = m$, we have that $low < m^* \\le high$.\n\n\nMaintenance:\n We use $high$, $low$, and $d$ to denote variable values in a given iteration, and $high'$, $low'$, and $d'$ to denote the same variable values in the next iteration. Thus, we wish to show that $d = 2^p$ for some integer $p \\ge -1$ implies $d' = 2^p$ for some integer $p' \\ge -1$, that $d = (high - low) / 2$ implies $d' = (high' - low') / 2$, and that $low < m^* \\le high$ implies $low' < m^* \\le high'$.\n\n\nTo see that $d' = 2^{p'}$, note that $d' = d / 2$, and so $d = 2^{p - 1}$. The condition that $d \\ge 1$ implies that $p \\ge 0$, and so $p' \\ge -1$.\n\n\nWithin each iteration, $s$ is set to $low + d$, and one of the following actions occurs:\n\n\n\n\n\n\nIf $L^{(s)}$ has any negative entries on the diagonal, then $high'$ is set to s and $d'$ is set to $d / 2$. Upon entering the next iteration, \n\n\n$$(high' - low') / 2 = (s - low') / 2 = ((low + d) - low) / 2 = d / 2 = d'.$$\n\n\nSince $L^{(s)}$ has a negative diagonal entry, we know that $m^* \\le s$. Because $high' = s$ and $low'= low$, we have that $low' < m^* \\le high'$.\n\n\n\n\n\n\nIf $L^{(s)}$ has no negative entries on the diagonal, then $low'$ is set to $s$, and $d'$ is set to $d / 2$. Upon entering the next iteration, \n\n\n$$(high' - low') / 2 = (high' - s) / 2 = (high - (low + d)) / 2 = (high - low) / 2 - d / 2 = d - d / 2 = d / 2 = d'.$$\n\n\nSince $L^{(s)}$ has no negative diagonal entries, we know that $m^* > s$. Because $low' = s$ and $high' = high$, we have that $low' < m^* \\le high'$.\n\n\n\n\n\n\nTermination:\n At termination, $d < 1$. Since $d = 2^p$ for some integer $p \\ge -1$, we must have $p = -1$, so that $d = 1 / 2$. By the second part of the loop invariant, if we multiply both sides by $2$, we get that $high - low = 2d = 1$. By the third part of the loop invariant, we know that $low < m^* \\le high$. Since $high - low = 2d = 1$ and $m^* > low$, the only possible value for $m^*$ is high, which the procedure returns.\n\n\nTime\n\n\nIf there is no negative-weight cycle, the first \nwhile\n loop iterates $\\Theta(\\lg n)$ times, and the total time is $\\Theta(n^3\\lg n)$.\n\n\nNow suppose that there is a negative-weight cycle. We claim that each time we call $\\text{EXTEND-SHORTEST-PATHS}(L^{(low)}, L^{(d)})$, we have already computed $L^{(low)}$ and $L^{(d)}$. Initially, since $low = m / 2$, we had already computed $L^{(low)}$ in the first \nwhile\n loop. In succeeding iterations of the second \nwhile\n loop, the only way that low changes is when it gets the value of $s$, and we have just computed $L^{(s)}$. As for $L^{(d)}$, observe that $d$ takes on the values $m / 4$, $m / 8$, $m / 16$, $\\ldots$, $1$, and again, we computed all of these $L$ matrices in the first \nwhile\n loop. Thus, the claim is proven. Each of the two \nwhile\n loops iterates $\\Theta(\\lg m^*)$ times. Since we have already computed the parameters to each call of $\\text{EXTEND-SHORTEST-PATHS}$, each iteration is dominated by the $\\Theta(n^3)$-time call to $\\text{EXTEND-SHORTEST-PATHS}$. Thus, the total time is $\\Theta(n^3\\lg m^*)$.\n\n\nIn general, therefore, the running time is $\\Theta(n^3\\lg\\min(n, m^*))$.\n\n\nSpace\n\n\nThe slower algorithm needs to keep only three matrices at any time, and so its space requirement is $\\Theta(n^3)$. This faster algorithm needs to maintain $\\Theta(\\lg\\min(n, m^*))$ matrices, and so the space requirement increases to $\\Theta(n^3\\lg\\min(n, m^*))$.",
            "title": "25.1 Shortest paths and matrix multiplication"
        },
        {
            "location": "/Chap25/25.1/#251-1",
            "text": "Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the weighted, directed graph of Figure 25.2, showing the matrices that result for each iteration of the loop. Then do the same for $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$.     Initial:  \\begin{pmatrix}\n     0 & \\infty & \\infty & \\infty &     -1 & \\infty \\\\\n     1 &      0 & \\infty &      2 & \\infty & \\infty \\\\\n\\infty &      2 &      0 & \\infty & \\infty &     -8 \\\\\n    -4 & \\infty & \\infty &      0 &      3 & \\infty \\\\\n\\infty &      7 & \\infty & \\infty &      0 & \\infty \\\\\n\\infty &      5 &     10 & \\infty & \\infty & 0\n\\end{pmatrix}    Slow:  $m = 2$:  \\begin{pmatrix}\n 0 &  6 & \\infty & \\infty &     -1 & \\infty \\\\\n-2 &  0 & \\infty &      2 &      0 & \\infty \\\\\n 3 & -3 &      0 &      4 & \\infty &     -8 \\\\\n-4 & 10 & \\infty &      0 &     -5 & \\infty \\\\\n 8 &  7 & \\infty &      9 &      0 & \\infty \\\\\n 6 &  5 &     10 &      7 & \\infty & 0\n\\end{pmatrix}  $m = 3$:  \\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-2 & -3 &      0 & -1 &  2 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  5 & 0\n\\end{pmatrix}  $m = 4$:  \\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -3 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}  $m = 5$:  \\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 & 0  & \\infty \\\\\n 3 &  5 &     10 &  7 & 2  & 0\n\\end{pmatrix}    Fast:  $m = 2$:  \\begin{pmatrix}\n 0 &  6 & \\infty & \\infty &     -1 & \\infty \\\\\n-2 &  0 & \\infty &      2 &      0 & \\infty \\\\\n 3 & -3 &      0 &      4 & \\infty &     -8 \\\\\n-4 & 10 & \\infty &      0 &     -5 & \\infty \\\\\n 8 &  7 & \\infty &      9 &      0 & \\infty \\\\\n 6 &  5 &     10 &      7 & \\infty & 0\n\\end{pmatrix}  $m = 4$:  \\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -3 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}  $m = 8$:  \\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}",
            "title": "25.1-1"
        },
        {
            "location": "/Chap25/25.1/#251-2",
            "text": "Why do we require that $w_{ii} = 0$ for all $1 \\le i \\le n$?   This is consistent with the fact that the shortest path from a vertex to itself is the empty path of weight $0$. If there were another path of weight less than $0$ then it must be a negative-weight cycle, since it starts and ends at $v_i$.",
            "title": "25.1-2"
        },
        {
            "location": "/Chap25/25.1/#251-3",
            "text": "What does the matrix  $$\nL^{(0)} = \n\\begin{pmatrix}\n     0 & \\infty & \\infty & \\cdots & \\infty \\\\\n\\infty &      0 & \\infty & \\cdots & \\infty \\\\\n\\infty & \\infty &      0 & \\cdots & \\infty \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\infty & \\infty & \\infty & \\cdots & 0\n\\end{pmatrix}\n$$  used in the shortest-paths algorithms correspond to in regular matrix multiplication?   The matrix $L^{(0)}$ corresponds to the identity matrix  $$\nI =\n\\begin{pmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{pmatrix}\n$$  of regular matrix multiplication. Substitute $0$ (the identity for $+$) for $\\infty$ (the identity for $\\min$), and $1$ (the identity for $\\cdot$) for $0$ (the identity for $+$).",
            "title": "25.1-3"
        },
        {
            "location": "/Chap25/25.1/#251-4",
            "text": "Show that matrix multiplication defined by $\\text{EXTEND-SHORTEST-PATHS}$ is associative.   To verify associativity, we need to check that $(W^iW^j)W^p = W^i(W^jW^p)$ for all $i$, $j$ and $p$, where we use the matrix multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Consider entry $(a, b)$ of the left hand side. This is:  \\begin{align}\n\\min_{1 \\le k \\le n} [W^iW^j]_{a, k} + W_{k, b}^p\n    & = \\min_{1 \\le k \\le n} \\min_{1 \\le q \\le n} W_{a, q}^i + W_{q, k}^j + W_{k, b}^p \\\\\n    & = \\min_{1 \\le q \\le n} W_{a, q}^i + \\min_{1 \\le k \\le n} W_{q, k}^j + W_{k, b}^p \\\\\n    & = \\min_{1 \\le q \\le n} W_{a, q}^i + [W^jW^p]_{q, b},\n\\end{align}  which is precisely entry $(a, b)$ of the right hand side.",
            "title": "25.1-4"
        },
        {
            "location": "/Chap25/25.1/#251-5",
            "text": "Show how to express the single-source shortest-paths problem as a product of matrices and a vector. Describe how evaluating this product corresponds to a Bellman-Ford-like algorithm (see Section 24.1).   The all-pairs shortest-paths algorithm in Section 25.1 computes  $$L^{(n - 1)} = W^{n - 1} = L^{(0)} \\cdot W^{n - 1},$$  where $l_{ij}^{(n - 1)} = \\delta(i, j)$ and $L^{(0)}$ is the identity matrix. That is, the entry in the $i$th row and $j$th column of the matrix ''product'' is the shortest-path distance from vertex $i$ to vertex $j$, and row $i$ of the product is the solution to the single-source shortest-paths problem for vertex $i$.  Notice that in a matrix ''product'' $C = A \\cdot B$, the $i$th row of $C$ is the $i$th row of $A$ ''multiplied'' by $B$. Since all we want is the $i$th row of $C$, we never need more than the $i$th row of $A$.  Thus the solution to the single-source shortest-paths from vertex $i$ is $L_i^{(0)} \\cdot W^{n - 1}$, where $L_i^{(0)}$ is the $i$th row of $L^{(0)}$\u2014a vector whose $i$th entry is $0$ and whose other entries are $\\infty$.  Doing the above ''multiplications'' starting from the left is essentially the same as the $\\text{BELLMAN-FORD}$ algorithm. The vector corresponds to the $d$ values in $\\text{BELLMAN-FORD}$\u2014the shortest-path estimates from the source to each vertex.   The vector is initially $0$ for the source and $\\infty$ for all other vertices, the same as the values set up for $d$ by $\\text{INITIALIZE-SINGLE-SOURCE}$.  Each ''multiplication'' of the current vector by $W$ relaxes all edges just as $\\text{BELLMAN-FORD}$ does. That is, a distance estimate in the row, say the distance to $v$, is updated to a smaller estimate, if any, formed by adding some $w(u, v)$ to the current estimate of the distance to $u$.  The relaxation/multiplication is done $n - 1$ times.",
            "title": "25.1-5"
        },
        {
            "location": "/Chap25/25.1/#251-6",
            "text": "Suppose we also wish to compute the vertices on shortest paths in the algorithms of this section. Show how to compute the predecessor matrix $\\prod$ from the completed matrix $L$ of shortest-path weights in $O(n^3)$ time.   For each source vertex $v_i$ we need to compute the shortest-paths tree for $v_i$. To do this, we need to compute the predecessor for each $j \\ne i$. For fixed $i$ and $j$, this is the value of $k$ such that $L_{i, k} + w(k, j) = L_{i, j}$. Since there are $n$ vertices whose trees need computing, $n$ vertices for each such tree whose predecessors need computing, and it takes $O(n)$ to compute this for each one (checking each possible $k$), the total time is $O(n^3)$.",
            "title": "25.1-6"
        },
        {
            "location": "/Chap25/25.1/#251-7",
            "text": "We can also compute the vertices on shortest paths as we compute the shortestpath weights. Define $\\pi_{ij}^{(m)}$ as the predecessor of vertex $j$ on any minimum-weight path from $i$ to $j$ that contains at most $m$ edges. Modify the $\\text{EXTEND-SHORTESTPATHS}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ procedures to compute the matrices$\\prod^{(1)}, \\prod^{(2)}, \\ldots, \\prod^{(n - 1)}$ as the matrices $L^{(1)}, L^{(2)}, \\ldots, L^{(n - 1)}$ are computed.   To have the procedure compute the predecessor along the shortest path, see the modified procedures, $\\text{EXTEND-SHORTEST-PATH-MOD}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD}$  EXTEND - SHORTEST - PATH - MOD ( \u220f ,   L ,   W ) \n     n   =   L . row \n     Let   L '   =   ( l ' _ { i ,   j })   be   a   new   n   \u00d7   n   matirx \n     \u220f'   =   ( PI ' _ { i ,   j })   is   a   new   n   \u00d7   n   matrix \n     for   i   =   1   to   n \n         for   j   =   1   to   n \n             l ' _ { i ,   j }   =   \u221e \n             PI_ { i ,   j }   =   NIL \n             for   k   =   1   to   n \n                 if   l_ { i ,   k }   +   l_ { j ,   k }   <   l_ { i ,   j } \n                     l_ { i ,   j }   =   l_ { i ,   k }   +   l_ { j ,   k } \n                     PI ' _ { i ,   j }   =   PI_ { k ,   j } \n     return   \u220f' ,   L '   SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD(W)\n    n = W.rows\n    L^{(1)} = W\n    \u220f^{(1)} = (PI_{i, j}^{(1)}) where PI_{i, j}^{(1)} = i if there is an edge from i to j, and NIL otherwise.\n    for m = 2 to n - 1\n        \u220f^{(m)}, L^{(m)} = EXTEND-SHORTEST-PATH-MOD(\u220f^{(m - 1)}, L^{(m - 1)}, W)\n    return \u220f^{(n - 1)}, L^{(n - 1)}",
            "title": "25.1-7"
        },
        {
            "location": "/Chap25/25.1/#251-8",
            "text": "The $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ procedure, as written, requires us to store $\\lceil \\lg(n - 1) \\rceil$ matrices, each with $n^2$ elements, for a total space requirement of $\\Theta(n^2\\lg n)$. Modify the procedure to require only $\\Theta(n^2)$ space by using only two $n \\times n$ matrices.   We can overwrite matrices as we go. Let $A \\star B$ denote multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Then we modify $\\text{FASTER-ALL-EXTEND-SHORTEST-PATHS}(W)$. We initially create an $n$ by $n$ matrix $L$. Delete line 5 of the algorithm, and change line 6 to $L = W \\star W$, followed by $W = L$.",
            "title": "25.1-8"
        },
        {
            "location": "/Chap25/25.1/#251-9",
            "text": "Modify $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ so that it can determine whether the graph contains a negative-weight cycle.   For the modification, keep computing for one step more than the original, that is, we compute all the way up to $L^{(2k + 1)}$ where $2^k > n - 1$. Then, if there aren't any negative weight cycles, then, we will have that the two matrices should be equal since having no negative weight cycles means that between any two vertices, there is a path that is tied for shortest and contains at most $n - 1$ edges.   However, if there is a cycle of negative total weight, we know that it's length is at most $n$, so, since we are allowing paths to be larger by $2k \\ge n$ between these two matrices, we have that we would need to have all of the vertices on the cycle have their distance reduce by at least the negative weight of the cycle. Since we can detect exactly when there is a negative cycle, based on when these two matrices are different. This algorithm works. It also only takes time equal to a single matrix multiplication which is littlee oh of the unmodified algorithm.",
            "title": "25.1-9"
        },
        {
            "location": "/Chap25/25.1/#251-10",
            "text": "Give an efficient algorithm to find the length (number of edges) of a minimum-length negative-weight cycle in a graph.   Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the graph. Look at the diagonal elements of $L^{(m)}$. Return the first value of $m$ for which one (or more) of the diagonal elements ($l_{ii}^{(m)}$) is negative. If $m$ reaches $n + 1$, then stop and declare that there are no negative-weight cycles.  Let the number of edges in a minimum-length negative-weight cycle be $m^*$, where $m^* = \\infty$ if the graph has no negative-weight cycles.  Correctness  Let's assume that for some value $m^* \\le n$ and some value of $i$, we find that $l_{ii}^{m^*} < 0$. Then the graph has a cycle with $m^*$ edges that goes from vertex $i$ to itself, and this cycle has negative weight (stored in $l_{ii}^{m^*}$). This is the minimum-length negative-weight cycle because $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ computes all paths of $1$ edge, then all paths of $2$ edges, and so on, and all cycles shorter than $m^*$ edges were checked before and did not have negative weight. Now assume that for all $m \\le n$, there is no negative $l_{ii}^{(m)}$ element. Then, there is no negativeweight cycle in the graph, because all cycles have length at most $n$.  Time  $O(n^4)$. More precisely, $\\Theta(n^3 \\cdot \\min(n, m^*))$.  Faster solution  Run $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ on the graph until the first time that the matrix $L^{(m)}$ has one or more negative values on the diagonal, or until we have computed $L^{(m)}$ for some $m > n$. If we find any negative entries on the diagonal, we know that the minimum-length negative-weight cycle has more than $m / 2$ edges and at most $m$ edges. We just need to binary search for the value of $m^*$ in the range $m / 2 < m^* \\le m$. The key observation is that on our way to computing $L^{(m)}$ , we computed $L^{(1)}, L^{(2)}, L^{(4)}, L^{(8)}, \\ldots, L^{(m / 2)}$, and these matrices suffice to compute every matrix we'll need. Here's pseudocode:  FIND - MIN - LENGTH - NEG - WEIGHT - CYCLE ( W ) \n     n   =   W . rows \n     L ( 1 )   =   W \n     m   =   1 \n     whiel   m   \u2264   n   and   no   diagonal   entries   of   L ( m )   are   negative \n         L ( 2 m )   =   EXTEND - SHORTEST - PATHS ( L ( m ),   L ( m )) \n         m   =   2 m \n     if   m   >   n   and   no   diagonal   entries   of   L ( m )   are   negative \n         return   \"no negative-weight cycles\" \n     eles   if   m   \u2264   2 \n         return   m \n     else \n         low   =   m   /   2 \n         high   =   m \n         d   =   m   /   4 \n         while   d   \u2265   1 \n             s   =   low   +   d \n             L ( s )   =   EXTEND - SHORTEST - PATHS ( L ( low ),   L ( d )) \n             if   L ( s )   has   any   negative   entries   on   the   diagonal \n                 high   =   s \n             else   low   =   s \n             d   =   d   /   2 \n         return   high   Correctness    If, after the first  while  loop, $m > n$ and no diagonal entries of $L^{(m)}$ are negative, then there is no negative-weight cycle. Otherwise, if $m \\le 2$, then either $m = 1$ or $m = 2$, and $L^{(m)}$ is the first matrix with a negative entry on the diagonal. Thus, the correct value to return is $m$.  If $m > 2$, then we maintain an interval bracketed by the values $low$ and $high$, such that the correct value $m^*$ is in the range $low < m^* \\le high$. We use the following loop invariant:  Loop invariant:  At the start of each iteration of the '' while  $d \\ge 1$'' loop,   $d = 2^p$ for some integer $p \\ge -1$,  $d = (high - low) / 2$,  $low < m^* \\le high$.   Initialization:  Initially, $m$ is an integer power of $2$ and $m > 2$. Since $d = m / 4$, we have that $d$ is an integer power of $2$ and $d > 1 / 2$, so that $d = 2^p$ for some integer $p \\ge 0$. We also have   (high - low) / 2 = (m - (m / 2)) / 2 = m / 4 = d.  Finally, $L^{(m)}$ has a negative entry on the diagonal and $L^{(m / 2)}$ does not. Since $low = m / 2$ and $high = m$, we have that $low < m^* \\le high$.  Maintenance:  We use $high$, $low$, and $d$ to denote variable values in a given iteration, and $high'$, $low'$, and $d'$ to denote the same variable values in the next iteration. Thus, we wish to show that $d = 2^p$ for some integer $p \\ge -1$ implies $d' = 2^p$ for some integer $p' \\ge -1$, that $d = (high - low) / 2$ implies $d' = (high' - low') / 2$, and that $low < m^* \\le high$ implies $low' < m^* \\le high'$.  To see that $d' = 2^{p'}$, note that $d' = d / 2$, and so $d = 2^{p - 1}$. The condition that $d \\ge 1$ implies that $p \\ge 0$, and so $p' \\ge -1$.  Within each iteration, $s$ is set to $low + d$, and one of the following actions occurs:    If $L^{(s)}$ has any negative entries on the diagonal, then $high'$ is set to s and $d'$ is set to $d / 2$. Upon entering the next iteration,   $$(high' - low') / 2 = (s - low') / 2 = ((low + d) - low) / 2 = d / 2 = d'.$$  Since $L^{(s)}$ has a negative diagonal entry, we know that $m^* \\le s$. Because $high' = s$ and $low'= low$, we have that $low' < m^* \\le high'$.    If $L^{(s)}$ has no negative entries on the diagonal, then $low'$ is set to $s$, and $d'$ is set to $d / 2$. Upon entering the next iteration,   $$(high' - low') / 2 = (high' - s) / 2 = (high - (low + d)) / 2 = (high - low) / 2 - d / 2 = d - d / 2 = d / 2 = d'.$$  Since $L^{(s)}$ has no negative diagonal entries, we know that $m^* > s$. Because $low' = s$ and $high' = high$, we have that $low' < m^* \\le high'$.    Termination:  At termination, $d < 1$. Since $d = 2^p$ for some integer $p \\ge -1$, we must have $p = -1$, so that $d = 1 / 2$. By the second part of the loop invariant, if we multiply both sides by $2$, we get that $high - low = 2d = 1$. By the third part of the loop invariant, we know that $low < m^* \\le high$. Since $high - low = 2d = 1$ and $m^* > low$, the only possible value for $m^*$ is high, which the procedure returns.  Time  If there is no negative-weight cycle, the first  while  loop iterates $\\Theta(\\lg n)$ times, and the total time is $\\Theta(n^3\\lg n)$.  Now suppose that there is a negative-weight cycle. We claim that each time we call $\\text{EXTEND-SHORTEST-PATHS}(L^{(low)}, L^{(d)})$, we have already computed $L^{(low)}$ and $L^{(d)}$. Initially, since $low = m / 2$, we had already computed $L^{(low)}$ in the first  while  loop. In succeeding iterations of the second  while  loop, the only way that low changes is when it gets the value of $s$, and we have just computed $L^{(s)}$. As for $L^{(d)}$, observe that $d$ takes on the values $m / 4$, $m / 8$, $m / 16$, $\\ldots$, $1$, and again, we computed all of these $L$ matrices in the first  while  loop. Thus, the claim is proven. Each of the two  while  loops iterates $\\Theta(\\lg m^*)$ times. Since we have already computed the parameters to each call of $\\text{EXTEND-SHORTEST-PATHS}$, each iteration is dominated by the $\\Theta(n^3)$-time call to $\\text{EXTEND-SHORTEST-PATHS}$. Thus, the total time is $\\Theta(n^3\\lg m^*)$.  In general, therefore, the running time is $\\Theta(n^3\\lg\\min(n, m^*))$.  Space  The slower algorithm needs to keep only three matrices at any time, and so its space requirement is $\\Theta(n^3)$. This faster algorithm needs to maintain $\\Theta(\\lg\\min(n, m^*))$ matrices, and so the space requirement increases to $\\Theta(n^3\\lg\\min(n, m^*))$.",
            "title": "25.1-10"
        },
        {
            "location": "/Chap25/25.2/",
            "text": "25.2-1\n\n\n\n\nRun the Floyd-Warshall algorithm on the weighted, directed graph of Figure 25.2. Show the matrix $D^{(k)}$ that results for each iteration of the outer loop.\n\n\n\n\n$k = 1$:\n\n\n\\begin{pmatrix}\n     0 & \\infty & \\infty & \\infty &     -1 & \\infty \\\\\n     1 &      0 & \\infty &      2 &      0 & \\infty \\\\\n\\infty &      2 &      0 & \\infty & \\infty &     -8 \\\\\n    -4 & \\infty & \\infty &      0 &     -5 & \\infty \\\\\n\\infty &      7 & \\infty & \\infty &      0 & \\infty \\\\\n\\infty &      5 &     10 & \\infty & \\infty & 0\n\\end{pmatrix}\n\n\n$k = 2$:\n\n\n\\begin{pmatrix}\n 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\\n 1 &      0 & \\infty &      2 &  0 & \\infty \\\\\n 3 &      2 &      0 &      4 &  2 & -    8 \\\\\n-4 & \\infty & \\infty &      0 & -5 & \\infty \\\\\n 8 &      7 & \\infty &      9 &  0 & \\infty \\\\\n 6 &      5 &     10 &      7 &  5 & 0\n\\end{pmatrix}\n\n\n$k = 3$:\n\n\n\\begin{pmatrix}\n 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\\n 1 &      0 & \\infty &      2 &  0 & \\infty \\\\\n 3 &      2 &      0 &      4 &  2 &     -8 \\\\\n-4 & \\infty & \\infty &      0 & -5 & \\infty \\\\\n 8 &      7 & \\infty &      9 &  0 & \\infty \\\\\n 6 &      5 &     10 &      7 &  5 & 0\n\\end{pmatrix}\n\n\n$k = 4$:\n\n\n\\begin{pmatrix}\n 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\\n-2 &      0 & \\infty &      2 & -3 & \\infty \\\\\n 0 &      2 &      0 &      4 & -1 &     -8 \\\\\n-4 & \\infty & \\infty &      0 & -5 & \\infty \\\\\n 5 &      7 & \\infty &      9 &  0 & \\infty \\\\\n 3 &      5 &     10 &      7 &  2 & 0\n\\end{pmatrix}\n\n\n$k = 5$:\n\n\n\\begin{pmatrix}\n 0 & 6 & \\infty & 8 & -1 & \\infty \\\\\n-2 & 0 & \\infty & 2 & -3 & \\infty \\\\\n 0 & 2 &      0 & 4 & -1 &     -8 \\\\\n-4 & 2 & \\infty & 0 & -5 & \\infty \\\\\n 5 & 7 & \\infty & 9 &  0 & \\infty \\\\\n 3 & 5 &     10 & 7 &  2 & 0\n\\end{pmatrix}\n\n\n$k = 6$:\n\n\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}\n\n\n25.2-2\n\n\n\n\nShow how to compute the transitive closure using the technique of Section 25.1.\n\n\n\n\nWe set $w_{ij} = 1$ if $(i, j)$ is an edge, and $w_{ij} = 0$ otherwise. Then we replace line 7 of $\\text{EXTEND-SHORTEST-PATHS}(L, W)$ by $l''_{ij} = l''_{ij} \\lor (l_{ik} \\land w_{kj})$. Then run the $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ algorithm.\n\n\n25.2-3\n\n\n\n\nModify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\prod^{(k)}$ matrices according to equations $\\text{(25.6)}$ and $\\text{(25.7)}$. Prove rigorously that for all $i \\in V$, the predecessor subgraph $G_{\\pi, i}$ is a shortest-paths tree with root $i$. ($\\textit{Hint:}$ To show that $G_{\\pi, i}$ is acyclic, first show that $\\pi_{ij}^{(k)} = l$ implies $d_{ij}^{(k)} \\ge d_{il}^{(k)} + w_{lj}$, according to the definition of $\\pi_{ij}^{(k)}$. Then, adapt the proof of Lemma 23.16.)\n\n\n\n\nMOD-FLOYD-WARSHALL(W)\n    n = W.rows\n    D^0 = W\n    PI^0 is a matrix with NIL in every entry\n    for i = 1 to n\n        for j = 1 to n\n            if i \u2260 j and D^0_{i, j} < \u221e\n                PI^0_{i, j} = i\n    for k = 1 to n\n        let D^k be a new n \u00d7 n matrix.\n        let PI^k be a new n \u00d7 n matrix\n        for i = 1 to n\n            for j = 1 to n\n                if d^{k - 1}_{ij} \u2264 d^{k - 1}_{ik} + d^{k - 1}_{kj}\n                    d^k_{ij} = d^{k - 1}_{ij}\n                    PI^k_{ij} = PI^{k - 1}_{ij}\n                else\n                    d^k_{ij} = d^{k - 1}_{ik} + d^{k - 1}_{kj}\n                    PI^k_{ij} = PI^{k - 1}_{kj}\n\n\n\n\nIn order to have that $\\pi^{(k)}_{ij} = l$, we need that $d^{(k)}_{ij} \\ge d^{(k)}_{il} + w_{lj}$. To see this fact, we will note that having $\\pi^{(k)}_{ij} = l$ means that a shortest path from $i$ to $j$ last goes through $l$. A path that last goes through $l$ corresponds to taking a chepest path from $i$ to $l$ and then following the single edge from $l$ to $j$. However, This means that $d_{il} \\le d_{ij} - w_{ij}$, which we can rearrange to get the desired inequality. We can just continue following this inequality\naround, and if we ever get some cycle, $i_1, i_2, \\ldots, i_c$, then we would have that $d_{ii_1} \\le d_{ii_1} + w_{i_1i_2} + w_{i_2i_3} + \\cdots + w_{i_ci_1}$. So, if we subtract the common term sfrom both sides, we get that $0 \\le w_{i_ci_1} + \\sum_{q = 1}^{c - 1} w_{i_qi_{q + 1}}$. So, we have that we would only have a cycle in the precedessor graph if we ahvt that there is a zero weight cycle in the original graph. However, we would never have to go around the weight zero cycle since the constructed path of shortest weight favors ones with a fewer number of edges because of the way that we handle the equality case in equation $\\text{(25.7)}$.\n\n\n25.2-4\n\n\n\n\nAs it appears above, the Floyd-Warshall algorithm requires $\\Theta(n^3)$ space, since we compute $d_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$. Show that the following procedure, which simply drops all the superscripts, is correct, and thus only $\\Theta(n^2)$ space is required.\n\n\nFLOYD\n-\nWARSHALL\n'\n(\nW\n)\n\n    \nn\n \n=\n \nW\n.\nrows\n\n    \nD\n \n=\n \nW\n\n    \nfor\n \nk\n \n=\n \n1\n \nto\n \nn\n\n        \nfor\n \ni\n \n=\n \n1\n \nto\n \nn\n\n            \nfor\n \nj\n \n=\n \n1\n \nto\n \nn\n\n                \nd\n[\ni\n][\nj\n]\n \n=\n \nmin\n(\nd\n[\ni\n][\nj\n],\n \nd\n[\ni\n][\nk\n]\n \n+\n \nd\n[\nk\n][\nj\n])\n\n    \nreturn\n \nD\n\n\n\n\n\n\n\nWith the superscripts, the computation is $d_{ij}^{(k)} = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)})$. If, having dropped the superscripts, we were to compute and store $d_{ik}$ or $d_{kj}$ before using these values to compute $d_{ij}$, we might be computing one of the following:\n\n\n\\begin{align}\nd_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k - 1)}), \\\\\nd_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k)}), \\\\\nd_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k)}),\n\\end{align}\n\n\nIn any of these scenarios, we're computing the weight of a shortest path from $i$ to $j$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. If we use $d_{ik}^{(k)}$, rather than $d_{ik}^{(k - 1)}$, in the computation, then we're using a subpath from $i$ to $k$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. But $k$ cannot be an \nintermediate\n vertex on a shortest path from $i$ to $k$, since otherwise there would be a cycle on this shortest path. Thus, $d_{ik}^{(k)} = d_{ik}^{(k - 1)}$. A similar argument applies to show that $d_{kj}^{(k)} = d_{kj}^{(k - 1)}$. Hence, we can drop the superscripts in the computation.\n\n\n25.2-5\n\n\n\n\nSuppose that we modify the way in which equation $\\text{(25.7)}$ handles equality:\n\n\n$$\n\\pi_{ij}^{(k)} =\n\\begin{cases}\n\\pi_{ij}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} <   d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}, \\\\\n\\pi_{kj}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} \\ge d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}.\n\\end{cases}\n$$\n\n\nIs this alternative definition of the predecessor matrix $\\prod$ correct?\n\n\n\n\nIf we change the way that we handle the equality case, we will still be generating a the correct values for the $\\pi$ matrix. This is because updating the $\\pi$ values to make paths that are longer but still tied for the lowest weight. Making $\\pi_{ij} = \\pi_{kj}$ means that we are making the shortest path from $i$ to $j$ passes through $k$ at some point. This has the same cost as just going from $i$ to $j$, since $d_{ij} = d_{ik} + d_{kj}$.\n\n\n25.2-6\n\n\n\n\nHow can we use the output of the Floyd-Warshall algorithm to detect the presence of a negative-weight cycle?\n\n\n\n\nHere are two ways to detect negative-weight cycles:\n\n\n\n\n\n\nCheck the main-diagonal entries of the result matrix for a negative value. There is a negative weight cycle if and only if $d_{ii}^{(n)} < 0$ for some vertex $i$:\n\n\n\n\n$d_{ii}^{(n)}$ is a path weight from $i$ to itself; so if it is negative, there is a path from $i$ to itself (i.e., a cycle), with negative weight.\n\n\nIf there is a negative-weight cycle, consider the one with the fewest vertices.  \n\n\nIf it has just one vertex, then some $w_{ii} < 0$, so $d_{ii}$ starts out negative, and since $d$ values are never increased, it is also negative when the algorithm terminates.\n\n\nIf it has at least two vertices, let $k$ be the highest-numbered vertex in the cycle, and let $i$ be some other vertex in the cycle. $d_{ik}^{(k - 1)}$ and $d_{ki}^{(k - 1)}$ have correct shortest-path weights, because they are not based on negativeweight cycles. (Neither $d_{ik}^{(k - 1)}$ nor $d_{ki}^{(k - 1)}$ can include $k$ as an intermediate vertex, and $i$ and $k$ are on the negative-weight cycle with the fewest vertices.) Since $i \\leadsto k \\leadsto i$ is a negative-weight cycle, the sum of those two weights is negative, so $d_{ii}^{(k)}$ will be set to a negative value. Since $d$ values are never increased, it is also negative when the algorithm terminates.\n\n\n\n\n\n\n\n\nIn fact, it suffices to check whether $d_{ii}^{(n - 1)} < 0$ for some vertex $i$. Here's why. A negative-weight cycle containing vertex $i$ either contains vertex $n$ or it does not. If it does not, then clearly $d_{ii}^{(n - 1)} < 0$. If the negative-weight cycle contains vertex $n$, then consider $d_{nn}^{(n - 1)}$. This value must be negative, since the cycle, starting and ending at vertex $n$, does not include vertex $n$ as an intermediate vertex. \n\n\n\n\n\n\nAlternatively, one could just run the normal $\\text{FLOYD-WARSHALL}$ algorithm one extra iteration to see if any of the $d$ values change. If there are negative cycles, then some shortest-path cost will be cheaper. If there are no such cycles, then no $d$ values will change because the algorithm gives the correct shortest paths.\n\n\n\n\n\n\n25.2-7\n\n\n\n\nAnother way to reconstruct shortest paths in the Floyd-Warshall algorithm uses values $\\phi_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$, where $\\phi_{ij}^{(k)}$ is the highest-numbered intermediate vertex of a shortest path from $i$ to $j$ in which all intermediate vertices are in the set $\\{1, 2, \\ldots, k \\}$. Give a recursive formulation for $\\phi_{ij}^{(k)}$, modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\phi_{ij}^{(k)}$ values, and rewrite the $\\text{PRINT-ALLPAIRS-SHORTEST-PATH}$ procedure to take the matrix $\\Phi = (\\phi_{ij}^{(n)})$ as an input. How is the matrix $\\Phi$ like the $s$ table in the matrix-chain multiplication problem of Section 15.2?\n\n\n\n\nWe can recursively compute the values of $\\phi_{ij}^{(n)}$ by, letting it be $\\phi_{ij}^{(k - 1)}$ if $d(k) + d_{ik}^{(k)} + d_{ik}^{(k)} \\ge d_{ij}(k - 1)$, and otherwise, let it be $k$. This works correctly because it perfectly captures whether we decided to use vertex $k$ when we were repeatedly allowing ourselves use of each vertex one at a time. To modify Floyd-Warshall to compute this, we would just need to stick within the innermost for loop, something that computes $\\phi(k)$ by this recursive rule, this would only be a constant amount of work in this innermost for loop, and so would not cause the asymptotic runtime to increase. It is similar to the s table in matrix-chain multiplication because it is computed by a similar recurrence.\n\n\nIf we already have the $n^3$ values in $\\phi_{ij}^{(k)}$ provided, then we can reconstruct the shortest path from $i$ to $j$ because we know that the largest vertex in the path from $i$ to $j$ is $\\phi_{ij}^{(n)}$, call it $a_1$. Then, we know that the largest vertex in the path before $a_1$ will be $\\phi_{ia_1}^{(a_1 - 1)}$ and the largest after $a_1$ will be $\\phi_{a_1j}^{(a_1 - 1)}$. By continuing to recurse until we get that the largest element showing up at some point is $\\text{NIL}$, we will be able to continue subdividing the path until it is entirely constructed.\n\n\n25.2-8\n\n\n\n\nGive an $O(VE)$-time algorithm for computing the transitive closure of a directed\ngraph $G = (V, E)$.\n\n\n\n\nCreate an $n$ by $n$ matrix $A$ filled with $0$'s. We are done if we can determine the vertices reachable from a particular vertex in $O(E)$ time, since we can just compute this for each $v \\in V$. To do this, assign each edge weight $1$. Then we have $\\delta(v, u) \\le |E|$ for all $u \\in V$. By Problem 24-4 (a) we can compute $\\delta(v, u)$ in $O(E)$ forall $u \\in V$. If $\\delta(v, u) < \\infty$, set $A_{ij} = 1$. Otherwise, leave it as $0$.\n\n\n25.2-9\n\n\n\n\nSuppose that we can compute the transitive closure of a directed acyclic graph in $f(|V|, |E|)$ time, where $f$ is a monotonically increasing function of $|V|$ and $|E|$. Show that the time to compute the transitive closure $G' = (V, E')$ of a general directed graph $G = (V, E)$ is then $f(|V|, |E|) + O(V + E')$.\n\n\n\n\nFirst, compute the strongly connected components of the directed graph, and look at it's component graph. This component graph is going to be acyclic and have at most as many vertices and at most as many edges as the original graph. Since it is acyclic, we can run our transitive closure algorithm on it. Then, for every edge $(S_1, S_2)$ that shows up in the transitive closure of the component graph, we add an edge from each vertex in $S_1$ to a vertex in $S_2$. This takes time equal to $O(V + E')$. So, the total time required is $\\le f(|V|, |E|) + O(V + E)$.",
            "title": "25.2 The Floyd-Warshall algorithm"
        },
        {
            "location": "/Chap25/25.2/#252-1",
            "text": "Run the Floyd-Warshall algorithm on the weighted, directed graph of Figure 25.2. Show the matrix $D^{(k)}$ that results for each iteration of the outer loop.   $k = 1$:  \\begin{pmatrix}\n     0 & \\infty & \\infty & \\infty &     -1 & \\infty \\\\\n     1 &      0 & \\infty &      2 &      0 & \\infty \\\\\n\\infty &      2 &      0 & \\infty & \\infty &     -8 \\\\\n    -4 & \\infty & \\infty &      0 &     -5 & \\infty \\\\\n\\infty &      7 & \\infty & \\infty &      0 & \\infty \\\\\n\\infty &      5 &     10 & \\infty & \\infty & 0\n\\end{pmatrix}  $k = 2$:  \\begin{pmatrix}\n 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\\n 1 &      0 & \\infty &      2 &  0 & \\infty \\\\\n 3 &      2 &      0 &      4 &  2 & -    8 \\\\\n-4 & \\infty & \\infty &      0 & -5 & \\infty \\\\\n 8 &      7 & \\infty &      9 &  0 & \\infty \\\\\n 6 &      5 &     10 &      7 &  5 & 0\n\\end{pmatrix}  $k = 3$:  \\begin{pmatrix}\n 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\\n 1 &      0 & \\infty &      2 &  0 & \\infty \\\\\n 3 &      2 &      0 &      4 &  2 &     -8 \\\\\n-4 & \\infty & \\infty &      0 & -5 & \\infty \\\\\n 8 &      7 & \\infty &      9 &  0 & \\infty \\\\\n 6 &      5 &     10 &      7 &  5 & 0\n\\end{pmatrix}  $k = 4$:  \\begin{pmatrix}\n 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\\n-2 &      0 & \\infty &      2 & -3 & \\infty \\\\\n 0 &      2 &      0 &      4 & -1 &     -8 \\\\\n-4 & \\infty & \\infty &      0 & -5 & \\infty \\\\\n 5 &      7 & \\infty &      9 &  0 & \\infty \\\\\n 3 &      5 &     10 &      7 &  2 & 0\n\\end{pmatrix}  $k = 5$:  \\begin{pmatrix}\n 0 & 6 & \\infty & 8 & -1 & \\infty \\\\\n-2 & 0 & \\infty & 2 & -3 & \\infty \\\\\n 0 & 2 &      0 & 4 & -1 &     -8 \\\\\n-4 & 2 & \\infty & 0 & -5 & \\infty \\\\\n 5 & 7 & \\infty & 9 &  0 & \\infty \\\\\n 3 & 5 &     10 & 7 &  2 & 0\n\\end{pmatrix}  $k = 6$:  \\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 & 0\n\\end{pmatrix}",
            "title": "25.2-1"
        },
        {
            "location": "/Chap25/25.2/#252-2",
            "text": "Show how to compute the transitive closure using the technique of Section 25.1.   We set $w_{ij} = 1$ if $(i, j)$ is an edge, and $w_{ij} = 0$ otherwise. Then we replace line 7 of $\\text{EXTEND-SHORTEST-PATHS}(L, W)$ by $l''_{ij} = l''_{ij} \\lor (l_{ik} \\land w_{kj})$. Then run the $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ algorithm.",
            "title": "25.2-2"
        },
        {
            "location": "/Chap25/25.2/#252-3",
            "text": "Modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\prod^{(k)}$ matrices according to equations $\\text{(25.6)}$ and $\\text{(25.7)}$. Prove rigorously that for all $i \\in V$, the predecessor subgraph $G_{\\pi, i}$ is a shortest-paths tree with root $i$. ($\\textit{Hint:}$ To show that $G_{\\pi, i}$ is acyclic, first show that $\\pi_{ij}^{(k)} = l$ implies $d_{ij}^{(k)} \\ge d_{il}^{(k)} + w_{lj}$, according to the definition of $\\pi_{ij}^{(k)}$. Then, adapt the proof of Lemma 23.16.)   MOD-FLOYD-WARSHALL(W)\n    n = W.rows\n    D^0 = W\n    PI^0 is a matrix with NIL in every entry\n    for i = 1 to n\n        for j = 1 to n\n            if i \u2260 j and D^0_{i, j} < \u221e\n                PI^0_{i, j} = i\n    for k = 1 to n\n        let D^k be a new n \u00d7 n matrix.\n        let PI^k be a new n \u00d7 n matrix\n        for i = 1 to n\n            for j = 1 to n\n                if d^{k - 1}_{ij} \u2264 d^{k - 1}_{ik} + d^{k - 1}_{kj}\n                    d^k_{ij} = d^{k - 1}_{ij}\n                    PI^k_{ij} = PI^{k - 1}_{ij}\n                else\n                    d^k_{ij} = d^{k - 1}_{ik} + d^{k - 1}_{kj}\n                    PI^k_{ij} = PI^{k - 1}_{kj}  In order to have that $\\pi^{(k)}_{ij} = l$, we need that $d^{(k)}_{ij} \\ge d^{(k)}_{il} + w_{lj}$. To see this fact, we will note that having $\\pi^{(k)}_{ij} = l$ means that a shortest path from $i$ to $j$ last goes through $l$. A path that last goes through $l$ corresponds to taking a chepest path from $i$ to $l$ and then following the single edge from $l$ to $j$. However, This means that $d_{il} \\le d_{ij} - w_{ij}$, which we can rearrange to get the desired inequality. We can just continue following this inequality\naround, and if we ever get some cycle, $i_1, i_2, \\ldots, i_c$, then we would have that $d_{ii_1} \\le d_{ii_1} + w_{i_1i_2} + w_{i_2i_3} + \\cdots + w_{i_ci_1}$. So, if we subtract the common term sfrom both sides, we get that $0 \\le w_{i_ci_1} + \\sum_{q = 1}^{c - 1} w_{i_qi_{q + 1}}$. So, we have that we would only have a cycle in the precedessor graph if we ahvt that there is a zero weight cycle in the original graph. However, we would never have to go around the weight zero cycle since the constructed path of shortest weight favors ones with a fewer number of edges because of the way that we handle the equality case in equation $\\text{(25.7)}$.",
            "title": "25.2-3"
        },
        {
            "location": "/Chap25/25.2/#252-4",
            "text": "As it appears above, the Floyd-Warshall algorithm requires $\\Theta(n^3)$ space, since we compute $d_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$. Show that the following procedure, which simply drops all the superscripts, is correct, and thus only $\\Theta(n^2)$ space is required.  FLOYD - WARSHALL ' ( W ) \n     n   =   W . rows \n     D   =   W \n     for   k   =   1   to   n \n         for   i   =   1   to   n \n             for   j   =   1   to   n \n                 d [ i ][ j ]   =   min ( d [ i ][ j ],   d [ i ][ k ]   +   d [ k ][ j ]) \n     return   D    With the superscripts, the computation is $d_{ij}^{(k)} = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)})$. If, having dropped the superscripts, we were to compute and store $d_{ik}$ or $d_{kj}$ before using these values to compute $d_{ij}$, we might be computing one of the following:  \\begin{align}\nd_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k - 1)}), \\\\\nd_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k)}), \\\\\nd_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k)}),\n\\end{align}  In any of these scenarios, we're computing the weight of a shortest path from $i$ to $j$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. If we use $d_{ik}^{(k)}$, rather than $d_{ik}^{(k - 1)}$, in the computation, then we're using a subpath from $i$ to $k$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. But $k$ cannot be an  intermediate  vertex on a shortest path from $i$ to $k$, since otherwise there would be a cycle on this shortest path. Thus, $d_{ik}^{(k)} = d_{ik}^{(k - 1)}$. A similar argument applies to show that $d_{kj}^{(k)} = d_{kj}^{(k - 1)}$. Hence, we can drop the superscripts in the computation.",
            "title": "25.2-4"
        },
        {
            "location": "/Chap25/25.2/#252-5",
            "text": "Suppose that we modify the way in which equation $\\text{(25.7)}$ handles equality:  $$\n\\pi_{ij}^{(k)} =\n\\begin{cases}\n\\pi_{ij}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} <   d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}, \\\\\n\\pi_{kj}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} \\ge d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}.\n\\end{cases}\n$$  Is this alternative definition of the predecessor matrix $\\prod$ correct?   If we change the way that we handle the equality case, we will still be generating a the correct values for the $\\pi$ matrix. This is because updating the $\\pi$ values to make paths that are longer but still tied for the lowest weight. Making $\\pi_{ij} = \\pi_{kj}$ means that we are making the shortest path from $i$ to $j$ passes through $k$ at some point. This has the same cost as just going from $i$ to $j$, since $d_{ij} = d_{ik} + d_{kj}$.",
            "title": "25.2-5"
        },
        {
            "location": "/Chap25/25.2/#252-6",
            "text": "How can we use the output of the Floyd-Warshall algorithm to detect the presence of a negative-weight cycle?   Here are two ways to detect negative-weight cycles:    Check the main-diagonal entries of the result matrix for a negative value. There is a negative weight cycle if and only if $d_{ii}^{(n)} < 0$ for some vertex $i$:   $d_{ii}^{(n)}$ is a path weight from $i$ to itself; so if it is negative, there is a path from $i$ to itself (i.e., a cycle), with negative weight.  If there is a negative-weight cycle, consider the one with the fewest vertices.    If it has just one vertex, then some $w_{ii} < 0$, so $d_{ii}$ starts out negative, and since $d$ values are never increased, it is also negative when the algorithm terminates.  If it has at least two vertices, let $k$ be the highest-numbered vertex in the cycle, and let $i$ be some other vertex in the cycle. $d_{ik}^{(k - 1)}$ and $d_{ki}^{(k - 1)}$ have correct shortest-path weights, because they are not based on negativeweight cycles. (Neither $d_{ik}^{(k - 1)}$ nor $d_{ki}^{(k - 1)}$ can include $k$ as an intermediate vertex, and $i$ and $k$ are on the negative-weight cycle with the fewest vertices.) Since $i \\leadsto k \\leadsto i$ is a negative-weight cycle, the sum of those two weights is negative, so $d_{ii}^{(k)}$ will be set to a negative value. Since $d$ values are never increased, it is also negative when the algorithm terminates.     In fact, it suffices to check whether $d_{ii}^{(n - 1)} < 0$ for some vertex $i$. Here's why. A negative-weight cycle containing vertex $i$ either contains vertex $n$ or it does not. If it does not, then clearly $d_{ii}^{(n - 1)} < 0$. If the negative-weight cycle contains vertex $n$, then consider $d_{nn}^{(n - 1)}$. This value must be negative, since the cycle, starting and ending at vertex $n$, does not include vertex $n$ as an intermediate vertex.     Alternatively, one could just run the normal $\\text{FLOYD-WARSHALL}$ algorithm one extra iteration to see if any of the $d$ values change. If there are negative cycles, then some shortest-path cost will be cheaper. If there are no such cycles, then no $d$ values will change because the algorithm gives the correct shortest paths.",
            "title": "25.2-6"
        },
        {
            "location": "/Chap25/25.2/#252-7",
            "text": "Another way to reconstruct shortest paths in the Floyd-Warshall algorithm uses values $\\phi_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$, where $\\phi_{ij}^{(k)}$ is the highest-numbered intermediate vertex of a shortest path from $i$ to $j$ in which all intermediate vertices are in the set $\\{1, 2, \\ldots, k \\}$. Give a recursive formulation for $\\phi_{ij}^{(k)}$, modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\phi_{ij}^{(k)}$ values, and rewrite the $\\text{PRINT-ALLPAIRS-SHORTEST-PATH}$ procedure to take the matrix $\\Phi = (\\phi_{ij}^{(n)})$ as an input. How is the matrix $\\Phi$ like the $s$ table in the matrix-chain multiplication problem of Section 15.2?   We can recursively compute the values of $\\phi_{ij}^{(n)}$ by, letting it be $\\phi_{ij}^{(k - 1)}$ if $d(k) + d_{ik}^{(k)} + d_{ik}^{(k)} \\ge d_{ij}(k - 1)$, and otherwise, let it be $k$. This works correctly because it perfectly captures whether we decided to use vertex $k$ when we were repeatedly allowing ourselves use of each vertex one at a time. To modify Floyd-Warshall to compute this, we would just need to stick within the innermost for loop, something that computes $\\phi(k)$ by this recursive rule, this would only be a constant amount of work in this innermost for loop, and so would not cause the asymptotic runtime to increase. It is similar to the s table in matrix-chain multiplication because it is computed by a similar recurrence.  If we already have the $n^3$ values in $\\phi_{ij}^{(k)}$ provided, then we can reconstruct the shortest path from $i$ to $j$ because we know that the largest vertex in the path from $i$ to $j$ is $\\phi_{ij}^{(n)}$, call it $a_1$. Then, we know that the largest vertex in the path before $a_1$ will be $\\phi_{ia_1}^{(a_1 - 1)}$ and the largest after $a_1$ will be $\\phi_{a_1j}^{(a_1 - 1)}$. By continuing to recurse until we get that the largest element showing up at some point is $\\text{NIL}$, we will be able to continue subdividing the path until it is entirely constructed.",
            "title": "25.2-7"
        },
        {
            "location": "/Chap25/25.2/#252-8",
            "text": "Give an $O(VE)$-time algorithm for computing the transitive closure of a directed\ngraph $G = (V, E)$.   Create an $n$ by $n$ matrix $A$ filled with $0$'s. We are done if we can determine the vertices reachable from a particular vertex in $O(E)$ time, since we can just compute this for each $v \\in V$. To do this, assign each edge weight $1$. Then we have $\\delta(v, u) \\le |E|$ for all $u \\in V$. By Problem 24-4 (a) we can compute $\\delta(v, u)$ in $O(E)$ forall $u \\in V$. If $\\delta(v, u) < \\infty$, set $A_{ij} = 1$. Otherwise, leave it as $0$.",
            "title": "25.2-8"
        },
        {
            "location": "/Chap25/25.2/#252-9",
            "text": "Suppose that we can compute the transitive closure of a directed acyclic graph in $f(|V|, |E|)$ time, where $f$ is a monotonically increasing function of $|V|$ and $|E|$. Show that the time to compute the transitive closure $G' = (V, E')$ of a general directed graph $G = (V, E)$ is then $f(|V|, |E|) + O(V + E')$.   First, compute the strongly connected components of the directed graph, and look at it's component graph. This component graph is going to be acyclic and have at most as many vertices and at most as many edges as the original graph. Since it is acyclic, we can run our transitive closure algorithm on it. Then, for every edge $(S_1, S_2)$ that shows up in the transitive closure of the component graph, we add an edge from each vertex in $S_1$ to a vertex in $S_2$. This takes time equal to $O(V + E')$. So, the total time required is $\\le f(|V|, |E|) + O(V + E)$.",
            "title": "25.2-9"
        },
        {
            "location": "/Chap25/25.3/",
            "text": "25.3-1\n\n\n\n\nUse Johnson's algorithm to find the shortest paths between all pairs of vertices in the graph of Figure 25.2. Show the values of $h$ and $\\hat w$ computed by the algorithm.\n\n\n\n\n\\begin{array}{c|c}\nv & h(v) \\\\\n\\hline\n1 & -5 \\\\\n2 & -3 \\\\\n3 &  0 \\\\\n4 & -1 \\\\\n5 & -6 \\\\\n6 & -8\n\\end{array}\n\n\n\\begin{array}{ccc|ccc}\nu & v & \\hat w(u, v) & u & v & \\hat w(u, v) \\\\\n\\hline\n1 & 2 & \\text{NIL} & 4 & 1 & 0          \\\\\n1 & 3 & \\text{NIL} & 4 & 2 & \\text{NIL} \\\\\n1 & 4 & \\text{NIL} & 4 & 3 & \\text{NIL} \\\\\n1 & 5 & 0          & 4 & 5 & 8          \\\\\n1 & 6 & \\text{NIL} & 4 & 6 & \\text{NIL} \\\\\n2 & 1 & 3          & 5 & 1 & \\text{NIL} \\\\\n2 & 3 & \\text{NIL} & 5 & 2 & 4          \\\\\n2 & 4 & 0          & 5 & 3 & \\text{NIL} \\\\\n2 & 5 & \\text{NIL} & 5 & 4 & \\text{NIL} \\\\\n2 & 6 & \\text{NIL} & 5 & 6 & \\text{NIL} \\\\\n3 & 1 & \\text{NIL} & 6 & 1 & \\text{NIL} \\\\\n3 & 2 & 5          & 6 & 2 & 0          \\\\\n3 & 4 & \\text{NIL} & 6 & 3 & 18         \\\\\n3 & 5 & \\text{NIL} & 6 & 4 & \\text{NIL} \\\\\n3 & 6 & 0          & 6 & 5 & \\text{NIL} \\\\\n\\end{array}\n\n\nSo, the $d_{ij}$ values that we get are\n\n\n$$\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 &      0\n\\end{pmatrix}\n.\n$$\n\n\n25.3-2\n\n\n\n\nWhat is the purpose of adding the new vertex $s$ to $V'$, yielding $V'$?\n\n\n\n\nThis is only important when there are negative-weight cycles in the graph. Using a dummy vertex gets us around the problem of trying to compute $-\\infty + \\infty$ to find $\\hat w$. Moreover, if we had instead used a vertex $v$ in the graph instead of the new vertex $s$, then we run into trouble if a vertex fails to be reachable from $v$.\n\n\n25.3-3\n\n\n\n\nSuppose that $w(u, v) \\ge 0$ for all edges $(u, v) \\in E$. What is the relationship between the weight functions $w$ and $\\hat w$?\n\n\n\n\nIf all the edge weights are nonnegative, then the values computed as the shortest distances when running Bellman-Ford will be all zero. This is because when constructing $G'$ on the first line of Johnson's algorithm, we place an edge of weight zero from s to every other vertex. Since any path within the graph has no negative edges, its cost cannot be negative, and so, cannot beat the trivial path that goes straight from $s$ to any given vertex. Since we have that $h(u) = h(v)$ for every $u$ and $v$, the reweighting that occurs only adds and subtracts $0$, and so we have that $w(u, v) = \\hat w(u, v)$\n\n\n25.3-4\n\n\n\n\nProfessor Greenstreet claims that there is a simpler way to reweight edges than the method used in Johnson's algorithm. Letting $w^* = \\min_{(u, v) \\in E} \\{w(u, v)\\}$, just define $\\hat w(u, v) = w(u, v) - w^*$ for all edges $(u, v) \\in E$. What is wrong with the professor's method of reweighting?\n\n\n\n\nIt changes shortest paths. Consider the following graph. $V = \\{s, x, y, z\\}$, and there are 4 edges: $w(s, x) = 2$, $w(x, y) = 2$, $w(s, y) = 5$, and $w(s, z) = -10$. So we'd add $10$ to every weight to make $\\hat w$. With $w$, the shortest path from $s$ to $y$ is $s \\to x \\to y$, with weight $4$. With $\\hat w$, the shortest path from $s$ to $y$ is $s \\to y$, with weight $15$. (The path $s \\to x \\to y$ has weight $24$.) The problem is that by just adding the same amount to every edge, you penalize paths with more edges, even if their weights are low.\n\n\n25.3-5\n\n\n\n\nSuppose that we run Johnson's algorithm on a directed graph $G$ with weight function $w$. Show that if $G$ contains a $0$-weight cycle $c$, then $\\hat w(u, v) = 0$ for every edge $(u, v)$ in $c$.\n\n\n\n\nIf $\\delta(s, v) - \\delta(s, u) \\le w(u, v)$, we have\n\n\n$$\\delta(s, u) \\le \\delta(s, v) + (0 - w(u, v)) < \\delta(s, u) + w(u, v) - w(u, v) = \\delta(s, u),$$\n\n\nwhich is impossible, thus $\\delta(s, v) - \\delta(s, u) = w(u, v)$, $\\hat w(u, v) = w(u, v) + \\delta(s, u) - \\delta(s, v) = 0$.\n\n\n25.3-6\n\n\n\n\nProfessor Michener claims that there is no need to create a new source vertex in line 1 of $\\text{JOHNSON}$. He claims that instead we can just use $G' = G$ and let $s$ be any vertex. Give an example of a weighted, directed graph $G$ for which incorporating the professor's idea into $\\text{JOHNSON}$ causes incorrect answers. Then show that if $G$ is strongly connected (every vertex is reachable from every other vertex), the results returned by $\\text{JOHNSON}$ with the professor's modification are correct.\n\n\n\n\nIn this solution, we assume that $\\infty - \\infty$ is undefined, in particular, it's not $0$.\n\n\nLet $G = (V, E)$, where $V = {s, u}$, $E = \\{(u, s)\\}$, and $w(u, s) = 0$. There is only one edge, and it enters $s$. When we run Bellman-Ford from $s$, we get $h(s) = \\delta(s, s) = 0$ and $h(u) = \\delta(s, u) = \\infty$. When we reweight, we get $\\hat w(u, s) = 0 + \\infty - 0 = \\infty$. We compute $\\hat\\delta(u, s) = \\infty$, and so we compute $d_{us} = \\infty + 0 - \\infty \\ne 0$. Since $\\delta(u, s) = 0$, we get an incorrect answer.\n\n\nIf the graph $G$ is strongly connected, then we get $h(v) = \\delta(s, v) < \\infty$ for all vertices $v \\in V$. Thus, the triangle inequality says that $h(v) \\le h(u) + w(u, v)$ for all edges $(u, v) \\in E$, and so $\\hat w(u, v) = w(u, v) + h(u) - h(v) \\ge 0$. Moreover, all edge weights $\\hat w(u, v)$ used in Lemma 25.1 are finite, and so the lemma holds. Therefore, the conditions we need in order to use Johnson's algorithm hold: that reweighting does not change shortest paths, and that all edge weights $\\hat w(u, v)$ are nonnegative. Again relying on $G$ being strongly connected, we get that $\\hat\\delta(u, v) < \\infty$ for all edges $(u, v) \\in E$, which means that $d_{uv} = \\hat\\delta(u, v) + h(v) - h(u)$ is finite and correct.",
            "title": "25.3 Johnson's algorithm for sparse graphs"
        },
        {
            "location": "/Chap25/25.3/#253-1",
            "text": "Use Johnson's algorithm to find the shortest paths between all pairs of vertices in the graph of Figure 25.2. Show the values of $h$ and $\\hat w$ computed by the algorithm.   \\begin{array}{c|c}\nv & h(v) \\\\\n\\hline\n1 & -5 \\\\\n2 & -3 \\\\\n3 &  0 \\\\\n4 & -1 \\\\\n5 & -6 \\\\\n6 & -8\n\\end{array}  \\begin{array}{ccc|ccc}\nu & v & \\hat w(u, v) & u & v & \\hat w(u, v) \\\\\n\\hline\n1 & 2 & \\text{NIL} & 4 & 1 & 0          \\\\\n1 & 3 & \\text{NIL} & 4 & 2 & \\text{NIL} \\\\\n1 & 4 & \\text{NIL} & 4 & 3 & \\text{NIL} \\\\\n1 & 5 & 0          & 4 & 5 & 8          \\\\\n1 & 6 & \\text{NIL} & 4 & 6 & \\text{NIL} \\\\\n2 & 1 & 3          & 5 & 1 & \\text{NIL} \\\\\n2 & 3 & \\text{NIL} & 5 & 2 & 4          \\\\\n2 & 4 & 0          & 5 & 3 & \\text{NIL} \\\\\n2 & 5 & \\text{NIL} & 5 & 4 & \\text{NIL} \\\\\n2 & 6 & \\text{NIL} & 5 & 6 & \\text{NIL} \\\\\n3 & 1 & \\text{NIL} & 6 & 1 & \\text{NIL} \\\\\n3 & 2 & 5          & 6 & 2 & 0          \\\\\n3 & 4 & \\text{NIL} & 6 & 3 & 18         \\\\\n3 & 5 & \\text{NIL} & 6 & 4 & \\text{NIL} \\\\\n3 & 6 & 0          & 6 & 5 & \\text{NIL} \\\\\n\\end{array}  So, the $d_{ij}$ values that we get are  $$\n\\begin{pmatrix}\n 0 &  6 & \\infty &  8 & -1 & \\infty \\\\\n-2 &  0 & \\infty &  2 & -3 & \\infty \\\\\n-5 & -3 &      0 & -1 & -6 &     -8 \\\\\n-4 &  2 & \\infty &  0 & -5 & \\infty \\\\\n 5 &  7 & \\infty &  9 &  0 & \\infty \\\\\n 3 &  5 &     10 &  7 &  2 &      0\n\\end{pmatrix}\n.\n$$",
            "title": "25.3-1"
        },
        {
            "location": "/Chap25/25.3/#253-2",
            "text": "What is the purpose of adding the new vertex $s$ to $V'$, yielding $V'$?   This is only important when there are negative-weight cycles in the graph. Using a dummy vertex gets us around the problem of trying to compute $-\\infty + \\infty$ to find $\\hat w$. Moreover, if we had instead used a vertex $v$ in the graph instead of the new vertex $s$, then we run into trouble if a vertex fails to be reachable from $v$.",
            "title": "25.3-2"
        },
        {
            "location": "/Chap25/25.3/#253-3",
            "text": "Suppose that $w(u, v) \\ge 0$ for all edges $(u, v) \\in E$. What is the relationship between the weight functions $w$ and $\\hat w$?   If all the edge weights are nonnegative, then the values computed as the shortest distances when running Bellman-Ford will be all zero. This is because when constructing $G'$ on the first line of Johnson's algorithm, we place an edge of weight zero from s to every other vertex. Since any path within the graph has no negative edges, its cost cannot be negative, and so, cannot beat the trivial path that goes straight from $s$ to any given vertex. Since we have that $h(u) = h(v)$ for every $u$ and $v$, the reweighting that occurs only adds and subtracts $0$, and so we have that $w(u, v) = \\hat w(u, v)$",
            "title": "25.3-3"
        },
        {
            "location": "/Chap25/25.3/#253-4",
            "text": "Professor Greenstreet claims that there is a simpler way to reweight edges than the method used in Johnson's algorithm. Letting $w^* = \\min_{(u, v) \\in E} \\{w(u, v)\\}$, just define $\\hat w(u, v) = w(u, v) - w^*$ for all edges $(u, v) \\in E$. What is wrong with the professor's method of reweighting?   It changes shortest paths. Consider the following graph. $V = \\{s, x, y, z\\}$, and there are 4 edges: $w(s, x) = 2$, $w(x, y) = 2$, $w(s, y) = 5$, and $w(s, z) = -10$. So we'd add $10$ to every weight to make $\\hat w$. With $w$, the shortest path from $s$ to $y$ is $s \\to x \\to y$, with weight $4$. With $\\hat w$, the shortest path from $s$ to $y$ is $s \\to y$, with weight $15$. (The path $s \\to x \\to y$ has weight $24$.) The problem is that by just adding the same amount to every edge, you penalize paths with more edges, even if their weights are low.",
            "title": "25.3-4"
        },
        {
            "location": "/Chap25/25.3/#253-5",
            "text": "Suppose that we run Johnson's algorithm on a directed graph $G$ with weight function $w$. Show that if $G$ contains a $0$-weight cycle $c$, then $\\hat w(u, v) = 0$ for every edge $(u, v)$ in $c$.   If $\\delta(s, v) - \\delta(s, u) \\le w(u, v)$, we have  $$\\delta(s, u) \\le \\delta(s, v) + (0 - w(u, v)) < \\delta(s, u) + w(u, v) - w(u, v) = \\delta(s, u),$$  which is impossible, thus $\\delta(s, v) - \\delta(s, u) = w(u, v)$, $\\hat w(u, v) = w(u, v) + \\delta(s, u) - \\delta(s, v) = 0$.",
            "title": "25.3-5"
        },
        {
            "location": "/Chap25/25.3/#253-6",
            "text": "Professor Michener claims that there is no need to create a new source vertex in line 1 of $\\text{JOHNSON}$. He claims that instead we can just use $G' = G$ and let $s$ be any vertex. Give an example of a weighted, directed graph $G$ for which incorporating the professor's idea into $\\text{JOHNSON}$ causes incorrect answers. Then show that if $G$ is strongly connected (every vertex is reachable from every other vertex), the results returned by $\\text{JOHNSON}$ with the professor's modification are correct.   In this solution, we assume that $\\infty - \\infty$ is undefined, in particular, it's not $0$.  Let $G = (V, E)$, where $V = {s, u}$, $E = \\{(u, s)\\}$, and $w(u, s) = 0$. There is only one edge, and it enters $s$. When we run Bellman-Ford from $s$, we get $h(s) = \\delta(s, s) = 0$ and $h(u) = \\delta(s, u) = \\infty$. When we reweight, we get $\\hat w(u, s) = 0 + \\infty - 0 = \\infty$. We compute $\\hat\\delta(u, s) = \\infty$, and so we compute $d_{us} = \\infty + 0 - \\infty \\ne 0$. Since $\\delta(u, s) = 0$, we get an incorrect answer.  If the graph $G$ is strongly connected, then we get $h(v) = \\delta(s, v) < \\infty$ for all vertices $v \\in V$. Thus, the triangle inequality says that $h(v) \\le h(u) + w(u, v)$ for all edges $(u, v) \\in E$, and so $\\hat w(u, v) = w(u, v) + h(u) - h(v) \\ge 0$. Moreover, all edge weights $\\hat w(u, v)$ used in Lemma 25.1 are finite, and so the lemma holds. Therefore, the conditions we need in order to use Johnson's algorithm hold: that reweighting does not change shortest paths, and that all edge weights $\\hat w(u, v)$ are nonnegative. Again relying on $G$ being strongly connected, we get that $\\hat\\delta(u, v) < \\infty$ for all edges $(u, v) \\in E$, which means that $d_{uv} = \\hat\\delta(u, v) + h(v) - h(u)$ is finite and correct.",
            "title": "25.3-6"
        },
        {
            "location": "/Chap25/Problems/25-1/",
            "text": "Suppose that we wish to maintain the transitive closure of a directed graph $G = (V, E)$ as we insert edges into $E$. That is, after each edge has been inserted, we want to update the transitive closure of the edges inserted so far. Assume that the graph $G$ has no edges initially and that we represent the transitive closure as a boolean matrix.\n\n\na.\n Show how to update the transitive closure $G^* = (V, E^*)$ of a graph $G = (V, E)$ in $O(V^2)$ time when a new edge is added to $G$.\n\n\nb.\n Give an example of a graph $G$ and an edge $e$ such that $\\Omega(V^2)$ time is required to update the transitive closure after the insertion of $e$ into $G$, no matter what algorithm is used.\n\n\nc.\n Describe an efficient algorithm for updating the transitive closure as edges are inserted into the graph. For any sequence of $n$ insertions, your algorithm should run in total time $\\sum_{i = 1}^n t_i = O(V^3)$, where $t_i$ is the time to update the transitive closure upon inserting the $i$th edge. Prove that your algorithm attains this time bound.\n\n\n\n\na.\n Let $T = (t_{ij})$ be the $|V| \\times |V|$ matrix representing the transitive closure, such that $t_{ij}$ is $1$ if there is a path from $i$ to $j$, and $0$ otherwise.\n\n\nInitialize $T$ (when there are no edge in $G$) as follows:\n\n\n$$\nt_{ij} =\n\\begin{cases}\n1 & \\text{if $i = j$}, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nWe update $T$ as follows when an edge $(u, v)$ is added to $G$:\n\n\nTRANSITIVE\n-\nCLOSURE\n-\nUPDATE\n(\nT\n,\n \nu\n,\n \nv\n)\n\n    \nlet\n \nT\n \nbe\n \n|\nV\n|\n \n\u00d7\n \n|\nV\n|\n\n    \nfor\n \ni\n \n=\n \n1\n \nto\n \n|\nV\n|\n\n        \nfor\n \nj\n \n=\n \n1\n \nto\n \n|\nV\n|\n\n            \nif\n \nt\n[\ni\n][\nu\n]\n \n==\n \n1\n \nand\n \nt\n[\nv\n][\nj\n]\n \n==\n \n1\n\n                \nt\n[\ni\n][\nj\n]\n \n=\n \n1\n\n\n\n\n\n\n\nWith this procedure, the effect of adding edge $(u, v)$ is to create a path (via the new edge) from every vertex that could already reach $u$ to every vertex that could already be reached from $v$.\n\n\nNote that the procedure sets $t_{uv} = 1$, because both $t_{uu}$ and $t_{vv}$ are initialized to $1$.\n\n\nThis procedure takes $\\Theta(V^2)$ time because of the two nested loops.\n\n\n\n\nb.\n Consider inserting the edge $(v_{|V|}, v_1)$ into the straight-line graph $v_1 \\to v_2 \\to \\cdots \\to v_{|V|}$.\n\n\nBefore this edge is inserted, only $|V|(|V| + 1) / 2$ entries in $T$ are $1$ (the entries on and above the main diagonal). After the edge is inserted, the graph is a cycle in which every vertex can reach every other vertex, so all $|V|^2$ entries in $T$ are $1$. Hence $|V|^2 - (|V|(|V| + 2) / 2) = \\Theta(V^2)$ entries must be changed in $T$, so any algorithm to update the transitive closure must take $\\Omega(V^2)$ time on this graph.\n\n\nc.\n The algorithm in part (a) would take $\\Theta(V^4)$ time to insert all possible $\\Theta(V^2)$ edges, so we need a more ef\ufb01cient algorithm in order for any sequence of insertions to take only $O(V^3)$ total time.\n\n\nTo improve the algorithm, notice that the loop over $j$ is pointless when $t_{iv} = 1$. That is, if there is already a path $i \\leadsto v$, then adding the edge $(u, v)$ cannot make any new vertices reachable from $i$. The loop to set $t_{ij}$ to $1$ for $j$ such that there exists a path $v \\leadsto j$ is just setting entries that are already $1$. Eliminate this redundant processing as follows:\n\n\nTRANSITIVE\n-\nCLOSURE\n-\nUPDATE\n(\nT\n,\n \nu\n,\n \nv\n)\n\n    \nlet\n \nT\n \nbe\n \n|\nV\n|\n \n\u00d7\n \n|\nV\n|\n\n    \nfor\n \ni\n \n=\n \n1\n \nto\n \n|\nV\n|\n\n        \nif\n \nt\n[\ni\n][\nu\n]\n \n==\n \n1\n \nand\n \nt\n[\ni\n][\nv\n]\n \n==\n \n0\n\n            \nfor\n \nj\n \n=\n \n1\n \nto\n \n|\nV\n|\n\n                \nif\n \nt\n[\nv\n][\nj\n]\n \n==\n \n1\n\n                    \nt\n[\ni\n][\nj\n]\n \n=\n \n1\n\n\n\n\n\nWe show that this procedure takes $O(V^3)$ time to update the transitive closure for any sequence of $n$ insertions:\n\n\n\n\nThere cannot be more than $|V|^2$ edges in $G$, so $n \\le |V|^2$.\n\n\nSummed over $n$ insertions, the time for the outer \nfor\n loop header and the test for $t_{iu} == 1$ and $t_{iv} == 0$ is $O(nV) = O(V^3)$.\n\n\nThe last three lines, which take $O(V^2)$ time, are executed only $O(V^2)$ times for $n$ insertions. To see why, notice that the last three lines are executed only when $t_{iv}$ equals $0$, and in that case, the last line sets $t_{iv} = 1$. Thus, the number of $0$ entries in $T$ is reduced by at least $1$ each time the last three lines run. Since there are only $|V|^2$ entries in $T$, these lines can run at most $|V|^2$ times.\n\n\nHence, the total running time over $n$ insertions is $O(V^3)$.",
            "title": "25-1 Transitive closure of a dynamic graph"
        },
        {
            "location": "/Chap25/Problems/25-2/",
            "text": "A graph $G = (V, E)$ is \n$\\epsilon$-dense\n if $|E| = \\Theta(V^{1 + \\epsilon})$ for some constant $\\epsilon$ in the range $0 < \\epsilon \\le 1$. By using $d$-ary min-heaps (see Problem 6-2) in shortest-paths algorithms on $\\epsilon$-dense graphs, we can match the running times of Fibonacci-heap-based algorithms without using as complicated a data structure.\n\n\na.\n What are the asymptotic running times for $\\text{INSERT}$, $\\text{EXTRACT-MIN}$, and $\\text{DECREASE-KEY}$, as a function of $d$ and the number $n$ of elements in a $d$-ary min-heap? What are these running times if we choose $d = \\Theta(n^\\alpha)$ for some constant $0 < \\alpha \\le 1$? Compare these running times to the amortized costs of these operations for a Fibonacci heap.\n\n\nb.\n Show how to compute shortest paths from a single source on an $\\epsilon$-dense directed graph $G = (V, E)$ with no negative-weight edges in $O(E)$ time. ($\\textit{Hint:}$ Pick $d$ as a function of $\\epsilon$.)\n\n\nc.\n Show how to solve the all-pairs shortest-paths problem on an $\\epsilon$-dense directed graph $G = (V, E)$ with no negative-weight edges in $O(VE)$ time. \n\n\nd.\n Show how to solve the all-pairs shortest-paths problem in $O(VE)$ time on an $\\epsilon$-dense directed graph $G = (V, E)$ that may have negative-weight edges but has no negative-weight cycles.\n\n\n\n\na.\n \n\n\n\n\n$\\text{INSERT}$: $\\Theta(\\log_d n) = \\Theta(1 / \\alpha)$.\n\n\n$\\text{EXTRACT-MIN}$: $\\Theta(d\\log_d n) = \\Theta(n^\\alpha / \\alpha)$.\n\n\n$\\text{DECREASE-KEY}$: $\\Theta(\\log_d n) = \\Theta(1 / \\alpha)$.\n\n\n\n\nb.\n Dijkstra, $O(d\\log_d V \\cdot V + \\log_d V \\cdot E)$, if $d = V^\\epsilon$, then\n\n\n\\begin{align}\nO(d \\log_d V \\cdot V + \\log_d V \\cdot E) \n& = O(V^\\epsilon \\cdot V / \\epsilon + E / \\epsilon) \\\\\n& = O((V^{1+\\epsilon} + E) / \\epsilon) \\\\\n& = O((E + E) / \\epsilon) \\\\\n& = O(E).\n\\end{align}\n\n\nc.\n Run $|V|$ times Dijkstra, since the algorithm is $O(E)$ based on (b), the total time is $O(VE)$.\n\n\nd.\n Johnson's reweight is $O(VE)$.",
            "title": "25-2 Shortest paths in epsilon-dense graphs"
        },
        {
            "location": "/Chap26/26.1/",
            "text": "26.1-1\n\n\n\n\nShow that splitting an edge in a flow network yields an equivalent network. More formally, suppose that flow network $G$ contains edge $(u, v)$, and we create a new flow network $G'$ by creating a new vertex $x$ and replacing $(u, v)$ by new edges $(u, x)$ and $(x, v)$ with $c(u, x) = c(x, v) = c(u, v)$. Show that a maximum flow in $G'$ has the same value as a maximum flow in $G$.\n\n\n\n\nWe will prove that for every flow in $G = (V, E)$, we can construct a flow in $G' = (V', E')$ that has the same value as that of the flow in $G$. The required result follows since a maximum flow in $G$ is also a flow. Let $f$ be a flow in $G$. By construction, $V' = V \\cup \\{x\\}$ and $E' = (E - \\{(u, v)\\}) \\cup \\{(u, x), (x, v)\\}$. Construct $f'$ in $G'$ as follows:\n\n\n$$\nf'(y, z) =\n\\begin{cases}\nf(y, z) & \\text{if $(y, z) \\ne (u, x)$ and $(y, z) \\ne (x, v)$}, \\\\\nf(u, z) & \\text{if $(y, z) =   (u, x)$ or  $(y, z) =   (x, v)$}.\n\\end{cases}\n$$\n\n\nInformally, $f'$ is the same as $f$, except that the flow $f(u, v)$ now passes through an intermediate vertex $x$. The vertex $x$ has incoming flow (if any) only from $u$, and has outgoing flow (if any) only to vertex $v$.\n\n\nWe first prove that $f'$ satisfies the required properties of a flow. It is obvious that the capacity constraint is satisfied for every edge in $E'$ and that every vertex in $V' - \\{u, v, x\\}$ obeys flow conservation.\n\n\nTo show that edges $(u, x)$ and $(x, v)$ obey the capacity constraint, we have\n\n\n\\begin{align}\nf(u, x) = f(u, v) & \\le c(u, v) = c(u, x), \\\\\nf(x, v) = f(u, v) & \\le c(u, v) = c(x, v). \n\\end{align}\n\n\nWe now prove flow conservation for $u$. Assuming that $u \\ne \\{s, t\\}$, we have\n\n\n\\begin{align}\n\\sum_{y \\in V'} f'(u, y)\n    & = \\sum_{y \\in V' -{x}} f'(u, y) + f'(u, x) \\\\\n    & = \\sum_{y \\in V- {v}} f(u, y) + f(u, v) \\\\\n    & = \\sum_{y \\in V} f(u, y) \\\\\n    & = \\sum_{y \\in V} f(y, u) \\quad \\text{(because $f$ obeys flow conservation)} \\\\\n    & = \\sum_{y \\in V'} f'(y, u).\n\\end{align}\n\n\nFor vertex $v$, a symmetric argument proves flow conservation.\n\n\nFor vertex $x$, we have\n\n\n\\begin{align}\n\\sum_{y \\in V'} f'(y, x)\n    & = f'(u, x) \\\\\n    & = f'(x, v) \\\\\n    & = \\sum_{y \\in V'} f'(x, y).\n\\end{align}\n\n\nThus, $f'$ is a valid flow in $G'$.\n\n\nWe now prove that the values of the flow in both cases are equal. If the source $s$ is not in $\\{u, v\\}$, the proof is trivial, since our construction assigns the same flows to incoming and outgoing edges of $s$. If $s = u$, then\n\n\n\\begin{align}\n|f'| & = \\sum_{y \\in V'} f'(u, y) - \\sum_{y \\in V'} f'(y, u) \\\\\n     & = \\sum_{y \\in V' - {x}} f'(u, y) - \\sum_{y \\in V'} f'(y, u) + f'(u, x) \\\\\n     & = \\sum_{y \\in V - {v}} f(u, y) - \\sum_{y \\in V} f(y, u) + f(u, v) \\\\\n     & = \\sum_{y \\in V} f(u, y) - \\sum_{y \\in V} f(y, u) \\\\\n     & = |f|.\n\\end{align}\n\n\nThe case when $s = v$ is symmetric. We conclude that $f'$ is a valid flow in $G'$ with $|f'| = |f|$.\n\n\n26.1-2\n\n\n\n\nExtend the flow properties and definitions to the multiple-source, multiple-sink problem. Show that any flow in a multiple-source, multiple-sink flow network corresponds to a flow of identical value in the single-source, single-sink network obtained by adding a supersource and a supersink, and vice versa.\n\n\n\n\nCapacity constraint: for all $u, v \\in V$, we require $0 \\le f(u, v) \\le c(u, v)$.\n\n\nFlow conservation: for all $u \\in V - S - T$, we require $\\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v)$.\n\n\n26.1-3\n\n\n\n\nSuppose that a flow network $G = (V, E)$ violates the assumption that the network contains a path $s \\leadsto v \\leadsto t$ for all vertices $v \\in V$. Let $u$ be a vertex for which there is no path $s \\leadsto u \\leadsto t$. Show that there must exist a maximum flow $f$ in $G$ such that $f(u, v) = f(v, u) = 0$ for all vertices $v \\in V$.\n\n\n\n\nWe show that, given any flow $f'$ in the flow network $G = (V, E)$, we can construct a flow $f$ as stated in the exercise. The result will follow when $f'$ is a maximum flow. The idea is that even if there is a path from $s$ to the connected component of $u$, no flow can enter the component, since the flow has no path to reach $t$. Thus, all the flow inside the component must be cyclic, which can be made zero without affecting the net value of the flow.\n\n\nTwo cases are possible: where $u$ is not connected to $t$, and where $u$ is not connected to $s$. We only analyze the former case. The analysis for the latter case is similar.\n\n\nLet $Y$ be the set of all vertices that have no path to $t$. Our roadmap will be to first prove that no flow can leave $Y$. We use this result and flow conservation to prove that no flow can enter $Y$. We shall then constuct the flow $f$, which has the required properties, and prove that $|f| = |f'|$.\n\n\nThe first step is to prove that there can be no flow from a vertex $y \\in Y$ to a vertex $v \\in V - Y$. That is, $f'(y, v) = 0$. This is so, because there are no edges $(y, v)$ in $E$. If there were an edge $(y, v) \\in E$, then there would be a path from $y$ to $t$, which contradicts how we defined the set $Y$.\n\n\nWe will now prove that $f'(v, y) = 0$, too. We will do so by applying flow conservation to each vertex in $Y$ and taking the sum over $Y$. By flow conservation, we have\n\n\n$$\\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V} f'(v, y).$$\n\n\nPartitioning $V$ into $Y$ and $V - Y$ gives\n\n\n$$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) + \\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) + \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y). \\tag{*}$$\n\n\nBut we also have\n\n\n$$\\sum_{y \\in Y} \\sum_{v \\in Y} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y),$$\n\n\nsince the left-hand side is the same as the right-hand side, except for a change of variable names $v$ and $y$. We also have\n\n\n$$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) = 0,$$\n\n\nsince $f'(y, v) = 0$ for each $y \\in Y$ and $v \\in V - Y$. Thus, equation $(*)$ simplifies to\n\n\n$$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) = 0.$$\n\n\nBecause the flow function is nonnegative, $f(v, y) = 0$ for each $v \\in V$ and $y \\in Y$. We conclude that there can be no flow between any vertex in $Y$ and any vertex in $V - Y$.\n\n\nThe same technique can show that if there is a path from $u$ to $t$ but not from $s$ to $u$, and we define $Z$ as the set of vertices that do not have have a path from $s$ to $u$, then there can be no flow between any vertex in $Z$ and any vertex in $V - Z$. Let $X = Y \\cup Z$. We thus have $f'(v, x) = f'(x, v) = 0$ if $x \\in X$ and $v \\ne X$.\n\n\nWe are now ready to construct flow $f$:\n\n\n$$\nf(u, v) =\n\\begin{cases}\nf'(u, v) & \\text{if $u, v \\ne X$}, \\\\\n0        & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nWe note that $f$ satisfies the requirements of the exercise. We now prove that $f$ also satisfies the requirements of a flow function.\n\n\nThe capacity constraint is satisfied, since whenever $f(u, v) = f'(u, v)$, we have $f(u, v) = f'(u, v) \\le c(u, v)$ and whenever $f(u, v) = 0$, we have $f(u, v) = 0 \\le c(u, v)$.\n\n\nFor flow conservation, let $x$ be some vertex other than $s$ or $t$. If $x \\in X$, then from the construction of $f$, we have\n\n\n$$\\sum_{v \\in V} f(x, v) = \\sum_{v \\in V} f(v, x) = 0.$$\n\n\nOtherwise, if $x \\ne X$, note that $f(x, v) = f'(x, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$. Thus,\n\n\n\\begin{align}\n\\sum_{v \\in V} f(x, v)\n    & = \\sum_{v \\in V} f'(x, v) \\\\\n    & = \\sum_{v \\in V} f'(v, x) \\quad \\text{(because $f'$ obeys flow conservation)} \\\\\n    & = \\sum_{v \\in V} f(v, x).\n\\end{align}\n\n\nFinally, we prove that the value of the flow remains the same. Since $s \\ne X$, we have $f(s, v) = f'(s, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$, and so\n\n\n\\begin{align}\n|f| & = \\sum_{v \\in V} f(s, v) - \\sum_{v \\in V} f(v, s) \\\\\n    & = \\sum_{v \\in V} f'(s, v) - \\sum_{v \\in V} f'(v, s) \\\\\n    & = |f'|.\n\\end{align}\n\n\n26.1-4\n\n\n\n\nLet $f$ be a flow in a network, and let $\\alpha$ be a real number. The \nscalar flow product\n, denoted $\\alpha f$, is a function from $V \\times V$ to $\\mathbb{R}$ defined by\n\n\n$(\\alpha f)(u, v) = \\alpha \\cdot f(u, v)$.\n\n\nProve that the flows in a network form a \nconvex set\n. That is, show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ in the range $0 \\le \\alpha \\le 1$.\n\n\n\n\nTo see that the flows form a convex set, we show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ such that $0 \\le \\alpha \\le 1$.\n\n\nFor the capacity constraint, first observe that $\\alpha \\le 1$ implies that $1 - \\alpha \\ge 0$. Thus, for any $u, v \\in V$, we have\n\n\n\\begin{align}\n\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v)\n    & \\ge 0 \\cdot f_1(u, v) + 0 \\cdot (1 - \\alpha) f_2(u, v) \\\\\n    & =   0.\n\\end{align}\n\n\nSince $f_1(u, v) \\le c(u, v)$ and $f_2(u, v) \\le c(u, v)$, we also have\n\n\n\\begin{align}\n\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v)\n    & \\le \\alpha c(u, v) + (1 - \\alpha) c(u, v) \\\\\n    & =   (\\alpha + (1 - \\alpha))c(u, v) \\\\\n    & =   c(u, v).\n\\end{align}\n\n\nFor flow conservation, observe that since $f_1$ and $f_2$ obey flow conservation, we have $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ and $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ for any $u \\in V - \\{s, t\\}$. We need to show that\n\n\n$$\\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u)) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v))$$\n\n\nfor any $u \\in V - \\{s, t\\}$. We multiply both sides of the equality for $f_1$ by $\\alpha$, multiply both sides of the equality for $f_2$ by $1 - \\alpha$, and add the left-hand and right-hand sides of the resulting equalities to get\n\n\n$$\\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u) = \\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v).$$\n\n\nObserving that\n\n\n\\begin{align}\n\\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u)\n\n    & = \\sum_{v \\in V} \\alpha f_1(v, u) + \\sum_{v \\in V} (1 - \\alpha) f_2(v, u) \\\\\n    & = \\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u))\n\\end{align}\n\n\nand, likewise, that\n\n\n$$\\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha)f_2(u, v))$$\n\n\ncompletes the proof that flow conservation holds, and thus that flows form a convex set.\n\n\n26.1-5\n\n\n\n\nState the maximum-flow problem as a linear-programming problem.\n\n\n\n\n\\begin{array}{ll}\n\\max & \\sum\\limits_{v \\in V} f(s, v) - \\sum\\limits_{v \\in V} f(v, s) \\\\\ns.t. & 0 \\le f(u, v) \\le c(u, v) \\\\\n     & \\sum\\limits_{v \\in V} f(v, u) - \\sum\\limits_{v \\in V} f(u, v) = 0\n\\end{array}\n\n\n26.1-6\n\n\n\n\nProfessor Adam has two children who, unfortunately, dislike each other. The problem is so severe that not only do they refuse to walk to school together, but in fact each one refuses to walk on any block that the other child has stepped on that day. The children have no problem with their paths crossing at a corner. Fortunately both the professor's house and the school are on corners, but beyond that he is not sure if it is going to be possible to send both of his children to the same school. The professor has a map of his town. Show how to formulate the problem of determining whether both his children can go to the same school as a maximum-flow problem.\n\n\n\n\nCreate a vertex for each corner, and if there is a street between corners $u$ and $v$, create directed edges $(u, v)$ and $(v, u)$. Set the capacity of each edge to $1$. Let the source be corner on which the professor's house sits, and let the sink be the corner on which the school is located. We wish to find a flow of value $2$ that also has the property that $f(u, v)$ is an integer for all vertices $u$ and $v$. Such a flow represents two edge-disjoint paths from the house to the school.\n\n\n26.1-7\n\n\n\n\nSuppose that, in addition to edge capacities, a flow network has \nvertex capacities\n. That is each vertex $v$ has a limit $l(v)$ on how much flow can pass though $v$. Show how to transform a flow network $G = (V, E)$ with vertex capacities into an equivalent flow network $G' = (V', E')$ without vertex capacities, such that a maximum flow in $G'$ has the same value as a maximum flow in $G$. How many vertices and edges does $G'$ have?\n\n\n\n\nWe will construct $G'$ by splitting each vertex $v$ of $G$ into two vertices $v_1$, $v_2$, joined by an edge of capacity $l(v)$. All incoming edges of $v$ are now incoming edges to $v_1$. All outgoing edges from $v$ are now outgoing edges from $v_2$.\n\n\nMore formally, construct $G' = (V', E')$ with capacity function $c'$ as follows. For every $v \\in V$, create two vertices $v_1$, $v_2$ in $V'$. Add an edge $(v_1, v_2)$ in $E'$ with $c'(v_1, v_2) = l(v)$. For every edge $(u, v) \\in E$, create an edge $(u_2, v_1)$ in $E'$ with capacity $c'(u_2, v_1) = c(u, v)$. Make $s_1$ and $t_2$ as the new source and target vertices in $G'$. Clearly, $|V'| = 2|V|$ and $|E'| = |E| + |V|$.\n\n\nLet $f$ be a flow in $G$ that respects vertex capacities. Create a flow function $f'$ in $G'$ as follows. For each edge $(u, v) \\in G$, let $f'(u_2, v_1) = f(u, v)$. For each vertex $u \\in V - \\{t\\}$, let $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v)$. Let $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t)$.\n\n\nWe readily see that there is a one-to-one correspondence between flows that respect vertex capacities in $G$ and flows in $G'$. For the capacity constraint, every edge in $G'$ of the form $(u_2, v_1)$ has a corresponding edge in $G$ with a corresponding capacity and flow and thus satisfies the capacity constraint. For edges in $E'$ of the form $(u_1, u_2)$, the capacities reflect the vertex capacities in $G$. Therefore, for $u \\in V - \\{s, t\\}$, we have $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v) \\le l(u) = c'(u_1, u_2)$. We also have $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t) \\le l(t) = c'(t_1, t_2)$. Note that this constraint also enforces the vertex capacities in $G$.\n\n\nNow, we prove flow conservation. By construction, every vertex of the form $u_1$ in $G'$ has exactly one outgoing edge $(u_1, u_2)$, and every incoming edge to $u_1$ corresponds to an incoming edge of $u \\in G$. Thus, for all vertices $u \\in V - \\{s, t\\}$, we have\n\n\n\\begin{align}\n\\text{incoming flow to $u_1$}\n    & = \\sum_{v \\in V'} f'(v, u_1) \\\\\n    & = \\sum_{v \\in V} f(v, u) \\\\\n    & = \\sum_{v \\in V} f(u, v) \\qquad \\text{(because $f$ obeys flow conservation)} \\\\\n    & = f'(u_1, u_2) \\\\\n    & = \\text{outgoing flow from $u_1$}.\n\\end{align}\n\n\nFor $t_1$, we have\n\n\n\\begin{align}\n\\text{incoming flow}\n    & = \\sum_{v \\in V'} f'(v, t_1) \\\\\n    & = \\sum_{v \\in V} f(v, u) \\\\\n    & = f'(t_1, t_2) \\\\\n    & = \\text{outgoing flow}.\n\\end{align}\n\n\nVertices of the form $u_2$ have exactly one incoming edge $(u_1, u_2)$, and every outgoing edge of $u_2$ corresponds to an outgoing edge of $u \\in G$. Thus, for $u_2 \\ne t_2$,\n\n\n\\begin{align}\n\\text{incoming flow}\n    & = f'(u_1, u_2) \\\\\n    & = \\sum_{v \\in V} f(u, v) \\\\\n    & = \\sum_{v \\in V'} f'(u_2, v) \\\\  \n\n    & = \\text{outgoing flow}.\n\\end{align}\n\n\nFinally, we prove that $|f'| = |f|$:\n\n\n\\begin{align}\n|f'| & = \\sum_{v \\in V'} f'(s_1, v) \\\\\n     & = f'(s_1, s_2) \\qquad \\text{(because there are no other outgoing edges from $s_1$)} \\\\\n     & = \\sum_{v \\in V} f(s, v) \\\\\n     & = |f|.\n\\end{align}",
            "title": "26.1 Flow networks"
        },
        {
            "location": "/Chap26/26.1/#261-1",
            "text": "Show that splitting an edge in a flow network yields an equivalent network. More formally, suppose that flow network $G$ contains edge $(u, v)$, and we create a new flow network $G'$ by creating a new vertex $x$ and replacing $(u, v)$ by new edges $(u, x)$ and $(x, v)$ with $c(u, x) = c(x, v) = c(u, v)$. Show that a maximum flow in $G'$ has the same value as a maximum flow in $G$.   We will prove that for every flow in $G = (V, E)$, we can construct a flow in $G' = (V', E')$ that has the same value as that of the flow in $G$. The required result follows since a maximum flow in $G$ is also a flow. Let $f$ be a flow in $G$. By construction, $V' = V \\cup \\{x\\}$ and $E' = (E - \\{(u, v)\\}) \\cup \\{(u, x), (x, v)\\}$. Construct $f'$ in $G'$ as follows:  $$\nf'(y, z) =\n\\begin{cases}\nf(y, z) & \\text{if $(y, z) \\ne (u, x)$ and $(y, z) \\ne (x, v)$}, \\\\\nf(u, z) & \\text{if $(y, z) =   (u, x)$ or  $(y, z) =   (x, v)$}.\n\\end{cases}\n$$  Informally, $f'$ is the same as $f$, except that the flow $f(u, v)$ now passes through an intermediate vertex $x$. The vertex $x$ has incoming flow (if any) only from $u$, and has outgoing flow (if any) only to vertex $v$.  We first prove that $f'$ satisfies the required properties of a flow. It is obvious that the capacity constraint is satisfied for every edge in $E'$ and that every vertex in $V' - \\{u, v, x\\}$ obeys flow conservation.  To show that edges $(u, x)$ and $(x, v)$ obey the capacity constraint, we have  \\begin{align}\nf(u, x) = f(u, v) & \\le c(u, v) = c(u, x), \\\\\nf(x, v) = f(u, v) & \\le c(u, v) = c(x, v). \n\\end{align}  We now prove flow conservation for $u$. Assuming that $u \\ne \\{s, t\\}$, we have  \\begin{align}\n\\sum_{y \\in V'} f'(u, y)\n    & = \\sum_{y \\in V' -{x}} f'(u, y) + f'(u, x) \\\\\n    & = \\sum_{y \\in V- {v}} f(u, y) + f(u, v) \\\\\n    & = \\sum_{y \\in V} f(u, y) \\\\\n    & = \\sum_{y \\in V} f(y, u) \\quad \\text{(because $f$ obeys flow conservation)} \\\\\n    & = \\sum_{y \\in V'} f'(y, u).\n\\end{align}  For vertex $v$, a symmetric argument proves flow conservation.  For vertex $x$, we have  \\begin{align}\n\\sum_{y \\in V'} f'(y, x)\n    & = f'(u, x) \\\\\n    & = f'(x, v) \\\\\n    & = \\sum_{y \\in V'} f'(x, y).\n\\end{align}  Thus, $f'$ is a valid flow in $G'$.  We now prove that the values of the flow in both cases are equal. If the source $s$ is not in $\\{u, v\\}$, the proof is trivial, since our construction assigns the same flows to incoming and outgoing edges of $s$. If $s = u$, then  \\begin{align}\n|f'| & = \\sum_{y \\in V'} f'(u, y) - \\sum_{y \\in V'} f'(y, u) \\\\\n     & = \\sum_{y \\in V' - {x}} f'(u, y) - \\sum_{y \\in V'} f'(y, u) + f'(u, x) \\\\\n     & = \\sum_{y \\in V - {v}} f(u, y) - \\sum_{y \\in V} f(y, u) + f(u, v) \\\\\n     & = \\sum_{y \\in V} f(u, y) - \\sum_{y \\in V} f(y, u) \\\\\n     & = |f|.\n\\end{align}  The case when $s = v$ is symmetric. We conclude that $f'$ is a valid flow in $G'$ with $|f'| = |f|$.",
            "title": "26.1-1"
        },
        {
            "location": "/Chap26/26.1/#261-2",
            "text": "Extend the flow properties and definitions to the multiple-source, multiple-sink problem. Show that any flow in a multiple-source, multiple-sink flow network corresponds to a flow of identical value in the single-source, single-sink network obtained by adding a supersource and a supersink, and vice versa.   Capacity constraint: for all $u, v \\in V$, we require $0 \\le f(u, v) \\le c(u, v)$.  Flow conservation: for all $u \\in V - S - T$, we require $\\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v)$.",
            "title": "26.1-2"
        },
        {
            "location": "/Chap26/26.1/#261-3",
            "text": "Suppose that a flow network $G = (V, E)$ violates the assumption that the network contains a path $s \\leadsto v \\leadsto t$ for all vertices $v \\in V$. Let $u$ be a vertex for which there is no path $s \\leadsto u \\leadsto t$. Show that there must exist a maximum flow $f$ in $G$ such that $f(u, v) = f(v, u) = 0$ for all vertices $v \\in V$.   We show that, given any flow $f'$ in the flow network $G = (V, E)$, we can construct a flow $f$ as stated in the exercise. The result will follow when $f'$ is a maximum flow. The idea is that even if there is a path from $s$ to the connected component of $u$, no flow can enter the component, since the flow has no path to reach $t$. Thus, all the flow inside the component must be cyclic, which can be made zero without affecting the net value of the flow.  Two cases are possible: where $u$ is not connected to $t$, and where $u$ is not connected to $s$. We only analyze the former case. The analysis for the latter case is similar.  Let $Y$ be the set of all vertices that have no path to $t$. Our roadmap will be to first prove that no flow can leave $Y$. We use this result and flow conservation to prove that no flow can enter $Y$. We shall then constuct the flow $f$, which has the required properties, and prove that $|f| = |f'|$.  The first step is to prove that there can be no flow from a vertex $y \\in Y$ to a vertex $v \\in V - Y$. That is, $f'(y, v) = 0$. This is so, because there are no edges $(y, v)$ in $E$. If there were an edge $(y, v) \\in E$, then there would be a path from $y$ to $t$, which contradicts how we defined the set $Y$.  We will now prove that $f'(v, y) = 0$, too. We will do so by applying flow conservation to each vertex in $Y$ and taking the sum over $Y$. By flow conservation, we have  $$\\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V} f'(v, y).$$  Partitioning $V$ into $Y$ and $V - Y$ gives  $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) + \\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) + \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y). \\tag{*}$$  But we also have  $$\\sum_{y \\in Y} \\sum_{v \\in Y} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y),$$  since the left-hand side is the same as the right-hand side, except for a change of variable names $v$ and $y$. We also have  $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) = 0,$$  since $f'(y, v) = 0$ for each $y \\in Y$ and $v \\in V - Y$. Thus, equation $(*)$ simplifies to  $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) = 0.$$  Because the flow function is nonnegative, $f(v, y) = 0$ for each $v \\in V$ and $y \\in Y$. We conclude that there can be no flow between any vertex in $Y$ and any vertex in $V - Y$.  The same technique can show that if there is a path from $u$ to $t$ but not from $s$ to $u$, and we define $Z$ as the set of vertices that do not have have a path from $s$ to $u$, then there can be no flow between any vertex in $Z$ and any vertex in $V - Z$. Let $X = Y \\cup Z$. We thus have $f'(v, x) = f'(x, v) = 0$ if $x \\in X$ and $v \\ne X$.  We are now ready to construct flow $f$:  $$\nf(u, v) =\n\\begin{cases}\nf'(u, v) & \\text{if $u, v \\ne X$}, \\\\\n0        & \\text{otherwise}.\n\\end{cases}\n$$  We note that $f$ satisfies the requirements of the exercise. We now prove that $f$ also satisfies the requirements of a flow function.  The capacity constraint is satisfied, since whenever $f(u, v) = f'(u, v)$, we have $f(u, v) = f'(u, v) \\le c(u, v)$ and whenever $f(u, v) = 0$, we have $f(u, v) = 0 \\le c(u, v)$.  For flow conservation, let $x$ be some vertex other than $s$ or $t$. If $x \\in X$, then from the construction of $f$, we have  $$\\sum_{v \\in V} f(x, v) = \\sum_{v \\in V} f(v, x) = 0.$$  Otherwise, if $x \\ne X$, note that $f(x, v) = f'(x, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$. Thus,  \\begin{align}\n\\sum_{v \\in V} f(x, v)\n    & = \\sum_{v \\in V} f'(x, v) \\\\\n    & = \\sum_{v \\in V} f'(v, x) \\quad \\text{(because $f'$ obeys flow conservation)} \\\\\n    & = \\sum_{v \\in V} f(v, x).\n\\end{align}  Finally, we prove that the value of the flow remains the same. Since $s \\ne X$, we have $f(s, v) = f'(s, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$, and so  \\begin{align}\n|f| & = \\sum_{v \\in V} f(s, v) - \\sum_{v \\in V} f(v, s) \\\\\n    & = \\sum_{v \\in V} f'(s, v) - \\sum_{v \\in V} f'(v, s) \\\\\n    & = |f'|.\n\\end{align}",
            "title": "26.1-3"
        },
        {
            "location": "/Chap26/26.1/#261-4",
            "text": "Let $f$ be a flow in a network, and let $\\alpha$ be a real number. The  scalar flow product , denoted $\\alpha f$, is a function from $V \\times V$ to $\\mathbb{R}$ defined by  $(\\alpha f)(u, v) = \\alpha \\cdot f(u, v)$.  Prove that the flows in a network form a  convex set . That is, show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ in the range $0 \\le \\alpha \\le 1$.   To see that the flows form a convex set, we show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ such that $0 \\le \\alpha \\le 1$.  For the capacity constraint, first observe that $\\alpha \\le 1$ implies that $1 - \\alpha \\ge 0$. Thus, for any $u, v \\in V$, we have  \\begin{align}\n\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v)\n    & \\ge 0 \\cdot f_1(u, v) + 0 \\cdot (1 - \\alpha) f_2(u, v) \\\\\n    & =   0.\n\\end{align}  Since $f_1(u, v) \\le c(u, v)$ and $f_2(u, v) \\le c(u, v)$, we also have  \\begin{align}\n\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v)\n    & \\le \\alpha c(u, v) + (1 - \\alpha) c(u, v) \\\\\n    & =   (\\alpha + (1 - \\alpha))c(u, v) \\\\\n    & =   c(u, v).\n\\end{align}  For flow conservation, observe that since $f_1$ and $f_2$ obey flow conservation, we have $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ and $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ for any $u \\in V - \\{s, t\\}$. We need to show that  $$\\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u)) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v))$$  for any $u \\in V - \\{s, t\\}$. We multiply both sides of the equality for $f_1$ by $\\alpha$, multiply both sides of the equality for $f_2$ by $1 - \\alpha$, and add the left-hand and right-hand sides of the resulting equalities to get  $$\\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u) = \\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v).$$  Observing that  \\begin{align}\n\\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u) \n    & = \\sum_{v \\in V} \\alpha f_1(v, u) + \\sum_{v \\in V} (1 - \\alpha) f_2(v, u) \\\\\n    & = \\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u))\n\\end{align}  and, likewise, that  $$\\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha)f_2(u, v))$$  completes the proof that flow conservation holds, and thus that flows form a convex set.",
            "title": "26.1-4"
        },
        {
            "location": "/Chap26/26.1/#261-5",
            "text": "State the maximum-flow problem as a linear-programming problem.   \\begin{array}{ll}\n\\max & \\sum\\limits_{v \\in V} f(s, v) - \\sum\\limits_{v \\in V} f(v, s) \\\\\ns.t. & 0 \\le f(u, v) \\le c(u, v) \\\\\n     & \\sum\\limits_{v \\in V} f(v, u) - \\sum\\limits_{v \\in V} f(u, v) = 0\n\\end{array}",
            "title": "26.1-5"
        },
        {
            "location": "/Chap26/26.1/#261-6",
            "text": "Professor Adam has two children who, unfortunately, dislike each other. The problem is so severe that not only do they refuse to walk to school together, but in fact each one refuses to walk on any block that the other child has stepped on that day. The children have no problem with their paths crossing at a corner. Fortunately both the professor's house and the school are on corners, but beyond that he is not sure if it is going to be possible to send both of his children to the same school. The professor has a map of his town. Show how to formulate the problem of determining whether both his children can go to the same school as a maximum-flow problem.   Create a vertex for each corner, and if there is a street between corners $u$ and $v$, create directed edges $(u, v)$ and $(v, u)$. Set the capacity of each edge to $1$. Let the source be corner on which the professor's house sits, and let the sink be the corner on which the school is located. We wish to find a flow of value $2$ that also has the property that $f(u, v)$ is an integer for all vertices $u$ and $v$. Such a flow represents two edge-disjoint paths from the house to the school.",
            "title": "26.1-6"
        },
        {
            "location": "/Chap26/26.1/#261-7",
            "text": "Suppose that, in addition to edge capacities, a flow network has  vertex capacities . That is each vertex $v$ has a limit $l(v)$ on how much flow can pass though $v$. Show how to transform a flow network $G = (V, E)$ with vertex capacities into an equivalent flow network $G' = (V', E')$ without vertex capacities, such that a maximum flow in $G'$ has the same value as a maximum flow in $G$. How many vertices and edges does $G'$ have?   We will construct $G'$ by splitting each vertex $v$ of $G$ into two vertices $v_1$, $v_2$, joined by an edge of capacity $l(v)$. All incoming edges of $v$ are now incoming edges to $v_1$. All outgoing edges from $v$ are now outgoing edges from $v_2$.  More formally, construct $G' = (V', E')$ with capacity function $c'$ as follows. For every $v \\in V$, create two vertices $v_1$, $v_2$ in $V'$. Add an edge $(v_1, v_2)$ in $E'$ with $c'(v_1, v_2) = l(v)$. For every edge $(u, v) \\in E$, create an edge $(u_2, v_1)$ in $E'$ with capacity $c'(u_2, v_1) = c(u, v)$. Make $s_1$ and $t_2$ as the new source and target vertices in $G'$. Clearly, $|V'| = 2|V|$ and $|E'| = |E| + |V|$.  Let $f$ be a flow in $G$ that respects vertex capacities. Create a flow function $f'$ in $G'$ as follows. For each edge $(u, v) \\in G$, let $f'(u_2, v_1) = f(u, v)$. For each vertex $u \\in V - \\{t\\}$, let $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v)$. Let $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t)$.  We readily see that there is a one-to-one correspondence between flows that respect vertex capacities in $G$ and flows in $G'$. For the capacity constraint, every edge in $G'$ of the form $(u_2, v_1)$ has a corresponding edge in $G$ with a corresponding capacity and flow and thus satisfies the capacity constraint. For edges in $E'$ of the form $(u_1, u_2)$, the capacities reflect the vertex capacities in $G$. Therefore, for $u \\in V - \\{s, t\\}$, we have $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v) \\le l(u) = c'(u_1, u_2)$. We also have $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t) \\le l(t) = c'(t_1, t_2)$. Note that this constraint also enforces the vertex capacities in $G$.  Now, we prove flow conservation. By construction, every vertex of the form $u_1$ in $G'$ has exactly one outgoing edge $(u_1, u_2)$, and every incoming edge to $u_1$ corresponds to an incoming edge of $u \\in G$. Thus, for all vertices $u \\in V - \\{s, t\\}$, we have  \\begin{align}\n\\text{incoming flow to $u_1$}\n    & = \\sum_{v \\in V'} f'(v, u_1) \\\\\n    & = \\sum_{v \\in V} f(v, u) \\\\\n    & = \\sum_{v \\in V} f(u, v) \\qquad \\text{(because $f$ obeys flow conservation)} \\\\\n    & = f'(u_1, u_2) \\\\\n    & = \\text{outgoing flow from $u_1$}.\n\\end{align}  For $t_1$, we have  \\begin{align}\n\\text{incoming flow}\n    & = \\sum_{v \\in V'} f'(v, t_1) \\\\\n    & = \\sum_{v \\in V} f(v, u) \\\\\n    & = f'(t_1, t_2) \\\\\n    & = \\text{outgoing flow}.\n\\end{align}  Vertices of the form $u_2$ have exactly one incoming edge $(u_1, u_2)$, and every outgoing edge of $u_2$ corresponds to an outgoing edge of $u \\in G$. Thus, for $u_2 \\ne t_2$,  \\begin{align}\n\\text{incoming flow}\n    & = f'(u_1, u_2) \\\\\n    & = \\sum_{v \\in V} f(u, v) \\\\\n    & = \\sum_{v \\in V'} f'(u_2, v) \\\\   \n    & = \\text{outgoing flow}.\n\\end{align}  Finally, we prove that $|f'| = |f|$:  \\begin{align}\n|f'| & = \\sum_{v \\in V'} f'(s_1, v) \\\\\n     & = f'(s_1, s_2) \\qquad \\text{(because there are no other outgoing edges from $s_1$)} \\\\\n     & = \\sum_{v \\in V} f(s, v) \\\\\n     & = |f|.\n\\end{align}",
            "title": "26.1-7"
        },
        {
            "location": "/Chap26/26.2/",
            "text": "26.2-1\n\n\n\n\nProve that the summations in equation $\\text{(26.6)}$ equal the summations in equation $\\text{(26.7)}$.\n\n\n\n\nLemma\n\n\n\n\nIf $v \\notin V_1$, then $f(s, v) = 0$.\n\n\nIf $v \\notin V_2$, then $f(v, s) = 0$.\n\n\nIf $v \\notin V_1 \\cup V_2$, then $f'(s, v) = 0$.\n\n\nIf $v \\notin V_1 \\cup V_2$, then $f'(v, s) = 0$.\n\n\n\n\nProof\n\n\n\n\nLet $v \\notin V_1$ be some vertex. From the definition of $V_1$, there is no edge from $s$ to $v$. Thus, $f(s, v) = 0$.\n\n\nLet $v \\notin V_2$ be some vertex. From the definition of $V_2$, there is no edge from $v$ to $s$. Thus, $f(v, s) = 0$.\n\n\nLet $v \\notin V_1 \\cup V_2$ be some vertex. From the definition of $V_1$ and $V_2$, neither $(s, v)$ nor $(v, s)$ exists. Therefore, the third condition of the definition of residual capacity (equation $\\text{(26.2)}$) applies, and $c_f(s, v) = 0$. Thus, $f'(s, v) = 0$.\n\n\nLet $v \\notin V_1 \\cup V_2$ be some vertex. By equation $\\text{(26.2)}$, we have that $c_f(v, s) = 0$ and thus $f'(v, s) = 0$.\n\n\n\n\n26.2-2\n\n\n\n\nIn Figure $\\text{26.1}$(b), what is the flow across the cut $(\\{s, v_2, v_4\\}, \\{v_1, v_3, t\\})$? What is the capacity of this cut?\n\n\n\n\n\\begin{align}\nf(S, T) & = f(s, v_1) + f(v_2, v_1) + f(v_4, v_3) + f(v_4, t) - f(v_3, v_2) = 11 + 1 + 7 + 4 - 4 = 19, \\\\\nc(S, T) & = c(s, v_1) + c(v_2, v_1) + c(v_4, v_3) + c(v_4, t) = 16 + 4 + 7 + 4 = 31.\n\\end{align}\n\n\n26.2-3\n\n\n\n\nShow the execution of the Edmonds-Karp algorithm on the flow network of Figure 26.1(a).\n\n\n\n\nIf we perform a breadth first search where we consider the neighbors of a vertex as they appear in the ordering $\\{s, v_1, v_2, v_3, v_4, t\\}$, the first path that we will find is $s, v_1, v_3, t$. The min capacity of this augmenting path is $12$, so we send $12$ units along it. We perform a $\\text{BFS}$ on the resulting residual network. This gets us the path $s, v_2, v_4, t$. The min capacity along this path is $4$, so we send $4$ units along it. Then, the only path remaining in the residual network is $\\{s, v_2, v_4, v_3\\}$ which has a min capacity of $7$, since that's all that's left, we find it in our $\\text{BFS}$. Putting it all together, the total flow that we have found has a value of $23$.\n\n\n26.2-4\n\n\n\n\nIn the example of Figure 26.6, what is the minimum cut corresponding to the maximum flow shown? Of the augmenting paths appearing in the example, which one cancels flow?\n\n\n\n\nA minimum cut corresponding to the maximum flow is $S = \\{s, v_1, v_2, v_4\\}$ and $T = \\{v_3, t\\}$. The augmenting path in part (c) cancels flow on edge $(v_3, v_2)$.\n\n\n26.2-5\n\n\n\n\nRecall that the construction in Section 26.1 that converts a flow network with multiple sources and sinks into a single-source, single-sink network adds edges with infinite capacity. Prove that any flow in the resulting network has a finite value if the edges of the original network with multiple sources and sinks have finite capacity.\n\n\n\n\nSince the only edges that have infinite value are those going from the supersource or to the supersink, as long as we pick a cut that has the supersource and all the original sources on one side, and the other side has the supersink as well as all the original sinks, then it will only cut through edges of finite capacity. Then, by Corollary 26.5, we have that the value of the flow is bounded above by the value of any of these types of cuts, which is finite.\n\n\n26.2-6\n\n\n\n\nSuppose that each source $s_i$ in a flow network with multiple sources and sinks produces exactly $p_i$ units of flow, so that $\\sum_{v \\in V} f(s_i, v) = p_i$. Suppose also that each sink $t_j$ consumes exactly $q_j$ units, so that $\\sum_{v \\in V} f(v, t_j) = q_j$, where $\\sum_i p_i = \\sum_j q_j$. Show how to convert the problem of finding a flow $f$ that obeys these additional constraints into the problem of finding a maximum flow in a single-source, single-sink flow network.\n\n\n\n\n$c(s, s_i) = p_i$, $c(t_j, t) = q_j$.\n\n\n26.2-7\n\n\n\n\nProve Lemma 26.2.\n\n\n\n\nTo check that $f_p$ is a flow, we make sure that it satisfies both the capacity constraints and the flow constraints. First, the capacity constraints. To see this, we recall our definition of $c_f(p)$, that is, it is the smallest residual capacity of any of the edges along the path $p$. Since we have that the residual capacity is always less than or equal to the initial capacity, we have that each value of the flow is less than the capacity. Second, we check the flow constraints, Since the only edges that are given any flow are along a path, we have that at each vertex interior to the path, the flow in from one edge is immediately canceled by the flow out to the next vertex in the path. Lastly, we can check that its value is equal to $c_f(p)$ because, while $s$ may show up at spots later on in the path, it will be canceled out as it leaves to go to the next vertex. So, the only net flow from s is the initial edge along the path, since it (along with all the other edges) is given flow $c_f(p)$, that is the value of the flow $f_p$.\n\n\n26.2-8\n\n\n\n\nSuppose that we redefine the residual network to disallow edges into $s$. Argue that the procedure $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow.\n\n\n\n\nLet $G_f$ be the residual network just before an iteration of the \nwhile\n loop of $\\text{FORD-FULKERSON}$, and let $E_s$ be the set of residual edges of $G_f$ into $s$. We'll show that the augmenting path $p$ chosen by $\\text{FORD-FULKERSON}$ does not include an edge in $E_s$. Thus, even if we redefine $G_f$ to disallow edges in $E_s$, the path $p$ still remains an augmenting path in the redefined network. Since $p$ remains unchanged, an iteration of the \nwhile\n loop of $\\text{FORD-FULKERSON}$ updates the flow in the same way as before the redefinition. Furthermore, by disallowing some edges, we do not introduce any new augmenting paths. Thus, $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow.\n\n\nNow, we prove that $\\text{FORD-FULKERSON}$ never chooses an augmenting path $p$ that includes an edge $(v, s) \\in E_s$. Why? The path $p$ always starts from $s$, and if $p$ included an edge $(v, s)$, the vertex $s$ would be repeated twice in the path. Thus, $p$ would no longer be a \nsimple\n path. Since $\\text{FORD-FULKERSON}$ chooses only simple paths, $p$ cannot include $(v, s)$.\n\n\n26.2-9\n\n\n\n\nSuppose that both $f$ and $f'$ are flows in a network $G$ and we compute flow $f \\uparrow f'$. Does the augmented flow satisfy the flow conservation property? Does it satisfy the capacity constraint?\n\n\n\n\nThe augmented flow $f \\uparrow f'$ satisfies the flow conservation property but not the capacity constraint property.\n\n\nFirst, we prove that $f \\uparrow f'$ satisfies the flow conservation property. We note that if edge $(u, v) \\in E$, then $(v, u) \\ne E$ and $f'(v, u) = 0$. Thus, we can rewrite the definition of flow augmentation (equation $\\text{(26.4)}$), when applied to two flows, as\n\n\n$$\n(f \\uparrow f')(u, v) =\n\\begin{cases}\nf(u, v) + f'(u, v) & \\text{if $(u, v) \\in E$}, \\\\\n0                  & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nThe definition implies that the new flow on each edge is simply the sum of the two flows on that edge. We now prove that in $f \\uparrow f'$, the net incoming flow for each vertex equals the net outgoing flow. Let $u \\ne {s, t}$ be any vertex of $G$. We have\n\n\n\\begin{align}\n\\sum_{v \\in V} (f \\uparrow f') (v, u)\n    & = \\sum_{v \\in V} (f(v, u) + f'(v, u)) \\\\\n    & = \\sum_{v \\in V} f(v, u) + \\sum_{v \\in V} f'(v, u) \\\\\n    & = \\sum_{v \\in V} f(u, v) + \\sum_{v \\in V} f'(u, v) \\quad \\text{(because $f$, $f'$ obey flow conservation)} \\\\\n    & = \\sum_{v \\in V} (f(u, v) + f'(u, v)) \\\\\n    & = \\sum_{v \\in V} (f \\uparrow f') (u, v).\n\\end{align}\n\n\nWe conclude that $f \\uparrow f'$ satisfies flow conservation.\n\n\nWe now show that $f \\uparrow f'$ need not satisfy the capacity constraint by giving a simple counterexample. Let the flow network $G$ have just a source and a target vertex, with a single edge $(s, t)$ having $c(s, t) = 1$. Define the flows $f$ and $f'$ to have $f(s, t) = f'(s, t) = 1$. Then, we have $(f \\uparrow f')(s, t) = 2 > c(s, t)$. We conclude that $f \\uparrow f'$ need not satisfy the capacity constraint.\n\n\n26.2-10\n\n\n\n\nShow how to find a maximum flow in a network $G = (V, E)$ by a sequence of at most $|E|$ augmenting paths. ($\\textit{Hint:}$ Determine the paths after finding the maximum flow.)\n\n\n\n\nSuppose we already have a maximum flow $f$. Consider a new graph $G$ where we set the capacity of edge $(u, v)$ to $f(u, v)$. Run Ford-Fulkerson, with the modification that we remove an edge if its flow reaches its capacity. In other words, if $f(u, v) = c(u, v)$ then there should be no reverse edge appearing in residual network. This will still produce correct output in our case because we never exceed the actual maximum flow through an edge, so it is never advantageous to cancel flow. The augmenting paths chosen in this modified version of Ford-Fulkerson are precisely the ones we want. There are at most $|E|$ because every augmenting path produces at least one edge whose flow is equal to its capacity, which we set to be the actual flow for the edge in a maximum flow, and our modification prevents us from ever destroying this progress.\n\n\n26.2-11\n\n\n\n\nThe \nedge connectivity\n of an undirected graph is the minimum number $k$ of edges that must be removed to disconnect the graph. For example, the edge connectivity of a tree is $1$, and the edge connectivity of a cyclic chain of vertices is $2$. Show how to determine the edge connectivity of an undirected graph $G = (V, E)$ by running a maximum-flow algorithm on at most $|V|$ flow networks, each having $O(V)$ vertices and $O(E)$ edges.\n\n\n\n\nFor any two vertices $u$ and $v$ in $G$, we can define a flow network $G_{uv}$ consisting of the directed version of $G$ with $s = u$, $t = v$, and all edge capacities set to $1$. (The flow network $G_{uv}$ has $V$ vertices and $2|E|$ edges, so that it has $O(V)$ vertices and $O(E)$ edges, as required. We want all capacities to be $1$ so that the number of edges of $G$ crossing a cut equals the capacity of the cut in $G_{uv}$.) Let $f_{uv}$ denote a maximum flow in $G_{uv}$.\n\n\nWe claim that for any $u \\in V$, the edge connectivity $k$ equals $\\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$. We'll show below that this claim holds. Assuming that it holds, we can find $k$ as follows:\n\n\nEDGE\n-\nCONNECTIVITY\n(\nG\n)\n\n    \nk\n \n=\n \n\u221e\n\n    \nselect\n \nany\n \nvertex\n \nu\n \n\u2208\n \nG\n.\nV\n\n    \nfor\n \neach\n \nvertex\n \nv\n \n\u2208\n \nG\n.\nV\n \n-\n \n{\nu\n}\n\n        \nset\n \nup\n \nthe\n \nflow\n \nnetwork\n \nG_\n{\nuv\n}\n \nas\n \ndescribed\n \nabove\n\n        \nfind\n \nthe\n \nmaximum\n \nflow\n \nf_\n{\nuv\n}\n \non\n \nG_\n{\nuv\n}\n\n        \nk\n \n=\n \nmin\n(\nk\n,\n \n|\nf_\n{\nuv\n}\n|\n)\n\n    \nreturn\n \nk\n\n\n\n\n\nThe claim follows from the max-flow min-cut theorem and how we chose capacities so that the capacity of a cut is the number of edges crossing it. We prove that $k = \\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$ for any $u \\in V$ by showing separately that $k$ is at least this minimum and that $k$ is at most this minimum.\n\n\n\n\nProof that $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$:\n\n    Let $m = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$. Suppose we remove only $m - 1$ edges from $G$. For any vertex $v$, by the max-flow min-cut theorem, $u$ and $v$ are still connected. (The max flow from $u$ to $v$ is at least $m$, hence any cut separating $u$ from $v$ has capacity at least $m$, which means at least $m$ edges cross any such cut. Thus at least one edge is left crossing the cut when we remove $m - 1$ edges.) Thus every vertex is connected to $u$, which implies that the graph is still connected. So at least $m$ edges must be removed to disconnect the graph\u2014i.e., $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$. \n\n\nProof that $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$:\n\n    Consider a vertex $v$ with the minimum $|f_{uv}|$. By the max-flow min-cut theorem, there is a cut of capacity $|f_{uv}|$ separating $u$ and $v$. Since all edge capacities are $1$, exactly $|f_{uv}|$ edges cross this cut. If these edges are removed, there is no path from $u$ to $v$, and so our graph becomes disconnected. Hence $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$\n\n\nThus, the claim that $k = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$ for any $u \\in V$ is true.\n\n\n\n\n26.2-12\n\n\n\n\nSuppose that you are given a flow network $G$, and $G$ has edges entering the source $s$. Let $f$ be a flow in $G$ in which one of the edges $(v, s)$ entering the source has $f(v, s) = 1$. Prove that there must exist another flow $f'$ with $f'(v, s) = 0$ such that $|f| = |f'|$. Give an $O(E)$-time algorithm to compute $f'$, given $f$, and assuming that all edge capacities are integers.\n\n\n\n\nThe idea of the proof is that if $f(v, s) = 1$, then there must exist a cycle containing the edge $(v, s)$ and for which each edge carries one unit of flow. If we reduce the flow on each edge in the cycle by one unit, we can reduce $f(v, s)$ to $0$ without affecting the value of the flow.\n\n\nGiven the flow network $G$ and the flow $f$, we say that vertex $y$ is \nflow-connected\n to vertex $z$ if there exists a path $p$ from $y$ to $z$ such that each edge of $p$ has a positive flow on it. We also define $y$ to be flow-connected to itself. In particular, $s$ is flow-connected to $s$.\n\n\nWe start by proving the following lemma:\n\n\nLemma\n \n\n\nLet $G = (V, E)$ be a flow network and $f$ be a flow in $G$. If $s$ is not flow-connected to $v$, then $f(v, s) = 0$.\n\n\nProof\n \n\n\nThe idea is that since $s$ is not flow-connected to $v$, there cannot be any flow from $s$ to $v$. By using flow conservation, we will prove that there cannot be any flow from $v$ to $s$ either, and thus that $f(v, s) = 0$.\n\n\nLet $Y$ be the set of all vertices $y$ such that $s$ is flow-connected to $y$. By applying flow conservation to vertices in $V - Y$ and taking the sum, we obtain\n\n\n$$\\sum_{z \\in V - Y} \\sum_{x \\in V} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V} f(z, x).$$\n\n\nPartitioning $V$ into $Y$ and $V - Y$ gives\n\n\n$$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x). \\tag{*}$$\n\n\nBut we have\n\n\n$$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x),$$\n\n\nsince the left-hand side is the same as the right-hand side, except for a change of variable names $x$ and $z$. We also have\n\n\n$$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = 0,$$\n\n\nsince the flow from any vertex in $Y$ to any vertex in $V - Y$ must be $0$. Thus, equation $(*)$ simplifies to\n\n\n$$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x) = 0.$$\n\n\nThe above equation implies that $f(z, x) = 0$ for each $z \\in V - Y$ and $x \\in Y$. In particular, since $v \\in V - Y$ and $s \\in Y$, we have that $f(v, s) = 0$.\n\n\nNow, we show how to construct the required flow $f'$. By the contrapositive of the lemma, $f(v, s) > 0$ implies that $s$ is flow-connected to $v$ through some path $p$. Let path $p'$ be the path $s \\overset{p}{\\leadsto} v \\to s$. Path $p'$ is a cycle that has positive flow on each edge. Because we assume that all edge capacities are integers, the flow on each edge of $p'$ is at least $1$. If we subtract $1$ from each edge of the cycle to obtain a flow $f'$, then $f'$ still satisfies the properties of a flow network and has the same value as $|f|$. Because edge $(v, s)$ is in the cycle, we have that $f'(v, s) = f(v, s) - 1 = 0$.\n\n\n26.2-13\n\n\n\n\nSuppose that you wish to find, among all minimum cuts in a flow network $G$ with integral capacities, one that contains the smallest number of edges. Show how to modify the capacities of $G$ to create a new flow network $G'$ in which any minimum cut in $G'$ is a minimum cut with the smallest number of edges in $G$.\n\n\n\n\nLet $(S, T)$ and $(X, Y)$ be two cuts in $G$ (and $G'$). Let $c'$ be the capacity function of $G'$. One way to define $c'$ is to add a small amount $\\delta$ to the capacity of each edge in $G$. That is, if $u$ and $v$ are two vertices, we set\n\n\n$$c'(u, v) = c(u, v) + \\delta.$$\n\n\nThus, if $c(S, T) = c(X, Y)$ and $(S, T)$ has fewer edges than $(X, Y)$, then we would have $c'(S, T) < c'(X, Y)$. We have to be careful and choose a small $\\delta$, lest we change the relative ordering of two unequal capacities. That is, if $c(S, T) < c(X, Y)$, then no matter many more edges $(S, T)$ has than $(X, Y)$, we still need to have $c'(S, T) < c'(X, Y)$. With this definition of $c'$, a minimum cut in $G'$ will be a minimum cut in $G$ that has the minimum number of edges.\n\n\nHow should we choose the value of $\\delta$? Let $m$ be the minimum difference between capacities of two unequal-capacity cuts in $G$. Choose $\\delta = m / (2|E|)$. For any cut $(S, T)$, since the cut can have at most $|E|$ edges, we can bound $c'(S, T)$ by\n\n\n$$c(S, T) \\le c'(S, T) \\le c(S, T) + |E| \\cdot \\delta.$$\n\n\nLet $c(S, T) < c(X, Y)$. We need to prove that $c'(S, T) < c'(X, Y)$. We have\n\n\n\\begin{align}\nc'(S, T)\n    & \\le c(S, T) + |E| \\cdot \\delta \\\\\n    & =   c(S, T) + m / 2 \\\\\n    & <   c(X, Y) \\qquad \\text{(since $c(X, Y) - c(S, T) \\ge m$)} \\\\\n    & \\le c'(X, Y).\n\\end{align}\n\n\nBecause all capacities are integral, we can choose $m = 1$, obtaining $\\delta = 1 / 2|E|$. To avoid dealing with fractional values, we can scale all capacities by $2|E|$ to obtain\n\n\n$$c'(u, v) = 2|E| \\cdot c(u, v) + 1.$$",
            "title": "26.2 The Ford-Fulkerson method"
        },
        {
            "location": "/Chap26/26.2/#262-1",
            "text": "Prove that the summations in equation $\\text{(26.6)}$ equal the summations in equation $\\text{(26.7)}$.   Lemma   If $v \\notin V_1$, then $f(s, v) = 0$.  If $v \\notin V_2$, then $f(v, s) = 0$.  If $v \\notin V_1 \\cup V_2$, then $f'(s, v) = 0$.  If $v \\notin V_1 \\cup V_2$, then $f'(v, s) = 0$.   Proof   Let $v \\notin V_1$ be some vertex. From the definition of $V_1$, there is no edge from $s$ to $v$. Thus, $f(s, v) = 0$.  Let $v \\notin V_2$ be some vertex. From the definition of $V_2$, there is no edge from $v$ to $s$. Thus, $f(v, s) = 0$.  Let $v \\notin V_1 \\cup V_2$ be some vertex. From the definition of $V_1$ and $V_2$, neither $(s, v)$ nor $(v, s)$ exists. Therefore, the third condition of the definition of residual capacity (equation $\\text{(26.2)}$) applies, and $c_f(s, v) = 0$. Thus, $f'(s, v) = 0$.  Let $v \\notin V_1 \\cup V_2$ be some vertex. By equation $\\text{(26.2)}$, we have that $c_f(v, s) = 0$ and thus $f'(v, s) = 0$.",
            "title": "26.2-1"
        },
        {
            "location": "/Chap26/26.2/#262-2",
            "text": "In Figure $\\text{26.1}$(b), what is the flow across the cut $(\\{s, v_2, v_4\\}, \\{v_1, v_3, t\\})$? What is the capacity of this cut?   \\begin{align}\nf(S, T) & = f(s, v_1) + f(v_2, v_1) + f(v_4, v_3) + f(v_4, t) - f(v_3, v_2) = 11 + 1 + 7 + 4 - 4 = 19, \\\\\nc(S, T) & = c(s, v_1) + c(v_2, v_1) + c(v_4, v_3) + c(v_4, t) = 16 + 4 + 7 + 4 = 31.\n\\end{align}",
            "title": "26.2-2"
        },
        {
            "location": "/Chap26/26.2/#262-3",
            "text": "Show the execution of the Edmonds-Karp algorithm on the flow network of Figure 26.1(a).   If we perform a breadth first search where we consider the neighbors of a vertex as they appear in the ordering $\\{s, v_1, v_2, v_3, v_4, t\\}$, the first path that we will find is $s, v_1, v_3, t$. The min capacity of this augmenting path is $12$, so we send $12$ units along it. We perform a $\\text{BFS}$ on the resulting residual network. This gets us the path $s, v_2, v_4, t$. The min capacity along this path is $4$, so we send $4$ units along it. Then, the only path remaining in the residual network is $\\{s, v_2, v_4, v_3\\}$ which has a min capacity of $7$, since that's all that's left, we find it in our $\\text{BFS}$. Putting it all together, the total flow that we have found has a value of $23$.",
            "title": "26.2-3"
        },
        {
            "location": "/Chap26/26.2/#262-4",
            "text": "In the example of Figure 26.6, what is the minimum cut corresponding to the maximum flow shown? Of the augmenting paths appearing in the example, which one cancels flow?   A minimum cut corresponding to the maximum flow is $S = \\{s, v_1, v_2, v_4\\}$ and $T = \\{v_3, t\\}$. The augmenting path in part (c) cancels flow on edge $(v_3, v_2)$.",
            "title": "26.2-4"
        },
        {
            "location": "/Chap26/26.2/#262-5",
            "text": "Recall that the construction in Section 26.1 that converts a flow network with multiple sources and sinks into a single-source, single-sink network adds edges with infinite capacity. Prove that any flow in the resulting network has a finite value if the edges of the original network with multiple sources and sinks have finite capacity.   Since the only edges that have infinite value are those going from the supersource or to the supersink, as long as we pick a cut that has the supersource and all the original sources on one side, and the other side has the supersink as well as all the original sinks, then it will only cut through edges of finite capacity. Then, by Corollary 26.5, we have that the value of the flow is bounded above by the value of any of these types of cuts, which is finite.",
            "title": "26.2-5"
        },
        {
            "location": "/Chap26/26.2/#262-6",
            "text": "Suppose that each source $s_i$ in a flow network with multiple sources and sinks produces exactly $p_i$ units of flow, so that $\\sum_{v \\in V} f(s_i, v) = p_i$. Suppose also that each sink $t_j$ consumes exactly $q_j$ units, so that $\\sum_{v \\in V} f(v, t_j) = q_j$, where $\\sum_i p_i = \\sum_j q_j$. Show how to convert the problem of finding a flow $f$ that obeys these additional constraints into the problem of finding a maximum flow in a single-source, single-sink flow network.   $c(s, s_i) = p_i$, $c(t_j, t) = q_j$.",
            "title": "26.2-6"
        },
        {
            "location": "/Chap26/26.2/#262-7",
            "text": "Prove Lemma 26.2.   To check that $f_p$ is a flow, we make sure that it satisfies both the capacity constraints and the flow constraints. First, the capacity constraints. To see this, we recall our definition of $c_f(p)$, that is, it is the smallest residual capacity of any of the edges along the path $p$. Since we have that the residual capacity is always less than or equal to the initial capacity, we have that each value of the flow is less than the capacity. Second, we check the flow constraints, Since the only edges that are given any flow are along a path, we have that at each vertex interior to the path, the flow in from one edge is immediately canceled by the flow out to the next vertex in the path. Lastly, we can check that its value is equal to $c_f(p)$ because, while $s$ may show up at spots later on in the path, it will be canceled out as it leaves to go to the next vertex. So, the only net flow from s is the initial edge along the path, since it (along with all the other edges) is given flow $c_f(p)$, that is the value of the flow $f_p$.",
            "title": "26.2-7"
        },
        {
            "location": "/Chap26/26.2/#262-8",
            "text": "Suppose that we redefine the residual network to disallow edges into $s$. Argue that the procedure $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow.   Let $G_f$ be the residual network just before an iteration of the  while  loop of $\\text{FORD-FULKERSON}$, and let $E_s$ be the set of residual edges of $G_f$ into $s$. We'll show that the augmenting path $p$ chosen by $\\text{FORD-FULKERSON}$ does not include an edge in $E_s$. Thus, even if we redefine $G_f$ to disallow edges in $E_s$, the path $p$ still remains an augmenting path in the redefined network. Since $p$ remains unchanged, an iteration of the  while  loop of $\\text{FORD-FULKERSON}$ updates the flow in the same way as before the redefinition. Furthermore, by disallowing some edges, we do not introduce any new augmenting paths. Thus, $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow.  Now, we prove that $\\text{FORD-FULKERSON}$ never chooses an augmenting path $p$ that includes an edge $(v, s) \\in E_s$. Why? The path $p$ always starts from $s$, and if $p$ included an edge $(v, s)$, the vertex $s$ would be repeated twice in the path. Thus, $p$ would no longer be a  simple  path. Since $\\text{FORD-FULKERSON}$ chooses only simple paths, $p$ cannot include $(v, s)$.",
            "title": "26.2-8"
        },
        {
            "location": "/Chap26/26.2/#262-9",
            "text": "Suppose that both $f$ and $f'$ are flows in a network $G$ and we compute flow $f \\uparrow f'$. Does the augmented flow satisfy the flow conservation property? Does it satisfy the capacity constraint?   The augmented flow $f \\uparrow f'$ satisfies the flow conservation property but not the capacity constraint property.  First, we prove that $f \\uparrow f'$ satisfies the flow conservation property. We note that if edge $(u, v) \\in E$, then $(v, u) \\ne E$ and $f'(v, u) = 0$. Thus, we can rewrite the definition of flow augmentation (equation $\\text{(26.4)}$), when applied to two flows, as  $$\n(f \\uparrow f')(u, v) =\n\\begin{cases}\nf(u, v) + f'(u, v) & \\text{if $(u, v) \\in E$}, \\\\\n0                  & \\text{otherwise}.\n\\end{cases}\n$$  The definition implies that the new flow on each edge is simply the sum of the two flows on that edge. We now prove that in $f \\uparrow f'$, the net incoming flow for each vertex equals the net outgoing flow. Let $u \\ne {s, t}$ be any vertex of $G$. We have  \\begin{align}\n\\sum_{v \\in V} (f \\uparrow f') (v, u)\n    & = \\sum_{v \\in V} (f(v, u) + f'(v, u)) \\\\\n    & = \\sum_{v \\in V} f(v, u) + \\sum_{v \\in V} f'(v, u) \\\\\n    & = \\sum_{v \\in V} f(u, v) + \\sum_{v \\in V} f'(u, v) \\quad \\text{(because $f$, $f'$ obey flow conservation)} \\\\\n    & = \\sum_{v \\in V} (f(u, v) + f'(u, v)) \\\\\n    & = \\sum_{v \\in V} (f \\uparrow f') (u, v).\n\\end{align}  We conclude that $f \\uparrow f'$ satisfies flow conservation.  We now show that $f \\uparrow f'$ need not satisfy the capacity constraint by giving a simple counterexample. Let the flow network $G$ have just a source and a target vertex, with a single edge $(s, t)$ having $c(s, t) = 1$. Define the flows $f$ and $f'$ to have $f(s, t) = f'(s, t) = 1$. Then, we have $(f \\uparrow f')(s, t) = 2 > c(s, t)$. We conclude that $f \\uparrow f'$ need not satisfy the capacity constraint.",
            "title": "26.2-9"
        },
        {
            "location": "/Chap26/26.2/#262-10",
            "text": "Show how to find a maximum flow in a network $G = (V, E)$ by a sequence of at most $|E|$ augmenting paths. ($\\textit{Hint:}$ Determine the paths after finding the maximum flow.)   Suppose we already have a maximum flow $f$. Consider a new graph $G$ where we set the capacity of edge $(u, v)$ to $f(u, v)$. Run Ford-Fulkerson, with the modification that we remove an edge if its flow reaches its capacity. In other words, if $f(u, v) = c(u, v)$ then there should be no reverse edge appearing in residual network. This will still produce correct output in our case because we never exceed the actual maximum flow through an edge, so it is never advantageous to cancel flow. The augmenting paths chosen in this modified version of Ford-Fulkerson are precisely the ones we want. There are at most $|E|$ because every augmenting path produces at least one edge whose flow is equal to its capacity, which we set to be the actual flow for the edge in a maximum flow, and our modification prevents us from ever destroying this progress.",
            "title": "26.2-10"
        },
        {
            "location": "/Chap26/26.2/#262-11",
            "text": "The  edge connectivity  of an undirected graph is the minimum number $k$ of edges that must be removed to disconnect the graph. For example, the edge connectivity of a tree is $1$, and the edge connectivity of a cyclic chain of vertices is $2$. Show how to determine the edge connectivity of an undirected graph $G = (V, E)$ by running a maximum-flow algorithm on at most $|V|$ flow networks, each having $O(V)$ vertices and $O(E)$ edges.   For any two vertices $u$ and $v$ in $G$, we can define a flow network $G_{uv}$ consisting of the directed version of $G$ with $s = u$, $t = v$, and all edge capacities set to $1$. (The flow network $G_{uv}$ has $V$ vertices and $2|E|$ edges, so that it has $O(V)$ vertices and $O(E)$ edges, as required. We want all capacities to be $1$ so that the number of edges of $G$ crossing a cut equals the capacity of the cut in $G_{uv}$.) Let $f_{uv}$ denote a maximum flow in $G_{uv}$.  We claim that for any $u \\in V$, the edge connectivity $k$ equals $\\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$. We'll show below that this claim holds. Assuming that it holds, we can find $k$ as follows:  EDGE - CONNECTIVITY ( G ) \n     k   =   \u221e \n     select   any   vertex   u   \u2208   G . V \n     for   each   vertex   v   \u2208   G . V   -   { u } \n         set   up   the   flow   network   G_ { uv }   as   described   above \n         find   the   maximum   flow   f_ { uv }   on   G_ { uv } \n         k   =   min ( k ,   | f_ { uv } | ) \n     return   k   The claim follows from the max-flow min-cut theorem and how we chose capacities so that the capacity of a cut is the number of edges crossing it. We prove that $k = \\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$ for any $u \\in V$ by showing separately that $k$ is at least this minimum and that $k$ is at most this minimum.   Proof that $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$: \n    Let $m = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$. Suppose we remove only $m - 1$ edges from $G$. For any vertex $v$, by the max-flow min-cut theorem, $u$ and $v$ are still connected. (The max flow from $u$ to $v$ is at least $m$, hence any cut separating $u$ from $v$ has capacity at least $m$, which means at least $m$ edges cross any such cut. Thus at least one edge is left crossing the cut when we remove $m - 1$ edges.) Thus every vertex is connected to $u$, which implies that the graph is still connected. So at least $m$ edges must be removed to disconnect the graph\u2014i.e., $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$.   Proof that $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$: \n    Consider a vertex $v$ with the minimum $|f_{uv}|$. By the max-flow min-cut theorem, there is a cut of capacity $|f_{uv}|$ separating $u$ and $v$. Since all edge capacities are $1$, exactly $|f_{uv}|$ edges cross this cut. If these edges are removed, there is no path from $u$ to $v$, and so our graph becomes disconnected. Hence $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$  Thus, the claim that $k = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$ for any $u \\in V$ is true.",
            "title": "26.2-11"
        },
        {
            "location": "/Chap26/26.2/#262-12",
            "text": "Suppose that you are given a flow network $G$, and $G$ has edges entering the source $s$. Let $f$ be a flow in $G$ in which one of the edges $(v, s)$ entering the source has $f(v, s) = 1$. Prove that there must exist another flow $f'$ with $f'(v, s) = 0$ such that $|f| = |f'|$. Give an $O(E)$-time algorithm to compute $f'$, given $f$, and assuming that all edge capacities are integers.   The idea of the proof is that if $f(v, s) = 1$, then there must exist a cycle containing the edge $(v, s)$ and for which each edge carries one unit of flow. If we reduce the flow on each edge in the cycle by one unit, we can reduce $f(v, s)$ to $0$ without affecting the value of the flow.  Given the flow network $G$ and the flow $f$, we say that vertex $y$ is  flow-connected  to vertex $z$ if there exists a path $p$ from $y$ to $z$ such that each edge of $p$ has a positive flow on it. We also define $y$ to be flow-connected to itself. In particular, $s$ is flow-connected to $s$.  We start by proving the following lemma:  Lemma    Let $G = (V, E)$ be a flow network and $f$ be a flow in $G$. If $s$ is not flow-connected to $v$, then $f(v, s) = 0$.  Proof    The idea is that since $s$ is not flow-connected to $v$, there cannot be any flow from $s$ to $v$. By using flow conservation, we will prove that there cannot be any flow from $v$ to $s$ either, and thus that $f(v, s) = 0$.  Let $Y$ be the set of all vertices $y$ such that $s$ is flow-connected to $y$. By applying flow conservation to vertices in $V - Y$ and taking the sum, we obtain  $$\\sum_{z \\in V - Y} \\sum_{x \\in V} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V} f(z, x).$$  Partitioning $V$ into $Y$ and $V - Y$ gives  $$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x). \\tag{*}$$  But we have  $$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x),$$  since the left-hand side is the same as the right-hand side, except for a change of variable names $x$ and $z$. We also have  $$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = 0,$$  since the flow from any vertex in $Y$ to any vertex in $V - Y$ must be $0$. Thus, equation $(*)$ simplifies to  $$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x) = 0.$$  The above equation implies that $f(z, x) = 0$ for each $z \\in V - Y$ and $x \\in Y$. In particular, since $v \\in V - Y$ and $s \\in Y$, we have that $f(v, s) = 0$.  Now, we show how to construct the required flow $f'$. By the contrapositive of the lemma, $f(v, s) > 0$ implies that $s$ is flow-connected to $v$ through some path $p$. Let path $p'$ be the path $s \\overset{p}{\\leadsto} v \\to s$. Path $p'$ is a cycle that has positive flow on each edge. Because we assume that all edge capacities are integers, the flow on each edge of $p'$ is at least $1$. If we subtract $1$ from each edge of the cycle to obtain a flow $f'$, then $f'$ still satisfies the properties of a flow network and has the same value as $|f|$. Because edge $(v, s)$ is in the cycle, we have that $f'(v, s) = f(v, s) - 1 = 0$.",
            "title": "26.2-12"
        },
        {
            "location": "/Chap26/26.2/#262-13",
            "text": "Suppose that you wish to find, among all minimum cuts in a flow network $G$ with integral capacities, one that contains the smallest number of edges. Show how to modify the capacities of $G$ to create a new flow network $G'$ in which any minimum cut in $G'$ is a minimum cut with the smallest number of edges in $G$.   Let $(S, T)$ and $(X, Y)$ be two cuts in $G$ (and $G'$). Let $c'$ be the capacity function of $G'$. One way to define $c'$ is to add a small amount $\\delta$ to the capacity of each edge in $G$. That is, if $u$ and $v$ are two vertices, we set  $$c'(u, v) = c(u, v) + \\delta.$$  Thus, if $c(S, T) = c(X, Y)$ and $(S, T)$ has fewer edges than $(X, Y)$, then we would have $c'(S, T) < c'(X, Y)$. We have to be careful and choose a small $\\delta$, lest we change the relative ordering of two unequal capacities. That is, if $c(S, T) < c(X, Y)$, then no matter many more edges $(S, T)$ has than $(X, Y)$, we still need to have $c'(S, T) < c'(X, Y)$. With this definition of $c'$, a minimum cut in $G'$ will be a minimum cut in $G$ that has the minimum number of edges.  How should we choose the value of $\\delta$? Let $m$ be the minimum difference between capacities of two unequal-capacity cuts in $G$. Choose $\\delta = m / (2|E|)$. For any cut $(S, T)$, since the cut can have at most $|E|$ edges, we can bound $c'(S, T)$ by  $$c(S, T) \\le c'(S, T) \\le c(S, T) + |E| \\cdot \\delta.$$  Let $c(S, T) < c(X, Y)$. We need to prove that $c'(S, T) < c'(X, Y)$. We have  \\begin{align}\nc'(S, T)\n    & \\le c(S, T) + |E| \\cdot \\delta \\\\\n    & =   c(S, T) + m / 2 \\\\\n    & <   c(X, Y) \\qquad \\text{(since $c(X, Y) - c(S, T) \\ge m$)} \\\\\n    & \\le c'(X, Y).\n\\end{align}  Because all capacities are integral, we can choose $m = 1$, obtaining $\\delta = 1 / 2|E|$. To avoid dealing with fractional values, we can scale all capacities by $2|E|$ to obtain  $$c'(u, v) = 2|E| \\cdot c(u, v) + 1.$$",
            "title": "26.2-13"
        },
        {
            "location": "/Chap26/26.3/",
            "text": "26.3-1\n\n\n\n\nRun the Ford-Fulkerson algorithm on the flow network in Figure 26.8(c) and show the residual network after each flow augmentation. Number the vertices in $L$ top to bottom from 1 to 5 and in $R$ top to bottom from 6 to 9. For each iteration, pick the augmenting path that is lexicographically smallest.\n\n\n\n\nFirst, we pick an augmenting path that passes through vertices 1 and 6. Then, we pick the path going through 2 and 8. Then, we pick the path going through 3 and 7. Then, the resulting residual graph has no path from $s$ to $t$. So, we know that we are done, and that we are pairing up vertices $(1, 6)$, $(2, 8)$, and $(3, 7)$. This number of unit augmenting paths agrees with the value of the cut where you cut the edges $(s, 3)$, $(6, t)$, and $(7, t)$.\n\n\n26.3-2\n\n\n\n\nProve Theorem 26.10.\n\n\n\n\nWe proceed by induction on the number of iterations of the while loop of Ford-Fulkerson. After the first iteration, since $c$ only takes on integer values and $(u, v).f$ is set to $0$, $c_f$ only takes on integer values. Thus, lines 7 and 8 of Ford-Fulkerson only assign integer values to $(u, v).f$. Assume that $(u, v).f \\in \\mathbb Z$ for all $(u, v)$ after the $n$th iteration. On the $(n + 1)$th iteration $c_f(p)$ is set to the minimum of $c_f(u, v)$ which is an integer by the induction hypothesis. Lines 7 and 8 compute $(u, v).f$ or $(v, u).f$. Either way, these the the sum or difference of integers by assumption, so after the $(n + 1)$th iteration we have that $(u, v).f$ is an integer for all $(u, v) \\in E$. Since the value of the flow is a sum of flows of edges, we must have $|f| \\in \\mathbb Z$ as well.\n\n\n26.3-3\n\n\n\n\nLet $G = (V, E)$ be a bipartite graph with vertex partition $V = L \\cup R$, and let $G'$ be its corresponding flow network. Give a good upper bound on the length of any augmenting path found in $G'$ during the execution of $\\text{FORD-FULKERSON}$.\n\n\n\n\nBy definition, an augmenting path is a simple path $s \\leadsto t$ in the residual network $G_f'$. Since $G$ has no edges between vertices in $L$ and no edges between vertices in $R$, neither does the flow network $G'$ and hence neither does $G_f'$. Also, the only edges involving $s$ or $t$ connect $s$ to $L$ and $R$ to $t$. Note that although edges in $G'$ can go only from $L$ to $R$, edges in $G_f'$ can also go from $R$ to $L$.\n\n\nThus any augmenting path must go\n\n\n$$s \\to L \\to R \\to \\cdots \\to L \\to R \\to t,$$\n\n\ncrossing back and forth between $L$ and $R$ at most as many times as it can do so without using a vertex twice. It contains $s$, $t$, and equal numbers of distinct vertices from $L$ and $R$\u2014at most $2 + 2 \\cdot \\min(|L|, |R|)$ vertices in all. The length of an augmenting path (i.e., its number of edges) is thus bounded above by $2 \\cdot \\min(|L|, |R|) + 1$.\n\n\n26.3-4 $\\star$\n\n\n\n\nA \nperfect matching\n is a matching in which every vertex is matched. Let $G = (V, E)$ be an undirected bipartite graph with vertex partition $V = L \\cup R$, where $|L| = |R|$. For any $X \\subseteq V$, define the \nneighborhood\n of $X$ as\n\n\n$N(X) = \\{y \\in V: (x, y) \\in E \\text{ for some } x \\in X\\}$,\n\n\nthat is, the set of vertices adjacent to some member of $X$. Prove \nHall's theorem\n: there exists a perfect matching in $G$ if and only if $|A| \\le |N(A)|$ for every subset $A \\subseteq L$.\n\n\n\n\nFirst suppose there exists a perfect matching in $G$. Then for any subset $A \\subseteq L$, each vertex of $A$ is matched with a neighbor in $R$, and since it is a matching, no two such vertices are matched with the same vertex in $R$. Thus, there are at least $|A|$ vertices in the neighborhood of $A$.\n\n\nNow suppose that $|A| \\le |N(A)|$ for all $A \\subseteq L$. Run Ford-Fulkerson on the corresponding flow network. The flow is increased by $1$ each time an augmenting path is found, so it will suffice to show that this happens $|L|$ times. Suppose the while loop has run fewer than $L$ times, but there is no augmenting path. Then fewer than $L$ edges from $L$ to $R$ have flow $1$. \n\n\nLet $v_1 \\in L$ be such that no edge from $v_1$ to a vertex in $R$ has nonzero flow. By assumption, $v_1$ has at least one neighbor $v_1' \\in R$. If any of $v_1$'s neighbors are connected to $t$ in $G_f$ then there is a path, so assume this is not the case. Thus, there must be some edge $(v_2, v_1)$ with flow $1$. By assumption, $N(\\{v_1, v_2\\}) \\ge 2$, so there must exist $v_2' \\ne v_1'$ such that $v_2'\\in N(\\{v_1, v_2 \\})$. If $(v_2', t)$ is an edge in the residual network we're done since $v_2'$ must be a neighbor of $v_2$, so $s$, $v_1$, $v_1'$, $v_2$, $v_2'$, and $t$ is a path in $G_f$. Otherwise $v_2'$ must have a neighbor $v_3 \\in L$ such that $(v_3, v_2')$ is in $G_f$. Specifically, $v_3 \\ne v_1$ since $(v_3, v_2')$ has flow $1$, and $v_3 \\ne v_2$ since $(v_2, v_1')$ has flow $1$, so no more flow can leave $v_2$ without violating conservation of flow. Again by our hypothesis, $N(\\{v_1, v_2, v_3\\}) \\ge 3$, so there is another neighbor $v_3' \\in R$.\n\n\nContinuing in this fashion, we keep building up the neighborhood $v_i'$, expanding each time we find that $(v_i', t)$ is not an edge in $G_f$. This cannot happen $L$ times, since we have run the Ford-Fulkerson while-loop fewer than $|L|$ times, so there still exist edges into $t$ in $G_f$. Thus, the process must stop at some vertex $v_k'$, and we obtain an augmenting path \n\n\n$$s, v_1, v_1', v_2, v_2', v_3, \\ldots, v_k, v_k', t,$$\n\n\ncontradicting our assumption that there was no such path. Therefore the while loop runs at least $|L|$ times. By Corollary 26.3 the flow strictly increases each time by $f_p$. By Theorem 26.10 $f_p$ is an integer. In particular, it is equal to $1$. This implies that $|f| \\ge |L|$. It is clear that $|f| \\le |L|$, so we must have $|f| = |L|$. By Corollary 26.11 this is the cardinality of a maximum matching. Since $|L| = |R|$, any maximum matching must be a perfect matching.\n\n\n26.3-5 $\\star$\n\n\n\n\nWe say that a bipartite graph $G = (V, E)$, where $V = L \\cup R$, is \n$d$-regular\n if every vertex $v \\in V$ has degree exactly $d$. Every $d$-regular bipartite graph has $|L| = |R|$. Prove that every $d$-regular bipartite graph has a matching of cardinality $|L|$ by arguing that a minimum cut of the corresponding flow network has capacity $|L|$.\n\n\n\n\nWe convert the bipartite graph into a flow problem by making a new vertex for the source which has an edge of unit capacity going to each of the vertices in $L$, and a new vertex for the sink that has an edge from each of the vertices in $R$, each with unit capacity. We want to show that the number of edge between the two parts of the cut is at least $L$, this would get us by the max-flow-min-cut theorem that there is a flow of value at least $|L|$. The, we can apply the integrality theorem that all of the flow values are integers, meaning that we are selecting $|L|$ disjoint edges between $L$ and $R$.\n\n\nTo see that every cut must have capacity at lest $|L|$, let $S_1$ be the side of the cut containing the source and let $S_2$ be the side of the cut containing the sink. Then, look at $L \\cap S_1$. The source has an edge going to each of $L \\cap (S_1)^c$, and there is an edge from $R \\cap S_1$ to the sink that will be cut. This means that we need that there are at least $|L \\cap S_1| - |R \\cap S_1|$ many edges going from $L \\cap S_1$ to $R \\cap S_2$. If we look at the set of all neighbors of $L \\cap S_1$, we get that there must be at least the same number of neighbors in $R$, because otherwise we could sum up the degrees going from $L \\cap S_1$ to $R$ on both sides, and get that some of the vertices in $R$ would need to have a degree higher than $d$. This means that the number of neighbors of $L \\cap S_1$ is at least $L \\cap S_1$, but we have that they are in $S_1$, but there are only $|R \\cap S_1|$ of those, so, we have that the size of the set of neighbors of $L \\cap S_1$ that are in $S_2$ is at least $|L \\cap S_1| - |R \\cap S_1|$. Since each of these neighbors has an edge crossing the cut, we have that the total number of edges that the cut breaks is at least \n\n\n$$(|L| - |L \\cap S_1|) + (|L \\cap S_1| - |R \\cap S_1|) + |R \\cap S_1| = |L|.$$\n\n\nSince each of these edges is unit valued, the value of the cut is at least $|L|$.",
            "title": "26.3 Maximum bipartite matching"
        },
        {
            "location": "/Chap26/26.3/#263-1",
            "text": "Run the Ford-Fulkerson algorithm on the flow network in Figure 26.8(c) and show the residual network after each flow augmentation. Number the vertices in $L$ top to bottom from 1 to 5 and in $R$ top to bottom from 6 to 9. For each iteration, pick the augmenting path that is lexicographically smallest.   First, we pick an augmenting path that passes through vertices 1 and 6. Then, we pick the path going through 2 and 8. Then, we pick the path going through 3 and 7. Then, the resulting residual graph has no path from $s$ to $t$. So, we know that we are done, and that we are pairing up vertices $(1, 6)$, $(2, 8)$, and $(3, 7)$. This number of unit augmenting paths agrees with the value of the cut where you cut the edges $(s, 3)$, $(6, t)$, and $(7, t)$.",
            "title": "26.3-1"
        },
        {
            "location": "/Chap26/26.3/#263-2",
            "text": "Prove Theorem 26.10.   We proceed by induction on the number of iterations of the while loop of Ford-Fulkerson. After the first iteration, since $c$ only takes on integer values and $(u, v).f$ is set to $0$, $c_f$ only takes on integer values. Thus, lines 7 and 8 of Ford-Fulkerson only assign integer values to $(u, v).f$. Assume that $(u, v).f \\in \\mathbb Z$ for all $(u, v)$ after the $n$th iteration. On the $(n + 1)$th iteration $c_f(p)$ is set to the minimum of $c_f(u, v)$ which is an integer by the induction hypothesis. Lines 7 and 8 compute $(u, v).f$ or $(v, u).f$. Either way, these the the sum or difference of integers by assumption, so after the $(n + 1)$th iteration we have that $(u, v).f$ is an integer for all $(u, v) \\in E$. Since the value of the flow is a sum of flows of edges, we must have $|f| \\in \\mathbb Z$ as well.",
            "title": "26.3-2"
        },
        {
            "location": "/Chap26/26.3/#263-3",
            "text": "Let $G = (V, E)$ be a bipartite graph with vertex partition $V = L \\cup R$, and let $G'$ be its corresponding flow network. Give a good upper bound on the length of any augmenting path found in $G'$ during the execution of $\\text{FORD-FULKERSON}$.   By definition, an augmenting path is a simple path $s \\leadsto t$ in the residual network $G_f'$. Since $G$ has no edges between vertices in $L$ and no edges between vertices in $R$, neither does the flow network $G'$ and hence neither does $G_f'$. Also, the only edges involving $s$ or $t$ connect $s$ to $L$ and $R$ to $t$. Note that although edges in $G'$ can go only from $L$ to $R$, edges in $G_f'$ can also go from $R$ to $L$.  Thus any augmenting path must go  $$s \\to L \\to R \\to \\cdots \\to L \\to R \\to t,$$  crossing back and forth between $L$ and $R$ at most as many times as it can do so without using a vertex twice. It contains $s$, $t$, and equal numbers of distinct vertices from $L$ and $R$\u2014at most $2 + 2 \\cdot \\min(|L|, |R|)$ vertices in all. The length of an augmenting path (i.e., its number of edges) is thus bounded above by $2 \\cdot \\min(|L|, |R|) + 1$.",
            "title": "26.3-3"
        },
        {
            "location": "/Chap26/26.3/#263-4-star",
            "text": "A  perfect matching  is a matching in which every vertex is matched. Let $G = (V, E)$ be an undirected bipartite graph with vertex partition $V = L \\cup R$, where $|L| = |R|$. For any $X \\subseteq V$, define the  neighborhood  of $X$ as  $N(X) = \\{y \\in V: (x, y) \\in E \\text{ for some } x \\in X\\}$,  that is, the set of vertices adjacent to some member of $X$. Prove  Hall's theorem : there exists a perfect matching in $G$ if and only if $|A| \\le |N(A)|$ for every subset $A \\subseteq L$.   First suppose there exists a perfect matching in $G$. Then for any subset $A \\subseteq L$, each vertex of $A$ is matched with a neighbor in $R$, and since it is a matching, no two such vertices are matched with the same vertex in $R$. Thus, there are at least $|A|$ vertices in the neighborhood of $A$.  Now suppose that $|A| \\le |N(A)|$ for all $A \\subseteq L$. Run Ford-Fulkerson on the corresponding flow network. The flow is increased by $1$ each time an augmenting path is found, so it will suffice to show that this happens $|L|$ times. Suppose the while loop has run fewer than $L$ times, but there is no augmenting path. Then fewer than $L$ edges from $L$ to $R$ have flow $1$.   Let $v_1 \\in L$ be such that no edge from $v_1$ to a vertex in $R$ has nonzero flow. By assumption, $v_1$ has at least one neighbor $v_1' \\in R$. If any of $v_1$'s neighbors are connected to $t$ in $G_f$ then there is a path, so assume this is not the case. Thus, there must be some edge $(v_2, v_1)$ with flow $1$. By assumption, $N(\\{v_1, v_2\\}) \\ge 2$, so there must exist $v_2' \\ne v_1'$ such that $v_2'\\in N(\\{v_1, v_2 \\})$. If $(v_2', t)$ is an edge in the residual network we're done since $v_2'$ must be a neighbor of $v_2$, so $s$, $v_1$, $v_1'$, $v_2$, $v_2'$, and $t$ is a path in $G_f$. Otherwise $v_2'$ must have a neighbor $v_3 \\in L$ such that $(v_3, v_2')$ is in $G_f$. Specifically, $v_3 \\ne v_1$ since $(v_3, v_2')$ has flow $1$, and $v_3 \\ne v_2$ since $(v_2, v_1')$ has flow $1$, so no more flow can leave $v_2$ without violating conservation of flow. Again by our hypothesis, $N(\\{v_1, v_2, v_3\\}) \\ge 3$, so there is another neighbor $v_3' \\in R$.  Continuing in this fashion, we keep building up the neighborhood $v_i'$, expanding each time we find that $(v_i', t)$ is not an edge in $G_f$. This cannot happen $L$ times, since we have run the Ford-Fulkerson while-loop fewer than $|L|$ times, so there still exist edges into $t$ in $G_f$. Thus, the process must stop at some vertex $v_k'$, and we obtain an augmenting path   $$s, v_1, v_1', v_2, v_2', v_3, \\ldots, v_k, v_k', t,$$  contradicting our assumption that there was no such path. Therefore the while loop runs at least $|L|$ times. By Corollary 26.3 the flow strictly increases each time by $f_p$. By Theorem 26.10 $f_p$ is an integer. In particular, it is equal to $1$. This implies that $|f| \\ge |L|$. It is clear that $|f| \\le |L|$, so we must have $|f| = |L|$. By Corollary 26.11 this is the cardinality of a maximum matching. Since $|L| = |R|$, any maximum matching must be a perfect matching.",
            "title": "26.3-4 $\\star$"
        },
        {
            "location": "/Chap26/26.3/#263-5-star",
            "text": "We say that a bipartite graph $G = (V, E)$, where $V = L \\cup R$, is  $d$-regular  if every vertex $v \\in V$ has degree exactly $d$. Every $d$-regular bipartite graph has $|L| = |R|$. Prove that every $d$-regular bipartite graph has a matching of cardinality $|L|$ by arguing that a minimum cut of the corresponding flow network has capacity $|L|$.   We convert the bipartite graph into a flow problem by making a new vertex for the source which has an edge of unit capacity going to each of the vertices in $L$, and a new vertex for the sink that has an edge from each of the vertices in $R$, each with unit capacity. We want to show that the number of edge between the two parts of the cut is at least $L$, this would get us by the max-flow-min-cut theorem that there is a flow of value at least $|L|$. The, we can apply the integrality theorem that all of the flow values are integers, meaning that we are selecting $|L|$ disjoint edges between $L$ and $R$.  To see that every cut must have capacity at lest $|L|$, let $S_1$ be the side of the cut containing the source and let $S_2$ be the side of the cut containing the sink. Then, look at $L \\cap S_1$. The source has an edge going to each of $L \\cap (S_1)^c$, and there is an edge from $R \\cap S_1$ to the sink that will be cut. This means that we need that there are at least $|L \\cap S_1| - |R \\cap S_1|$ many edges going from $L \\cap S_1$ to $R \\cap S_2$. If we look at the set of all neighbors of $L \\cap S_1$, we get that there must be at least the same number of neighbors in $R$, because otherwise we could sum up the degrees going from $L \\cap S_1$ to $R$ on both sides, and get that some of the vertices in $R$ would need to have a degree higher than $d$. This means that the number of neighbors of $L \\cap S_1$ is at least $L \\cap S_1$, but we have that they are in $S_1$, but there are only $|R \\cap S_1|$ of those, so, we have that the size of the set of neighbors of $L \\cap S_1$ that are in $S_2$ is at least $|L \\cap S_1| - |R \\cap S_1|$. Since each of these neighbors has an edge crossing the cut, we have that the total number of edges that the cut breaks is at least   $$(|L| - |L \\cap S_1|) + (|L \\cap S_1| - |R \\cap S_1|) + |R \\cap S_1| = |L|.$$  Since each of these edges is unit valued, the value of the cut is at least $|L|$.",
            "title": "26.3-5 $\\star$"
        },
        {
            "location": "/Chap26/26.4/",
            "text": "26.4-1\n\n\n\n\nProve that, after the procedure $\\text{INITIALIZE-PREFLOW}(G, S)$ terminates, we have $s.e \\le -|f^*|$, where $f^*$ is a maximum flow for $G$.\n\n\n\n\nWe apply the definition of excess flow (equation $\\text{(26.14)}$) to the initial preflow $f$ created by $\\text{INITIALIZE-PREFLOW}$ (equation $\\text{(26.15)}$) to obtain\n\n\n\\begin{align}\ne(s) & = \\sum_{v \\in V} f(v, s) - \\sum_{v \\in V} f(s, v) \\\\\n     & = 0 - \\sum_{v \\in V} c(s, v) \\\\\n     & = -\\sum_{v \\in V} c(s, v).\n\\end{align}\n\n\nNow,\n\n\n\\begin{align}\n-|f^*| & =   \\sum_{v \\in V} f^*(v, s) - \\sum_{v \\in V} f^*(s, v) \\\\\n        & \\ge 0 - \\sum_{v \\in V} c(s, v) \\qquad \\text{(since $f^*(v, s) \\ge 0$ and $f^*(s, v) \\le c(s, v)$)} \\\\\n        & = e(s).\n\\end{align}\n\n\n26.4-2\n\n\n\n\nShow how to implement the generic push-relabel algorithm using $O(V)$ time per relabel operation, $O(1)$ time per push, and $O(1)$ time to select an applicable operation, for a total time of $O(V^2E)$.\n\n\n\n\nWe must select an appropriate data structure to store all the information which will allow us to select a valid operation in constant time. To do this, we will need to maintain a list of overflowing vertices. By Lemma 26.14, a push or a relabel operation always applies to an overflowing vertex. To determine which operation to perform, we need to determine whether $u.h = v.h + 1$ for some $v \\in N(u)$. We'll do this by maintaining a list $u.high$ of all neighbors of $u$ in $G_f$ which have height greater than or equal to $u$. We'll update these attributes in the $\\text{PUSH}$ and $\\text{RELABEL}$ functions. It is clear from the pseudocode given for $\\text{PUSH}$ that we can execute it in constant time, provided we have maintain the attributes $\\delta_f(u, v)$, $u.e$, $c_f(u, v)$, $(u, v).f$ and $u.h$. Each time we call $\\text{PUSH}(u, v)$ the result is that $u$ is no longer overflowing, so we must remove it from the list.\n\n\nMaintain a pointer $u.overflow$ to $u$'s position in the overflow list. If a vertex $u$ is not overflowing, set $u.overflow = \\text{NIL}$. Next, check if $v$ became overflowing. If so, set $v.overflow$ equal to the head of the overflow list. Since we can update the pointer in constant time and delete from a linked list given a pointer to the element to be deleted in constant time, we can maintain the list in $O(1)$.\n\n\nThe $\\text{RELABEL}$ operation takes $O(V)$ because we need to compute the minimum $v.h$ from among all $(u, v) \\in E_f$, and there could be $|V| - 1$ many such $v$. We will also need to update $u.high$ during $\\text{RELABEL}$. When $\\text{RELABEL}(u)$ is called, set $u.high$ equal to the empty list and for each vertex $v$ which is adjacent to $u$, if $v.h = u.h + 1$, add $u$ to the list $v.high$. Since this takes constant time per adjacent vertex we can maintain the attribute in $O(V)$ per call to relabel.\n\n\n26.4-3\n\n\n\n\nProve that the generic push-relabel algorithm spends a total of only $O(VE)$ time in performing all the $O(V^2)$ relabel operations.\n\n\n\n\nEach time we call $\\text{RELABEL}(u)$, we examine all edges $(u, v) \\in E_f$. Since the number of relabel operations is at most $2|V| - 1$ per vertex, edge $(u, v)$ will be examined during relabel operations at most $4|V| - 2 = O(V)$ times (at most $2|V| - 1$ times during calls to $\\text{RELABEL}(u)$ and at most $2|V| - 1$ times during calls to $\\text{RELABEL}(v)$). Summing up over all the possible residual edges, of which there are at most $2|E| = O(E)$, we see that the total time spent relabeling vertices is $O(VE)$.\n\n\n26.4-4\n\n\n\n\nSuppose that we have found a maximum flow in a flow network $G = (V, E)$ using a push-relabel algorithm. Give a fast algorithm to find a minimum cut in $G$.\n\n\n\n\nWe can find a minimum cut, given a maximum flow found in $G = (V, E)$ by a push-relabel algorithm, in $O(V)$ time. First, find a height $\\hat h$ such that $0 < \\hat h < |V|$ and there is no vertex whose height equals $\\hat h$ at termination of the algorithm. We need consider only $|V| - 2$ vertices, since $s.h = |V|$ and $t.h = 0$. Because $\\hat h$ can be one of at most $|V| - 1$ possible values, we know that for at least one number in $1, 2, \\ldots, |V| - 1$, there will be no vertex of that height. Hence, $\\hat h$ is well defined, and it is easy to find in $O(V)$ time by using a simple boolean array indexed by heights $1, 2, \\ldots, |V| - 1$.\n\n\nLet $S = {u \\in V: u.h > \\hat h}$ and $T = {v \\in V: v.h < \\hat h}$. Because we know that $s.h = |V| > \\hat h$, we have $s \\in S$, and because $t.h = 0 < \\hat h$, y we have $t \\in T$, as required for a cut.\n\n\nWe need to show that $f(u, v) = c(u, v)$, i.e., that $(u, v) \\notin E_f$, for all $u \\in S$ and $v \\in T$. Once we do that, we have that $f(S, T) = c(S, T)$, and by Corollary 26.5, $(S, T)$ is a minimum cut.\n\n\nSuppose for the purpose of contradiction that there exist vertices $u \\in S$ and $v \\in T$ such that $(u, v) \\in E_f$. Because $h$ is always maintained as a height function (Lemma 26.16), we have that $u.h \\le v.h + 1$. But we also have $v.h < \\hat h < u.h$, and because all values are integer, $v.h \\le u.h - 2$. Thus, we have $u.h \\le v.h + 1 \\le u.h - 2 + 1 = u.h - 1$, which gives the contradiction that $u.height \\le u.height - 1$. Thus, $(S, T)$ is a minimum cut.\n\n\n26.4-5\n\n\n\n\nGive an efficient push-relabel algorithm to find a maximum matching in a bipartite graph. Analyze your algorithm.\n\n\n\n\nFirst, construct the flow network for the bipartite graph as in the previous section. Then, we relabel everything in $L$. Then, we push from every vertex in $L$ to a vertex in $R$, so long as it is possible.\n\n\nKeeping track of those that vertices of $L$ that are still overflowing can be done by a simple bit vector. Then, we relabel everything in R and push to the last vertex. Once these operations have been done, The only possible valid operations are to relabel the vertices of $L$ that weren't able to find an edge that they could push their flow along, so could possibly have to get a push back from $R$ to $L$. This continues until there are no more operations to do. This takes time of $O(V(E + V))$.\n\n\n26.4-6\n\n\n\n\nSuppose that all edge capacities in a flow network $G = (V, E)$ are in the set ${1, 2, \\ldots, k}$. Analyze the running time of the generic push-relabel algorithm in terms of $|V|$, $|E|$, and $k$. ($\\textit{Hint:}$ How many times can each edge support a nonsaturating push before it becomes saturated?)\n\n\n\n\nThe number of relabel operations and saturating pushes is the same as before. An edge can handle at most $k$ nonsaturating pushes before it becomes saturated, so the number of nonsaturating pushes is at most $2k|V||E|$. Thus, the total number of basic operations is at most $2|V|^2 + 2|V||E| + 2k|V||E| = O(kVE)$.\n\n\n26.4-7\n\n\n\n\nShow that we could change line 6 of $\\text{INITIALIZE-PREFLOW}$ to \n\n\n \n6\n \ns\n.\nh\n \n=\n \n|\nG\n.\nV\n|\n \n-\n \n2\n \n\n\n\n\nwithout affecting the correctness or asymptotic performance of the generic pushrelabel algorithm.\n\n\n\n\nIf we set $s.h = |V| - 2$, we have to change our definition of a height function to allow $s.h = |V| - 2$, rather than $s.h = |V|$. The only change we need to make to the proof of correctness is to update the proof of Lemma 26.17. The original proof derives the contradiction that $s.h \\le k < |V|$, which is at odds with $s.h = |V|$. When $s.h = |V| - 2$, there is no contradiction.\n\n\nAs in the original proof, let us suppose that we have a simple augmenting path $\\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = s$ and $v_k = t$, so that $k < |V|$. How could $(s, v_1)$ be a residual edge? It had been saturated in $\\text{INITIALIZE-PREFLOW}$, which means that we had to have pushed some flow from $v_1$ to $s$. In order for that to have happened, we must have had $v_1.h = s.h + 1$. If we set $s.h = |V| - 2$, then $v_1.h$ was $|V| - 1$ at the time. Since then, $v_1.h$ did not decrease, and so we have $v_1.h \\ge |V| - 1$. Working backwards over our augmenting path, we have $v_{k - i}.h \\le t.h + i$ for $i = 0, 1, \\ldots, k$. As before, because the augmenting path is simple, $k < |V|$. Letting $i = k - 1$, we have $v_1.h \\le t.h + k - 1 < 0 + |V| - 1$. We now have the contradiction that $v_1.h \\ge |V| - 1$ and $v_1.h < |V| - 1$, which shows that Lemma 26.17 still holds.\n\n\nNothing in the analysis changes asymptotically.\n\n\n26.4-8\n\n\n\n\nLet $\\delta_f(u, v)$ be the distance (number of edges) from $u$ to $v$ in the residual network $G_f$. Show that the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the properties that $u.h < |V|$ implies $u.h \\le \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| \\le \\delta_f(u, s)$.\n\n\n\n\nWe'll prove the claim by induction on the number of push and relabel operations. Initially, we have $u.h = |V|$ if $u = s$ and $0$ otherwise. We have $s.h - |V| = 0 \\le \\delta_f(s, s) = 0$ and $u.h = 0 \\le \\delta_f(u, t)$ for all $u \\ne s$, so the claim holds prior to the first iteration of the while loop on line 2 of the $\\text{GENERIC-PUSH-RELABEL}$ algorithm. \n\n\nSuppose that the properties have been maintained thus far. If the next iteration is a nonsaturating push then the properties are maintained because the heights and existence of edges in the residual network are preserved. If it is a saturating push then edge $(u, v)$ is removed from the residual network, which increases both $\\delta_f(u, t)$ and $\\delta_f(u, s)$, so the properties are maintained regardless of the height of $u$. \n\n\nNow suppose that the next iteration causes a relabel of vertex $u$. For all $v$ such that $(u, v) \\in E_f$ we must have $u.h \\le v.h$. Let $v' = \\min{v.h \\mid (u,v) \\in E_f}$. There are two cases to consider. \n\n\n\n\n\n\nFirst, suppose that $v.h < |V|$. Then after relabeling we have \n\n\n$$u.h = 1 + v'.h \\le 1 + \\min_{(u, v)} \\in E_f \\delta_f(v, t) = \\delta_f(u, t).$$\n\n\n\n\n\n\nSecond, suppose that $v'.h \\ge |V|$. Then after relabeling we have\n\n\n$$u.h = 1 + v'.h \\le 1 + |V| + \\min_{(u, v)} \\in E_f \\delta_f(v, s) = \\delta_f(u, s) + |V|,$$\n\n\nwhich implies that $u.h - |V| \\le \\delta_f(u, s)$.\n\n\n\n\n\n\nTherefore, the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the desired properties.\n\n\n26.4-9 $\\star$\n\n\n\n\nAs in the previous exercise, let $\\delta_f(u, v)$ be the distance from $u$ to $v$ in the residual network $G_f$. Show how to modify the generic push-relabel algorithm to maintain the property that $u.h < |V|$ implies $u.h = \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| = \\delta_f(u, s)$. The total time that your implementation dedicates to maintaining this property should be $O(VE)$.\n\n\n\n\nWhat we should do is to, for successive backwards neighborhoods of $t$, relabel everything in that neighborhood. This will only take at most $O(VE)$ time (see 26.4-3). This also has the upshot of making it so that once we are done with it, every vertex's height is equal to the quantity $\\delta_f(u, t)$. Then, since we begin with equality, after doing this, the inductive step we had in the solution to the previous exercise shows that this equality is preserved.\n\n\n26.4-10\n\n\n\n\nShow that the number of nonsaturating pushes executed by the $\\text{GENERIC-PUSH-RELABEL}$ procedure on a flow network $G = (V, E)$ is at most $4|V|^2|E|$ for $|V| \\ge 4$.\n\n\n\n\nEach vertex has maximum height $2|V| - 1$. Since heights don't decrease, and there are $|V| - 2$ vertices which can be overflowing, the maximum contribution of relabels to $\\Phi$ over all vertices is $(2|V| - 1)(|V| - 2)$. A saturating push from $u$ to $v$ increases $\\Phi$ by at most $v.h \\le 2|V| - 1$, and there are at most $2|V||E|$ saturating pushes, so the total contribution over all saturating pushes to $\\Phi$ is at most $(2|V| - 1)(2|V||E|)$. Since each nonsaturating push decrements $\\Phi$ by at least on and $\\Phi$ must equal zero upon termination, we must have that the number of nonsaturating pushes is at most\n\n\n$$(2|V| - 1)(|V| - 2) + (2|V| - 1)(2|V||E|) = 4|V|^2|E| + 2|V|^2 - 5|V| + 3 - 2|V||E|.$$\n\n\nUsing the fact that $|E| \\ge |V| - 1$ and $|V| \\ge 4$ we can bound the number of\nsaturating pushes by $4|V|^2|E|$.",
            "title": "26.4 Push-relabel algorithms"
        },
        {
            "location": "/Chap26/26.4/#264-1",
            "text": "Prove that, after the procedure $\\text{INITIALIZE-PREFLOW}(G, S)$ terminates, we have $s.e \\le -|f^*|$, where $f^*$ is a maximum flow for $G$.   We apply the definition of excess flow (equation $\\text{(26.14)}$) to the initial preflow $f$ created by $\\text{INITIALIZE-PREFLOW}$ (equation $\\text{(26.15)}$) to obtain  \\begin{align}\ne(s) & = \\sum_{v \\in V} f(v, s) - \\sum_{v \\in V} f(s, v) \\\\\n     & = 0 - \\sum_{v \\in V} c(s, v) \\\\\n     & = -\\sum_{v \\in V} c(s, v).\n\\end{align}  Now,  \\begin{align}\n-|f^*| & =   \\sum_{v \\in V} f^*(v, s) - \\sum_{v \\in V} f^*(s, v) \\\\\n        & \\ge 0 - \\sum_{v \\in V} c(s, v) \\qquad \\text{(since $f^*(v, s) \\ge 0$ and $f^*(s, v) \\le c(s, v)$)} \\\\\n        & = e(s).\n\\end{align}",
            "title": "26.4-1"
        },
        {
            "location": "/Chap26/26.4/#264-2",
            "text": "Show how to implement the generic push-relabel algorithm using $O(V)$ time per relabel operation, $O(1)$ time per push, and $O(1)$ time to select an applicable operation, for a total time of $O(V^2E)$.   We must select an appropriate data structure to store all the information which will allow us to select a valid operation in constant time. To do this, we will need to maintain a list of overflowing vertices. By Lemma 26.14, a push or a relabel operation always applies to an overflowing vertex. To determine which operation to perform, we need to determine whether $u.h = v.h + 1$ for some $v \\in N(u)$. We'll do this by maintaining a list $u.high$ of all neighbors of $u$ in $G_f$ which have height greater than or equal to $u$. We'll update these attributes in the $\\text{PUSH}$ and $\\text{RELABEL}$ functions. It is clear from the pseudocode given for $\\text{PUSH}$ that we can execute it in constant time, provided we have maintain the attributes $\\delta_f(u, v)$, $u.e$, $c_f(u, v)$, $(u, v).f$ and $u.h$. Each time we call $\\text{PUSH}(u, v)$ the result is that $u$ is no longer overflowing, so we must remove it from the list.  Maintain a pointer $u.overflow$ to $u$'s position in the overflow list. If a vertex $u$ is not overflowing, set $u.overflow = \\text{NIL}$. Next, check if $v$ became overflowing. If so, set $v.overflow$ equal to the head of the overflow list. Since we can update the pointer in constant time and delete from a linked list given a pointer to the element to be deleted in constant time, we can maintain the list in $O(1)$.  The $\\text{RELABEL}$ operation takes $O(V)$ because we need to compute the minimum $v.h$ from among all $(u, v) \\in E_f$, and there could be $|V| - 1$ many such $v$. We will also need to update $u.high$ during $\\text{RELABEL}$. When $\\text{RELABEL}(u)$ is called, set $u.high$ equal to the empty list and for each vertex $v$ which is adjacent to $u$, if $v.h = u.h + 1$, add $u$ to the list $v.high$. Since this takes constant time per adjacent vertex we can maintain the attribute in $O(V)$ per call to relabel.",
            "title": "26.4-2"
        },
        {
            "location": "/Chap26/26.4/#264-3",
            "text": "Prove that the generic push-relabel algorithm spends a total of only $O(VE)$ time in performing all the $O(V^2)$ relabel operations.   Each time we call $\\text{RELABEL}(u)$, we examine all edges $(u, v) \\in E_f$. Since the number of relabel operations is at most $2|V| - 1$ per vertex, edge $(u, v)$ will be examined during relabel operations at most $4|V| - 2 = O(V)$ times (at most $2|V| - 1$ times during calls to $\\text{RELABEL}(u)$ and at most $2|V| - 1$ times during calls to $\\text{RELABEL}(v)$). Summing up over all the possible residual edges, of which there are at most $2|E| = O(E)$, we see that the total time spent relabeling vertices is $O(VE)$.",
            "title": "26.4-3"
        },
        {
            "location": "/Chap26/26.4/#264-4",
            "text": "Suppose that we have found a maximum flow in a flow network $G = (V, E)$ using a push-relabel algorithm. Give a fast algorithm to find a minimum cut in $G$.   We can find a minimum cut, given a maximum flow found in $G = (V, E)$ by a push-relabel algorithm, in $O(V)$ time. First, find a height $\\hat h$ such that $0 < \\hat h < |V|$ and there is no vertex whose height equals $\\hat h$ at termination of the algorithm. We need consider only $|V| - 2$ vertices, since $s.h = |V|$ and $t.h = 0$. Because $\\hat h$ can be one of at most $|V| - 1$ possible values, we know that for at least one number in $1, 2, \\ldots, |V| - 1$, there will be no vertex of that height. Hence, $\\hat h$ is well defined, and it is easy to find in $O(V)$ time by using a simple boolean array indexed by heights $1, 2, \\ldots, |V| - 1$.  Let $S = {u \\in V: u.h > \\hat h}$ and $T = {v \\in V: v.h < \\hat h}$. Because we know that $s.h = |V| > \\hat h$, we have $s \\in S$, and because $t.h = 0 < \\hat h$, y we have $t \\in T$, as required for a cut.  We need to show that $f(u, v) = c(u, v)$, i.e., that $(u, v) \\notin E_f$, for all $u \\in S$ and $v \\in T$. Once we do that, we have that $f(S, T) = c(S, T)$, and by Corollary 26.5, $(S, T)$ is a minimum cut.  Suppose for the purpose of contradiction that there exist vertices $u \\in S$ and $v \\in T$ such that $(u, v) \\in E_f$. Because $h$ is always maintained as a height function (Lemma 26.16), we have that $u.h \\le v.h + 1$. But we also have $v.h < \\hat h < u.h$, and because all values are integer, $v.h \\le u.h - 2$. Thus, we have $u.h \\le v.h + 1 \\le u.h - 2 + 1 = u.h - 1$, which gives the contradiction that $u.height \\le u.height - 1$. Thus, $(S, T)$ is a minimum cut.",
            "title": "26.4-4"
        },
        {
            "location": "/Chap26/26.4/#264-5",
            "text": "Give an efficient push-relabel algorithm to find a maximum matching in a bipartite graph. Analyze your algorithm.   First, construct the flow network for the bipartite graph as in the previous section. Then, we relabel everything in $L$. Then, we push from every vertex in $L$ to a vertex in $R$, so long as it is possible.  Keeping track of those that vertices of $L$ that are still overflowing can be done by a simple bit vector. Then, we relabel everything in R and push to the last vertex. Once these operations have been done, The only possible valid operations are to relabel the vertices of $L$ that weren't able to find an edge that they could push their flow along, so could possibly have to get a push back from $R$ to $L$. This continues until there are no more operations to do. This takes time of $O(V(E + V))$.",
            "title": "26.4-5"
        },
        {
            "location": "/Chap26/26.4/#264-6",
            "text": "Suppose that all edge capacities in a flow network $G = (V, E)$ are in the set ${1, 2, \\ldots, k}$. Analyze the running time of the generic push-relabel algorithm in terms of $|V|$, $|E|$, and $k$. ($\\textit{Hint:}$ How many times can each edge support a nonsaturating push before it becomes saturated?)   The number of relabel operations and saturating pushes is the same as before. An edge can handle at most $k$ nonsaturating pushes before it becomes saturated, so the number of nonsaturating pushes is at most $2k|V||E|$. Thus, the total number of basic operations is at most $2|V|^2 + 2|V||E| + 2k|V||E| = O(kVE)$.",
            "title": "26.4-6"
        },
        {
            "location": "/Chap26/26.4/#264-7",
            "text": "Show that we could change line 6 of $\\text{INITIALIZE-PREFLOW}$ to     6   s . h   =   | G . V |   -   2    without affecting the correctness or asymptotic performance of the generic pushrelabel algorithm.   If we set $s.h = |V| - 2$, we have to change our definition of a height function to allow $s.h = |V| - 2$, rather than $s.h = |V|$. The only change we need to make to the proof of correctness is to update the proof of Lemma 26.17. The original proof derives the contradiction that $s.h \\le k < |V|$, which is at odds with $s.h = |V|$. When $s.h = |V| - 2$, there is no contradiction.  As in the original proof, let us suppose that we have a simple augmenting path $\\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = s$ and $v_k = t$, so that $k < |V|$. How could $(s, v_1)$ be a residual edge? It had been saturated in $\\text{INITIALIZE-PREFLOW}$, which means that we had to have pushed some flow from $v_1$ to $s$. In order for that to have happened, we must have had $v_1.h = s.h + 1$. If we set $s.h = |V| - 2$, then $v_1.h$ was $|V| - 1$ at the time. Since then, $v_1.h$ did not decrease, and so we have $v_1.h \\ge |V| - 1$. Working backwards over our augmenting path, we have $v_{k - i}.h \\le t.h + i$ for $i = 0, 1, \\ldots, k$. As before, because the augmenting path is simple, $k < |V|$. Letting $i = k - 1$, we have $v_1.h \\le t.h + k - 1 < 0 + |V| - 1$. We now have the contradiction that $v_1.h \\ge |V| - 1$ and $v_1.h < |V| - 1$, which shows that Lemma 26.17 still holds.  Nothing in the analysis changes asymptotically.",
            "title": "26.4-7"
        },
        {
            "location": "/Chap26/26.4/#264-8",
            "text": "Let $\\delta_f(u, v)$ be the distance (number of edges) from $u$ to $v$ in the residual network $G_f$. Show that the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the properties that $u.h < |V|$ implies $u.h \\le \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| \\le \\delta_f(u, s)$.   We'll prove the claim by induction on the number of push and relabel operations. Initially, we have $u.h = |V|$ if $u = s$ and $0$ otherwise. We have $s.h - |V| = 0 \\le \\delta_f(s, s) = 0$ and $u.h = 0 \\le \\delta_f(u, t)$ for all $u \\ne s$, so the claim holds prior to the first iteration of the while loop on line 2 of the $\\text{GENERIC-PUSH-RELABEL}$ algorithm.   Suppose that the properties have been maintained thus far. If the next iteration is a nonsaturating push then the properties are maintained because the heights and existence of edges in the residual network are preserved. If it is a saturating push then edge $(u, v)$ is removed from the residual network, which increases both $\\delta_f(u, t)$ and $\\delta_f(u, s)$, so the properties are maintained regardless of the height of $u$.   Now suppose that the next iteration causes a relabel of vertex $u$. For all $v$ such that $(u, v) \\in E_f$ we must have $u.h \\le v.h$. Let $v' = \\min{v.h \\mid (u,v) \\in E_f}$. There are two cases to consider.     First, suppose that $v.h < |V|$. Then after relabeling we have   $$u.h = 1 + v'.h \\le 1 + \\min_{(u, v)} \\in E_f \\delta_f(v, t) = \\delta_f(u, t).$$    Second, suppose that $v'.h \\ge |V|$. Then after relabeling we have  $$u.h = 1 + v'.h \\le 1 + |V| + \\min_{(u, v)} \\in E_f \\delta_f(v, s) = \\delta_f(u, s) + |V|,$$  which implies that $u.h - |V| \\le \\delta_f(u, s)$.    Therefore, the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the desired properties.",
            "title": "26.4-8"
        },
        {
            "location": "/Chap26/26.4/#264-9-star",
            "text": "As in the previous exercise, let $\\delta_f(u, v)$ be the distance from $u$ to $v$ in the residual network $G_f$. Show how to modify the generic push-relabel algorithm to maintain the property that $u.h < |V|$ implies $u.h = \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| = \\delta_f(u, s)$. The total time that your implementation dedicates to maintaining this property should be $O(VE)$.   What we should do is to, for successive backwards neighborhoods of $t$, relabel everything in that neighborhood. This will only take at most $O(VE)$ time (see 26.4-3). This also has the upshot of making it so that once we are done with it, every vertex's height is equal to the quantity $\\delta_f(u, t)$. Then, since we begin with equality, after doing this, the inductive step we had in the solution to the previous exercise shows that this equality is preserved.",
            "title": "26.4-9 $\\star$"
        },
        {
            "location": "/Chap26/26.4/#264-10",
            "text": "Show that the number of nonsaturating pushes executed by the $\\text{GENERIC-PUSH-RELABEL}$ procedure on a flow network $G = (V, E)$ is at most $4|V|^2|E|$ for $|V| \\ge 4$.   Each vertex has maximum height $2|V| - 1$. Since heights don't decrease, and there are $|V| - 2$ vertices which can be overflowing, the maximum contribution of relabels to $\\Phi$ over all vertices is $(2|V| - 1)(|V| - 2)$. A saturating push from $u$ to $v$ increases $\\Phi$ by at most $v.h \\le 2|V| - 1$, and there are at most $2|V||E|$ saturating pushes, so the total contribution over all saturating pushes to $\\Phi$ is at most $(2|V| - 1)(2|V||E|)$. Since each nonsaturating push decrements $\\Phi$ by at least on and $\\Phi$ must equal zero upon termination, we must have that the number of nonsaturating pushes is at most  $$(2|V| - 1)(|V| - 2) + (2|V| - 1)(2|V||E|) = 4|V|^2|E| + 2|V|^2 - 5|V| + 3 - 2|V||E|.$$  Using the fact that $|E| \\ge |V| - 1$ and $|V| \\ge 4$ we can bound the number of\nsaturating pushes by $4|V|^2|E|$.",
            "title": "26.4-10"
        },
        {
            "location": "/Chap26/26.5/",
            "text": "26.5-1\n\n\n\n\nIllustrate the execution of $\\text{RELABEL-TO-FRONT}$ in the manner of Figure 26.10 for the flow network in Figure 26.1(a). Assume that the initial ordering of vertices in $L$ is $\\langle v_1, v_2, v_3, v_4 \\rangle$ and that the neighbor lists are\n\n\n\\begin{align}\nv_1.N & = \\langle s, v_2, v_3 \\rangle, \\\\\nv_2.N & = \\langle s, v_1, v_3, v_4 \\rangle, \\\\\nv_3.N & = \\langle v_1, v_2, v_4, t \\rangle, \\\\\nv_4.N & = \\langle v_2, v_3, t \\rangle.\n\\end{align}\n\n\n\n\nWhen we initialize the preflow, we have $26$ units of flow leaving $s$. Then, we consider $v_1$ since it is the first element in the $L$ list. When we discharge it, we increase it's height to $1$ so that it can dump $12$ of it's excess along its edge to vertex $v_3$, to discharge the rest of it, it has to increase it's height to $|V| + 1$ to discharge it back to $s$. It was already at the front, so, we consider $v_2$. We increase its height to $1$. Then, we send all of its excess along its edge to $v_4$. We move it to the front, which means we next consider $v_1$, and do nothing because it is not overflowing. Up next is vertex $v_3$. After increasing its height to $1$, it can send all of its excess to $t$. This puts $v_3$ at the front, and we consider the non-overflowing vertices $v_2$ and $v_1$. Then, we consider $v_4$, it increases its height to $1$, then sends $4$ units to $t$. Since it still has an excess of $10$ units, it increases its height once again. Then it becomes valid for it to send flow back to $v_2$ or to $v_3$. It considers $v_4$ first because of the ordering of its neighbor list. This means that $10$ units of flow are pushed back to $v_2$. Since $v_4.h$ increased, it moves to the front of the list Then, we consider $v_2$ since it is the only still overflowing vertex. We increase its height to $3$. Then, it is overflowing by $10$ so it increases its height to $3$ to send $6$ units to $v_4$. It's height increased so it goes to the  of the list. Then, we consider $v_4$, which is overflowing. it increases its height to $3$, then it sends $6$ units to $v_3$. Again, it goes to the front of the list. Up next is $v_2$ which is not overflowing, $v_3$ which is, so it increases it's height by $1$ to send $4$ units of flow to $t$. Then sends $2$ units to $v_4$ after increasing in height. The excess flow keeps bobbing around the four vertices, each time requiring them to increase their height a bit to discharge to a neighbor only to have that neighbor increase to discharge it back until $v_2$ has increased in height enough to send all of it's excess back to s, this completes and gives us a maximum flow of $23$.\n\n\n26.5-2 $\\star$\n\n\n\n\nWe would like to implement a push-relabel algorithm in which we maintain a firstin, first-out queue of overflowing vertices. The algorithm repeatedly discharges the vertex at the head of the queue, and any vertices that were not overflowing before the discharge but are overflowing afterward are placed at the end of the queue. After the vertex at the head of the queue is discharged, it is removed. When the queue is empty, the algorithm terminates. Show how to implement this algorithm to compute a maximum flow in $O(V^3)$ time.\n\n\n\n\nInitially, the vertices adjacent to $s$ are the only ones which are overflowing. The implementation is as follows:\n\n\nPUSH\n-\nRELABEL\n-\nQUEUE\n(\nG\n,\n \ns\n)\n\n    \nINITIALIZE\n-\nPREFLOW\n(\nG\n,\n \ns\n)\n\n    \nInitialize\n \nan\n \nempty\n \nqueue\n \nq\n\n    \nfor\n \nv\n \n\u2208\n \nG\n.\nAdj\n[\ns\n]\n\n        \nq\n.\npush\n(\nv\n)\n\n    \nwhile\n \nq\n.\nhead\n \n!=\n \nNIL\n\n        \nDISCHARGE\n(\nq\n.\nhead\n)\n\n        \nq\n.\npop\n()\n\n\n\n\n\nNote that we need to modify the $\\text{DISCHARGE}$ algorithm to push vertices $v$ onto the queue if $v$ was not overflowing before a discharge but is overflowing after one.\n\n\nBetween lines 7 and 8 of $\\text{DISCHARGE}(u)$, add the line ''if $v.e > 0$, $q.push(v)$.'' This is an implementation of the generic push-relabel algorithm, so we know it is correct. The analysis of runtime is almost identical to that of Theorem 26.30. We just need to verify that there are at most $|V|$ calls to $\\text{DISCHARGE}$ between two consecutive relabel operations. Observe that after calling $\\text{PUSH}(u, v)$, Corollary 26.28 tells us that no admissible edges are entering $v$. Thus, once $v$ is put into the queue because of the push, it can't be added again until it has been relabeled. Thus, at most $|V|$ vertices are added to the queue between relabel operations.\n\n\n26.5-3\n\n\n\n\nShow that the generic algorithm still works if $\\text{RELABEL}$ updates $u.h$ by simply computing $u.h = u.h + 1$. How would this change affect the analysis of $\\text{RELABEL-TO-FRONT}$?\n\n\n\n\nIf we change relabel to just increment the value of $u$, we will not be ruining the correctness of the Algorithm. This is because since it only applies when $u.h \\le v.h$, we won't be every creating a graph where $h$ ceases to be a height function, since $u.h$ will only ever be increasing by exactly $1$ whenever relabel is called, ensuring that $u.h + 1 \\le v.h$. This means that Lemmatae 26.15 and 26.16 will still hold. Even Corollary 26.21 holds since all it counts on is that relabel causes some vertex's $h$ value to increase by at least $1$, it will still work when we have all of the operations causing it to increase by exactly $1$. However, Lemma 26.28 will no longer hold. That is, it may require more than a single relabel operation to cause an admissible edge to appear, if for example, $u.h$ was strictly less than the $h$ values of all its neighbors. However, this lemma is not used in the proof of Exercise 26.4-3, which bounds the number of relabel operations. Since the number of relabel operations still have the same bound, and we know that we can simulate the old relabel operation by doing (possibly many) of these new relabel operations, we have the same bound as in the original algorithm with this different relabel operation.\n\n\n26.5-4 $\\star$\n\n\n\n\nShow that if we always discharge a highest overflowing vertex, we can make the\n\n\n\n\nWe'll keep track of the heights of the overflowing vertices using an array and a series of doubly linked lists. In particular, let $A$ be an array of size $|V|$, and let $A[i]$ store a list of the elements of height $i$. Now we create another list $L$, which is a list of lists. The head points to the list containing the vertices of highest height. The next pointer of this list points to the next nonempty list stored in $A$, and so on. This allows for constant time insertion of a vertex into $A$, and also constant time access to an element of largest height, and because all lists are doubly linked, we can add and delete elements in constant time. Essentially, we are implementing the algorithm of Exercise 26.5-2, but with the queue replaced by a priority queue with constant time operations. As before, it will suffice to show that there are at most $|V|$ calls to discharge between consecutive relabel operations.\n\n\nConsider what happens when a vertex $v$ is put into the priority queue. There must exist a vertex $u$ for which we have called $\\text{PUSH}(u, v)$. After this, no ad- missible edge is entering $v$, so it can't be added to the priority queue again until after a relabel operation has occurred on $v$. Moreover, every call to $\\text{DISCHARGE}$ terminates with a $\\text{PUSH}$, so for every call to $\\text{DISCHARGE}$ there is another vertex which can't be added until a relabel operation occurs. After $|V|$ $\\text{DISCHARGE}$ operations and no relabel operations, there are no remaining valid $\\text{PUSH}$ operations, so either the algorithm terminates, or there is a valid relabel operation which is performed. Thus, there are $O(V^3)$ calls to $\\text{DISCHARGE}$. By carrying out the rest of the analysis of Theorem 26.30, we conclude that the runtime is $O(V^3)$.\n\n\n26.5-5\n\n\n\n\nSuppose that at some point in the execution of a push-relabel algorithm, there exists an integer $0 < k \\le |V| - 1$ for which no vertex has $v.h = k$. Show that all vertices with $v.h > k$ are on the source side of a minimum cut. If such a $k$ exists, the \ngap heuristic\n updates every vertex $v \\in V - {s}$ for which $v.h > k$, to set $v.h = \\max(v.h, |V| + 1)$. Show that the resulting attribute $h$ is a height function. (The gap heuristic is crucial in making implementations of the push-relabel method perform well in practice.)\n\n\n\n\nSuppose to try and obtain a contradiction that there were some minimum cut for which a vertex that had $v.h > k$ were on the sink side of that cut. For that minimum cut, there is a residual flow network for which that cut is saturated. Then, if there were any vertices that were also on the sink side of the cut which had an edge going to $v$ in this residual flow network, since it's $h$ value cannot be equal to $k$, we know that it must be greater than $k$ since it could be only at most one less than $v$. We can continue in this way to let $S$ be the set of vertices on the sink side of the graph which have an $h$ value greater than $k$. Suppose that there were some simple path from a vertex in $S$ to $s$. Then, at each of these steps, the height could only decrease by at most $1$, since it cannot get from above $k$ to $0$ without going through $k$, we know that there is no path in the residual flow network going from a vertex in $S$ to $s$. Since a minimal cut corresponds to disconnected parts of the residual graph for a maximum flow, and we know there is no path from $S$ to $s$, there is a minimum cut for which $S$ lies entirely on the source side of the cut. This was a contradiction to how we selected $v$, and so have shown the first claim.\n\n\nNow we show that after updating the $h$ values as suggested, we are still left with a height function. Suppose we had an edge $(u, v)$ in the residual graph. We knew from before that $u.h \\le v.h + 1$. However, this means that if $u.h > k$, so must be $v.h$. So, if both were above $k$, we would be making them equal, causing the inequality to still hold. Also, if just $v.k$ were above $k$, then we have not decreased it's $h$ value, meaning that the inequality also still must hold. Since we have not changed the value of $s.h$, and $t.h$, we have all the required properties to have a height function after modifying the $h$ values as described.",
            "title": "26.5 The relabel-to-front algorithm"
        },
        {
            "location": "/Chap26/26.5/#265-1",
            "text": "Illustrate the execution of $\\text{RELABEL-TO-FRONT}$ in the manner of Figure 26.10 for the flow network in Figure 26.1(a). Assume that the initial ordering of vertices in $L$ is $\\langle v_1, v_2, v_3, v_4 \\rangle$ and that the neighbor lists are  \\begin{align}\nv_1.N & = \\langle s, v_2, v_3 \\rangle, \\\\\nv_2.N & = \\langle s, v_1, v_3, v_4 \\rangle, \\\\\nv_3.N & = \\langle v_1, v_2, v_4, t \\rangle, \\\\\nv_4.N & = \\langle v_2, v_3, t \\rangle.\n\\end{align}   When we initialize the preflow, we have $26$ units of flow leaving $s$. Then, we consider $v_1$ since it is the first element in the $L$ list. When we discharge it, we increase it's height to $1$ so that it can dump $12$ of it's excess along its edge to vertex $v_3$, to discharge the rest of it, it has to increase it's height to $|V| + 1$ to discharge it back to $s$. It was already at the front, so, we consider $v_2$. We increase its height to $1$. Then, we send all of its excess along its edge to $v_4$. We move it to the front, which means we next consider $v_1$, and do nothing because it is not overflowing. Up next is vertex $v_3$. After increasing its height to $1$, it can send all of its excess to $t$. This puts $v_3$ at the front, and we consider the non-overflowing vertices $v_2$ and $v_1$. Then, we consider $v_4$, it increases its height to $1$, then sends $4$ units to $t$. Since it still has an excess of $10$ units, it increases its height once again. Then it becomes valid for it to send flow back to $v_2$ or to $v_3$. It considers $v_4$ first because of the ordering of its neighbor list. This means that $10$ units of flow are pushed back to $v_2$. Since $v_4.h$ increased, it moves to the front of the list Then, we consider $v_2$ since it is the only still overflowing vertex. We increase its height to $3$. Then, it is overflowing by $10$ so it increases its height to $3$ to send $6$ units to $v_4$. It's height increased so it goes to the  of the list. Then, we consider $v_4$, which is overflowing. it increases its height to $3$, then it sends $6$ units to $v_3$. Again, it goes to the front of the list. Up next is $v_2$ which is not overflowing, $v_3$ which is, so it increases it's height by $1$ to send $4$ units of flow to $t$. Then sends $2$ units to $v_4$ after increasing in height. The excess flow keeps bobbing around the four vertices, each time requiring them to increase their height a bit to discharge to a neighbor only to have that neighbor increase to discharge it back until $v_2$ has increased in height enough to send all of it's excess back to s, this completes and gives us a maximum flow of $23$.",
            "title": "26.5-1"
        },
        {
            "location": "/Chap26/26.5/#265-2-star",
            "text": "We would like to implement a push-relabel algorithm in which we maintain a firstin, first-out queue of overflowing vertices. The algorithm repeatedly discharges the vertex at the head of the queue, and any vertices that were not overflowing before the discharge but are overflowing afterward are placed at the end of the queue. After the vertex at the head of the queue is discharged, it is removed. When the queue is empty, the algorithm terminates. Show how to implement this algorithm to compute a maximum flow in $O(V^3)$ time.   Initially, the vertices adjacent to $s$ are the only ones which are overflowing. The implementation is as follows:  PUSH - RELABEL - QUEUE ( G ,   s ) \n     INITIALIZE - PREFLOW ( G ,   s ) \n     Initialize   an   empty   queue   q \n     for   v   \u2208   G . Adj [ s ] \n         q . push ( v ) \n     while   q . head   !=   NIL \n         DISCHARGE ( q . head ) \n         q . pop ()   Note that we need to modify the $\\text{DISCHARGE}$ algorithm to push vertices $v$ onto the queue if $v$ was not overflowing before a discharge but is overflowing after one.  Between lines 7 and 8 of $\\text{DISCHARGE}(u)$, add the line ''if $v.e > 0$, $q.push(v)$.'' This is an implementation of the generic push-relabel algorithm, so we know it is correct. The analysis of runtime is almost identical to that of Theorem 26.30. We just need to verify that there are at most $|V|$ calls to $\\text{DISCHARGE}$ between two consecutive relabel operations. Observe that after calling $\\text{PUSH}(u, v)$, Corollary 26.28 tells us that no admissible edges are entering $v$. Thus, once $v$ is put into the queue because of the push, it can't be added again until it has been relabeled. Thus, at most $|V|$ vertices are added to the queue between relabel operations.",
            "title": "26.5-2 $\\star$"
        },
        {
            "location": "/Chap26/26.5/#265-3",
            "text": "Show that the generic algorithm still works if $\\text{RELABEL}$ updates $u.h$ by simply computing $u.h = u.h + 1$. How would this change affect the analysis of $\\text{RELABEL-TO-FRONT}$?   If we change relabel to just increment the value of $u$, we will not be ruining the correctness of the Algorithm. This is because since it only applies when $u.h \\le v.h$, we won't be every creating a graph where $h$ ceases to be a height function, since $u.h$ will only ever be increasing by exactly $1$ whenever relabel is called, ensuring that $u.h + 1 \\le v.h$. This means that Lemmatae 26.15 and 26.16 will still hold. Even Corollary 26.21 holds since all it counts on is that relabel causes some vertex's $h$ value to increase by at least $1$, it will still work when we have all of the operations causing it to increase by exactly $1$. However, Lemma 26.28 will no longer hold. That is, it may require more than a single relabel operation to cause an admissible edge to appear, if for example, $u.h$ was strictly less than the $h$ values of all its neighbors. However, this lemma is not used in the proof of Exercise 26.4-3, which bounds the number of relabel operations. Since the number of relabel operations still have the same bound, and we know that we can simulate the old relabel operation by doing (possibly many) of these new relabel operations, we have the same bound as in the original algorithm with this different relabel operation.",
            "title": "26.5-3"
        },
        {
            "location": "/Chap26/26.5/#265-4-star",
            "text": "Show that if we always discharge a highest overflowing vertex, we can make the   We'll keep track of the heights of the overflowing vertices using an array and a series of doubly linked lists. In particular, let $A$ be an array of size $|V|$, and let $A[i]$ store a list of the elements of height $i$. Now we create another list $L$, which is a list of lists. The head points to the list containing the vertices of highest height. The next pointer of this list points to the next nonempty list stored in $A$, and so on. This allows for constant time insertion of a vertex into $A$, and also constant time access to an element of largest height, and because all lists are doubly linked, we can add and delete elements in constant time. Essentially, we are implementing the algorithm of Exercise 26.5-2, but with the queue replaced by a priority queue with constant time operations. As before, it will suffice to show that there are at most $|V|$ calls to discharge between consecutive relabel operations.  Consider what happens when a vertex $v$ is put into the priority queue. There must exist a vertex $u$ for which we have called $\\text{PUSH}(u, v)$. After this, no ad- missible edge is entering $v$, so it can't be added to the priority queue again until after a relabel operation has occurred on $v$. Moreover, every call to $\\text{DISCHARGE}$ terminates with a $\\text{PUSH}$, so for every call to $\\text{DISCHARGE}$ there is another vertex which can't be added until a relabel operation occurs. After $|V|$ $\\text{DISCHARGE}$ operations and no relabel operations, there are no remaining valid $\\text{PUSH}$ operations, so either the algorithm terminates, or there is a valid relabel operation which is performed. Thus, there are $O(V^3)$ calls to $\\text{DISCHARGE}$. By carrying out the rest of the analysis of Theorem 26.30, we conclude that the runtime is $O(V^3)$.",
            "title": "26.5-4 $\\star$"
        },
        {
            "location": "/Chap26/26.5/#265-5",
            "text": "Suppose that at some point in the execution of a push-relabel algorithm, there exists an integer $0 < k \\le |V| - 1$ for which no vertex has $v.h = k$. Show that all vertices with $v.h > k$ are on the source side of a minimum cut. If such a $k$ exists, the  gap heuristic  updates every vertex $v \\in V - {s}$ for which $v.h > k$, to set $v.h = \\max(v.h, |V| + 1)$. Show that the resulting attribute $h$ is a height function. (The gap heuristic is crucial in making implementations of the push-relabel method perform well in practice.)   Suppose to try and obtain a contradiction that there were some minimum cut for which a vertex that had $v.h > k$ were on the sink side of that cut. For that minimum cut, there is a residual flow network for which that cut is saturated. Then, if there were any vertices that were also on the sink side of the cut which had an edge going to $v$ in this residual flow network, since it's $h$ value cannot be equal to $k$, we know that it must be greater than $k$ since it could be only at most one less than $v$. We can continue in this way to let $S$ be the set of vertices on the sink side of the graph which have an $h$ value greater than $k$. Suppose that there were some simple path from a vertex in $S$ to $s$. Then, at each of these steps, the height could only decrease by at most $1$, since it cannot get from above $k$ to $0$ without going through $k$, we know that there is no path in the residual flow network going from a vertex in $S$ to $s$. Since a minimal cut corresponds to disconnected parts of the residual graph for a maximum flow, and we know there is no path from $S$ to $s$, there is a minimum cut for which $S$ lies entirely on the source side of the cut. This was a contradiction to how we selected $v$, and so have shown the first claim.  Now we show that after updating the $h$ values as suggested, we are still left with a height function. Suppose we had an edge $(u, v)$ in the residual graph. We knew from before that $u.h \\le v.h + 1$. However, this means that if $u.h > k$, so must be $v.h$. So, if both were above $k$, we would be making them equal, causing the inequality to still hold. Also, if just $v.k$ were above $k$, then we have not decreased it's $h$ value, meaning that the inequality also still must hold. Since we have not changed the value of $s.h$, and $t.h$, we have all the required properties to have a height function after modifying the $h$ values as described.",
            "title": "26.5-5"
        },
        {
            "location": "/Chap26/Problems/26-1/",
            "text": "A$n \\times n$ \ngrid\n is an undirected graph consisting of $n$ rows and $n$ columns of vertices, as shown in Figure 26.11. We denote the vertex in the $i$th row and the $j$th column by $(i, j)$. All vertices in a grid have exactly four neighbors, except for the boundary vertices, which are the points $(i, j)$ for which $i = 1$, $i = n$, $j = 1$, or $j = n$.\n\n\nGiven $m \\le n^2$ starting points $(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)$ in the grid, the \nescape problem\n is to determine whether or not there are $m$ vertex-disjoint paths from the starting points to any $m$ different points on the boundary. For example, the grid in Figure 26.11(a) has an escape, but the grid in Figure 26.11(b) does not.\n\n\na.\n Consider a flow network in which vertices, as well as edges, have capacities. That is, the total positive flow entering any given vertex is subject to a capacity constraint. Show that determining the maximum flow in a network with edge and vertex capacities can be reduced to an ordinary maximum-flow problem on a flow network of comparable size.\n\n\nb.\n Describe an efficient algorithm to solve the escape problem, and analyze its running time.\n\n\n\n\na.\n This problem is identical to exercise 26.1-7.\n\n\nb.\n Construct a vertex constrained flow network from the instance of the escape problem by letting our flow network have a vertex (each with unit capacity) for each intersection of grid lines, and have a bidirectional edge with unit capacity for each pair of vertices that are adjacent in the grid. Then, we will put a unit capacity edge going from $s$ to each of the distinguished vertices, and a unit capacity edge going from each vertex on the sides of the grid to $t$. Then, we know that a solution to this problem will correspond to a solution to the escape problem because all of the augmenting paths will be a unit flow, because every edge has unit capacity. This means that the flows through the grid will be the paths taken. This gets us the escaping paths if the total flow is equal to $m$ (we know it cannot be greater than $m$ by looking at the cut which has $s$ by itself). And, if the max flow is less than $m$, we know that the escape problem is not solvable, because otherwise we could construct a flow with value $m$ from the list of disjoint paths that the people escaped along.",
            "title": "26-1 Escape problem"
        },
        {
            "location": "/Chap26/Problems/26-2/",
            "text": "A \npath cover\n of a directed graph $G = (V, E)$ is a set $P$ of vertex-disjoint paths such that every vertex in $V$ is included in exactly one path in $P$. Paths may start and end anywhere, and they may be of any length, including $0$. A \nminimum path cover\n of $G$ is a path cover containing the fewest possible paths.\n\n\na.\n Give an efficient algorithm to find a minimum path cover of a directed acyclic graph $G = (V, E)$. ($\\textit{Hint:}$ Assuming that $V = \\{1, 2, \\ldots, n\\}$, construct the graph $G' = (V', E')$, where\n\n\n\\begin{align}\nV' & = \\{x_0, x_1, \\ldots, x_n\\} \\cup \\{y_0, y_1, \\ldots, y_n\\}, \\\\\nE' & = \\{(x_0, x_i): i \\in V\\} \\cup \\{(y_i, y_0): i \\in V\\} \\cup \\{(x_i, y_j): (i, j) \\in E\\},\n\\end{align}\n\n\nand run a maximum-flow algorithm.)\n\n\nb.\n Does your algorithm work for directed graphs that contain cycles? Explain.\n\n\n\n\na.\n The idea is to use a maximum-flow algorithm to find a maximum bipartite matching that selects the edges to use in a minimum path cover. We must show how to formulate the max-flow problem and how to construct the path cover from the resulting matching, and we must prove that the algorithm indeed finds a minimum path cover.\n\n\nDefine $G'$ as suggested, with directed edges. Make $G'$ into a flow network with source $x_0$ and sink $y_0$ by defining all edge capacities to be $1$. $G'$ is the flow network corresponding to a bipartite graph $G''$ in which $L = \\{x_1, \\ldots, x_n\\}$, $R = \\{y_1, \\ldots, y_n\\}$, and the edges are the (undirected version of the) subset of $E'$ that doesn't involve $x_0$ or $y_0$ .\n\n\nThe relationship of $G$ to the bipartite graph $G''$ is that every vertex $i$ in $G$ is represented by two vertices, $x_i$ and $y_i$, in $G''$. Edge $(i, j)$ in $G$ corresponds to edge $(x_i, y_j)$ in $G''$. That is, an edge $(x_i, y_j)$ in $G''$ means that an edge in $G$ leaves $i$ and enters $j$. Vertex $x_i$ tells us about edges leaving $i$, and $y_i$ tells us about edges entering $i$.\n\n\nThe edges in a bipartite matching in $G''$ can be used in a path cover of $G$, for the following reasons:\n\n\n\n\nIn a bipartite matching, no vertex is used more than once. In a bipartite matching in $G''$, since no $x_i$ is used more than once, at most one edge in the matching leaves any vertex $i$ in $G$. Similarly, since no $y_j$ is used more than once, at most one edge in the matching enters any vertex $j$ in $G$.\n\n\nIn a path cover, since no vertex appears in more than one path, at most one path edge enters each vertex and at most one path edge leaves each vertex.\n\n\n\n\nWe can construct a path cover $P$ from any bipartite matching $M$ (not just a maximum matching) by moving from some $x_i$ to its matching $y_j$ (if any), then from $x_j$ to its matching $y_k$, and so on, as follows:\n\n\n\n\nStart a new path containing a vertex $i$ that has not yet been placed in a path.\n\n\nIf $x_i$ is unmatched, the path can't go any farther; just add it to $P$.\n\n\nIf $x_i$ is matched to some $y_j$, add $j$ to the current path. If $j$ has already been placed in a path (i.e., though we've just entered $j$ by processing $y_j$, we've already built a path that leaves $j$ by processing $x_j$), combine this path with that one and go back to step 1. Otherwise go to step 2 to process $x_j$.\n\n\n\n\nThis algorithm constructs a path cover, for the following reasons:\n\n\n\n\nEvery vertex is put into some path, because we keep picking an unused vertex from which to start a path until there are no unused vertices.\n\n\nNo vertex is put into two paths, because every $x_i$ is matched to at most one $y_j$, and vice versa. That is, at most one candidate edge leaves each vertex, and at most one candidate edge enters each vertex. When building a path, we start or enter a vertex and then leave it, building a single path. If we ever enter a vertex that was left earlier, it must have been the start of another path, since there are no cycles, and we combine those paths so that the vertex is entered and left on a single path.\n\n\n\n\nEvery edge in $M$ is used in some path because we visit every $x_i$, and we incorporate the single edge, if any, from each visited $x_i$. Thus, there is a one-to-one correspondence between edges in the matching and edges in the constructed path cover.\n\n\nWe now show that the path cover $P$ constructed above has the fewest possible paths when the matching is maximum.\n\n\nLet $f$ be the flow corresonding to the bipartite matching $M$.\n\n\n\\begin{align}\n|V| & = \\sum_{p \\in P} \\text{(# vertices in $p$)} & \\text{(every vertex is on exactly 1 path)} \\\\\n    & = \\sum_{p \\in P} \\text{(1 + # edges in $p$)} \\\\\n    & = \\sum_{p \\in P} 1 + \\sum_{p \\in P} \\text{(# edges in $p$)} \\\\\n    & = |P| + |M|                                 & \\text{(by 1-to-1 correspondence)} \\\\\n    & = |P| + |f|.                                & \\text{(by Lemma 26.9)}\n\\end{align}\n\n\nThus, for the fixed set $V$ in our graph $G$, $|P|$ (the number of paths) is minimized when the flow $f$ is maximized.\n\n\nThe overall algorithm is as follows:\n\n\n\n\nUse $\\text{FORD-FULKERSON}$ to find a maximum flow in $G'$ and hence a maximum bipartite matching $M$ in $G''$.\n\n\nConstruct the path cover as described above.\n\n\n\n\nTime\n\n\n$O(VE)$ total:\n\n\n\n\n$O(V + E)$ to set up $G'$, \n\n\n$O(VE)$ to find the maximum bipartite matching,\n\n\n$O(E)$ to trace the paths, because each edge $\\in M$ is traversed only once and there are $O(E)$ edges in $M$.\n\n\n\n\nb.\n The algorithm does not work if there are cycles.\n\n\nConsider a graph $G$ with $4$ vertices, consisting of a directed triangle and an edge pointing to the triangle:\n\n\n$$E = \\{(1, 2), (2, 3), (3, 1), (4, 1)\\}.$$\n\n\n$G$ can be covered with a single path: $4 \\to 1 \\to 2 \\to 3$, but our algorithm might find only a $2$-path cover.\n\n\nIn the bipartite graph $G'$, the edges $(x_i, y_j)$ are\n\n\n$$(x_1, y_2), (x_2, y_3), (x_3, y_1), (x_4, y_1).$$\n\n\nThere are $4$ edges from an $x_i$ to a $y_j$, but $2$ of them lead to $y_1$, so a maximum bipartite matching can have only $3$ edges (and the maximum flow in $G'$ has value $3$). In fact, there are $2$ possible maximum matchings. It is always possible to match $(x_1, y_2)$ and $(x_2, y_3)$, and then either $(x_3, y_1)$ or $(x_4, y_1)$ can be chosen, but not both.\n\n\nThe maximum flow found by one of our max-flow algorithms could find the flow corresponding to either of these matchings, since both are maximal. If it finds the matching with edge $(x_3, x_1)$, then the matching would not contain $(x_4, x_1)$; given that matching, our path algorithm is forced to produce $2$ paths, one of which contains just the vertex $4$.",
            "title": "26-2 Minimum path cover"
        },
        {
            "location": "/Chap26/Problems/26-3/",
            "text": "Professor Gore wants to open up an algorithmic consulting company. He has identified n important subareas of algorithms (roughly corresponding to different portions of this textbook), which he represents by the set $A = \\{A_1, A_2, \\ldots, A_n\\}$. In each subarea $A_k$, he can hire an expert in that area for $c_k$ dollars. The consulting company has lined up a set $J = \\{J_1, J_2, \\ldots, J_m\\}$ of potential jobs. In order to perform job $J_i$, the company needs to have hired experts in a subset $R_i \\subseteq A$ of subareas. Each expert can work on multiple jobs simultaneously. If the company chooses to accept job $J_i$, it must have hired experts in all subareas in $R_i$, and it will take in revenue of $p_i$ dollars.\n\n\nProfessor Gore's job is to determine which subareas to hire experts in and which jobs to accept in order to maximize the net revenue, which is the total income from jobs accepted minus the total cost of employing the experts.\n\n\nConsider the following flow network $G$. It contains a source vertex $s$, vertices $A_1, A_2, \\ldots, A_n$, vertices $J_1, J_2, \\ldots, J_m$, and a sink vertex $t$. For $k = 1, 2, \\ldots, n$, the flow network contains an edge $(s, A_k)$ with capacity $c(s, A_k) = c_k$, and for $i = 1, 2, \\ldots, m$, the flow network contains an edge $(J_i, t)$ with capacity $c(J_i, t) = p_i$. For $k = 1, 2, \\ldots, n$ and $i = 1, 2, \\ldots, m$, if $A_k \\in R_i$, then $G$ contains an edge $(A_k, J_i)$ with capacity $c(A_k, J_i) = \\infty$.\n\n\na.\n Show that if $J_i \\in T$ for a finite-capacity cut $(S, T)$ of $G$, then $A_k \\in T$ for each $A_k \\in R_i$.\n\n\nb.\n Show how to determine the maximum net revenue from the capacity of a minimum cut of $G$ and the given $p_i$ values.\n\n\nc.\n Give an efficient algorithm to determine which jobs to accept and which experts to hire. Analyze the running time of your algorithm in terms of $m$, $n$, and $r = \\sum_{i = 1}^m |R_i|$.\n\n\n\n\na.\n Assume for the sake of contradiction that $A_k \\notin T$ for some $A_k \\in R_i$. Since $A_k \\notin T$, we must have $A_k \\in S$. On the other hand, we have $J_i \\in T$. Thus, the edge $(A_k, J_i)$ crosses the cut $(S, T)$. But $c(A_k, J_i) = \\infty$ by construction, which contradicts the assumption that $(S, T)$ is a finite-capacity cut.\n\n\nb.\n Let us define a \nproject-plan\n as a set of jobs to accept and experts to hire. Let $P$ be a project-plan. We assume that $P$ has two attributes. The attribute $P.J$ denotes the set of accepted jobs, and $P.A$ denotes the set of hired experts.\n\n\nA \nvalid\n project-plan is one in which we have hired all experts that are required by the accepted jobs. Specifically, let $P$ be a valid project plan. If $J_i \\in P.J$, then $A_k \\in P.A$ for each $A_k \\in R_i$. Note that Professor Gore might decide to hire more experts than those that are actually required.\n\n\nWe define the \nrevenue\n of a project-plan as the total profit from the accepted jobs minus the total cost of the hired experts. The problem asks us to find a valid project plan with maximum revenue.\n\n\nWe start by proving the following lemma, which establishes the relationship between the capacity of a cut in flow network $G$ and the revenue of a valid project-plan.\n\n\nLemma (Min-cut max-revenue)\n \n\n\nThere exists a finite-capacity cut $(S, T)$ of $G$ with capacity $c(S, T)$ if and only if there exists a valid project-plan with net revenue $(\\sum_{J_i \\in J} p_i) - c(S, T)$.\n\n\nProof\n \n\n\nLet $(S, T)$ be a finite-capacity cut of $G$ with capacity $c(S, T)$. We prove one direction of the lemma by constructing the required project-plan.\n\n\nConstruct the project-plan $P$ by including $J_i$ in $P.J$ if and only if $J_i \\in T$ and including $A_k$ in $P.A$ if and only if $A_k \\in T$. From part (a), $P$ is a valid project-plan, since, for every $J_i \\in P.J$, we have $A_k \\in P.A$ for each $A_k \\in R_i$.\n\n\nSince the capacity of the cut is finite, there cannot be any edges of the form $(A_k, J_i)$ crossing the cut, where $A_k \\in S$ and $J_i \\in T$. All edges going from a vertex in $S$ to a vertex in $T$ must be either of the form $(s, A_k)$ or of the form $(J_i, t)$. Let $E_A$ be the set of edges of the form $(s, A_k)$ that cross the cut, and let $E_J$ be the set of edges of the form $(J_i, t)$ that cross the cut, so that\n\n\n$$c(S, T) = \\sum_{(s, A_k) \\in E_A} c(s, A_k) + \\sum_{(J_i, j) \\in E_J} c(J_i, t).$$\n\n\nConsider edges of the form $(s, A_k)$. We have\n\n\n\\begin{align}\n(s, A_k) \\in E_A\n    & \\text{ if and only if $A_k \\in T$} \\\\\n    & \\text{ if and only if $A_k \\in P.A$}.\n\\end{align}\n\n\nBy construction, $c(s, A_k) = c_k$. Taking summations over $E_A$ and over $P.A$, we obtain\n\n\n$$\\sum_{(s, A_k) \\in E_A} c(s, A_k) = \\sum_{A_k \\in P.A} c_k.$$\n\n\nSimilarly, consider edges of the form $(J_i, t)$. We have\n\n\n\\begin{align}\n(J_i, t) \\in E_J\n    & \\text{ if and only if $J_i \\in S$} \\\\\n    & \\text{ if and only if $J_i \\notin T$} \\\\\n    & \\text{ if and only if $J_i \\notin P.J$}.\n\\end{align}\n\n\nBy construction, $c(J_i, t) = p_i$. Taking summations over $E_J$ and over $P.J$, we obtain\n\n\n$$\\sum_{(J_i, t) \\in E_J} c(J_i, t) = \\sum_{J_i \\notin P.J} p_i.$$\n\n\nLet $v$ be the net revenue of $P$. Then, we have\n\n\n\\begin{align}\nv & = \\sum_{J_i \\in P.J} p_i - \\sum_{A_k \\in P.A} c_k \\\\\n  & = \\Bigg( \\sum_{J_i \\in J} p_i - \\sum_{J_i \\notin P.J} p_i \\Bigg) - \\sum_{A_k \\in P.A} c_k \\\\\n  & = \\sum_{J_i \\in J} p_i - \\Bigg( \\sum_{J_i \\notin P.J} p_i + \\sum_{A_k \\in P.A} c_k \\Bigg) \\\\\n  & = \\sum_{J_i \\in J} p_i - \\Bigg( \\sum_{(J_i, t) \\in E_J} c(J_i, t) + \\sum_{(s, A_k) \\in E_A} c(s, A_k) \\Bigg) \\\\\n  & = \\Bigg( \\sum_{J_i \\in J} p_i \\Bigg) - c(S, T).\n\\end{align}\n\n\nNow, we prove the other direction of the lemma by constructing the required cut from a valid project-plan.\n\n\nConstruct the cut $(S, T)$ as follows. For every $J_i \\in P.J$, let $J_i \\in T$. For every $A_k \\in P.A$, let $A_k \\in T$.\n\n\nFirst, we prove that the cut $(S, T)$ is a finite-capacity cut. Since edges of the form $(A_k, J_i)$ are the only infinite-capacity edges, it suffices to prove that there are no edges $(A_k, J_i)$ such that $A_k \\in S$ and $J_i \\in T$.\n\n\nFor the purpose of contradiction, assume there is an edge $(A_k, J_i)$ such that $A_k \\in S$ and $J_i \\in T$. By our constuction, we must have $J_i \\in P.J$ and $A_k \\notin P.A$. But since the edge $(A_k, J_i)$ exists, we have $A_k \\in R_i$. Since $P$ is a valid project-plan, we derive the contradiction that $A_k$ must have been in $P.A$.\n\n\nFrom here on, the analysis is the same as the previous direction. In particular, the last equation from the previous analysis holds: the net revenue $v$ equals $(\\sum_{J_i \\in J} p_i) - c(S, T)$.\n\n\nWe conclude that the problem of finding a maximum-revenue project-plan reduces to the problem of finding a minimum cut in $G$. Let $(S, T)$ be a minimum cut. From the lemma, the maximum net revenue is given by\n\n\n$$\\Bigg( \\sum_{j_i \\in J} p_i \\Bigg) - c(S, T).$$\n\n\nc.\n Construct the flow network $G$ as shown in the problem statement. Obtain a minimum cut $(S, T)$ by running any of the maximum-flow algorithms (say, Edmonds-Karp). Construct the project plan $P$ as follows: add $J_i$ to $P.J$ if and only if $J_i \\in T$. Add $A_k$ to $P.A$ if and only if $A_k \\in T$.\n\n\nFirst, we note that the number of vertices in $G$ is $|V| = m + n + 2$, and the number of edges in $G$ is $|E| = r + m + n$. Constructing $G$ and recovering the project-plan from the minimum cut are clearly linear-time operations. The running time of our algorithm is thus asymptotically the same as the running time of the algorithm used to find the minimum cut. If we use Edmonds-Karp to find the minimum cut, the running time is $O(VE^2)$.",
            "title": "26-3 Algorithmic consulting"
        },
        {
            "location": "/Chap26/Problems/26-4/",
            "text": "Let $G = (V, E)$ be a flow network with source $s$, sink $t$, and integer capacities. Suppose that we are given a maximum flow in $G$.\n\n\na.\n Suppose that we increase the capacity of a single edge $(u, v) \\in E$ by $1$. Give an $O(V + E)$-time algorithm to update the maximum flow.\n\n\nb.\n Suppose that we decrease the capacity of a single edge $(u, v) \\in E$ by $1$. Give an $O(V + E)$-time algorithm to update the maximum flow.\n\n\n\n\na.\n Just execute one iteration of the Ford-Fulkerson algorithm. The edge $(u, v)$ in $E$ with increased capacity ensures that the edge $(u, v)$ is in the residual network. So look for an augmenting path and update the flow if a path is found.\n\n\nTime\n\n\n$O(V + E) = O(E)$ if we find the augmenting path with either depth-first or breadth-first search.\n\n\nTo see that only one iteration is needed, consider separately the cases in which $(u, v)$ is or is not an edge that crosses a minimum cut. If $(u, v)$ does not cross a minimum cut, then increasing its capacity does not change the capacity of any minimum cut, and hence the value of the maximum flow does not change. If $(u, v)$ does cross a minimum cut, then increasing its capacity by $1$ increases the capacity of that minimum cut by $1$, and hence possibly the value of the maximum flow by $1$. In this case, there is either no augmenting path (in which case there was some other minimum cut that $(u, v)$ does not cross), or the augmenting path increases flow by $1$. No matter what, one iteration of Ford-Fulkerson suffices.\n\n\nb.\n Let $f$ be the maximum flow before reducing $c(u, v)$.\n\n\n\n\nIf $f(u, v) = 0$, we don't need to do anything.\n\n\nIf $f(u, v) > 0$, we will need to update the maximum flow. Assume from now on that $f(u, v) > 0$, which in turn implies that $f(u, v) \\ge 1$.\n\n\n\n\nDefine $f'(x, y) = f(x, y)$ for all $x, y \\in V$, except that $f'(u, v) = f(u, v) - 1$. Although $f'$ obeys all capacity contraints, even after $c(u, v)$ has been reduced, it is not a legal flow, as it violates flow conservation at $u$ (unless $u = s$) and $v$ (unless $v = t$). $f'$ has one more unit of flow entering $u$ than leaving $u$, and it has one more unit of flow leaving $v$ than entering $v$.\n\n\nThe idea is to try to reroute this unit of flow so that it goes out of $u$ and into $v$ via some other path. If that is not possible, we must reduce the flow from $s$ to $u$ and from $v$ to $t$ by one unit.\n\n\nLook for an augmenting path from $u$ to $v$ (note: \nnot\n from $s$ to $t$).\n\n\n\n\nIf there is such a path, augment the flow along that path.\n\n\nIf there is no such path, reduce the flow from $s$ to $u$ by augmenting the flow from $u$ to $s$. That is, find an augmenting path $u \\leadsto s$ and augment the flow along that path. (There definitely is such a path, because there is flow from $s$ to $u$.) Similarly, reduce the flow from $v$ to $t$ by finding an augmenting path $t \\leadsto v$ and augmenting the flow along that path.\n\n\n\n\nTime\n\n\n$O(V + E) = O(E)$ if we find the paths with either $\\text{DFS}$ or $\\text{BFS}$.",
            "title": "26-4 Updating maximum flow"
        },
        {
            "location": "/Chap26/Problems/26-5/",
            "text": "Let $G = (V, E)$ be a flow network with source $s$, sink $t$, and an integer capacity $c(u, v)$ on each edge $(u, v) \\in E$. Let $C = \\max_{(u, v) \\in E} c(u, v)$.\n\n\na.\n Argue that a minimum cut of $G$ has capacity at most $C|E|$.\n\n\nb.\n For a given number $K$, show how to find an augmenting path of capacity at least $K$ in $O(E)$ time, if such a path exists.\n\n\nWe can use the following modification of $\\text{FORD-FULKERSON-METHOD}$ to compute a maximum flow in $G$:\n\n\nMAX\n-\nFLOW\n-\nBY\n-\nSCALING\n(\nG\n,\n \ns\n,\n \nt\n)\n\n    \nC\n \n=\n \nmax_\n{(\nu\n,\n \nv\n)\n \n\u2208\n \nE\n}\n \nc\n(\nu\n,\n \nv\n)\n\n    \ninitialize\n \nflow\n \nf\n \nto\n \n0\n\n    \nK\n \n=\n \n2\n^\n{\nfloor\n(\nlg\n \nC\n)}\n\n    \nwhile\n \nK\n \n\u2265\n \n1\n\n        \nwhile\n \nthere\n \nexists\n \nan\n \naugmenting\n \npath\n \np\n \nof\n \ncapacity\n \nat\n \nleast\n \nK\n \naugment\n \nflow\n \nf\n \nalong\n \np\n\n        \nK\n \n=\n \nK\n \n/\n \n2\n\n    \nreturn\n \nf\n\n\n\n\n\nc.\n Argue that $\\text{MAX-FLOW-BY-SCALING}$ returns a maximum flow.\n\n\nd.\n Show that the capacity of a minimum cut of the residual network $G_f$ is at most $2K|E|$ each time line 4 is executed.\n\n\ne.\n Argue that the inner \nwhile\n loop of lines 5\u20136 executes $O(E)$ times for each value of $K$.\n\n\nf.\n Conclude that $\\text{MAX-FLOW-BY-SCALING}$ can be implemented so that it runs in $O(E^2\\lg C)$ time.\n\n\n\n\na.\n The capacity of a cut is defined to be the sum of the capacities of the edges crossing it. Since the number of such edges is at most $|E|$, and the capacity of each edge is at most $C$, the capacity of \nany\n cut of $G$ is at most $C|E|$.\n\n\nb.\n The capacity of an augmenting path is the minimum capacity of any edge on the path, so we are looking for an augmenting path whose edges \nall\n have capacity at least $K$. Do a breadth-first search or depth-first-search as usual to find the path, considering only edges with residual capacity at least $K$. (Treat lower-capacity edges as though they don't exist.) This search takes $O(V + E) = O(E)$ time. (Note that $|V| = O(E)$ in a flow network.)\n\n\nc.\n $\\text{MAX-FLOW-BY-SCALING}$ uses the Ford-Fulkerson method. It repeatedly augments the flow along an augmenting path until there are no augmenting paths with capacity at least $1$. Since all the capacities are integers, and the capacity of an augmenting path is positive, when there are no augmenting paths with capacity at least $1$, there must be no augmenting paths whatsoever in the residual network. Thus, by the max-flow min-cut theorem, $\\text{MAX-FLOW-BY-SCALING}$ returns a maximum flow.\n\n\nd.\n \n\n\n\n\nThe first time line 4 is executed, the capacity of any edge in $G_f$ equals its capacity in G, and by part (a) the capacity of a minimum cut of $G$ is at most $C|E|$. Initially $K = 2^{\\lfloor \\lg C \\rfloor}$, and so $2K = 2 \\cdot 2^{\\lfloor \\lg C \\rfloor + 1} > 2^{\\lg C} = C$. Thus, the capacity of a minimum cut of $G_f$ is initially less than $2K|E|$.\n\n\nThe other times line 4 is executed, $K$ has just been halved, and so the capacity of a cut of $G_f$ is at most $2K|E|$ at line 4 if and only if that capacity was at most $K|E|$ when the \nwhile\n loop of lines 5\u20136 last terminated. Thus, we want to show that when line 7 is reached, the capacity of a minimum cut of $G_f$ is at most $K|E|$.\n\n    Let $G_f$ be the residual network when line 7 is reached. When we reach line 7, $G_f$ contains no augmenting path with capacity at least $K$. Therefore, a maximum flow $f'$ in $G_f$ has value $|f'| < K|E|$. Then, by the max-flow min-cut theorem, a minimum cut in $G_f$ has capacity less than $K|E|$.\n\n\n\n\ne.\n By part (d), when line 4 is reached, the capacity of a minimum cut of $G_f$ is at most $2K|E|$, and thus the maximum flow in $G_f$ is at most $2K|E|$. The following lemma shows that the value of a maximum flow in $G$ equals the value of the current flow $f$ in $G$ plus the value of a maximum flow in $G_f$.\n\n\nLemma\n \n\n\nLet $f$ be a flow in flow network $G$, and $f'$ be a maximum flow in the residual network $G_f$. Then $f \\uparrow f'$ is a maximum flow in $G$.\n\n\nProof\n \n\n\nBy the max-flow min-cut theorem, $|f'| = c_f(S, T)$ for some cut $(S, T)$ of $G_f$, which is also a cut of $G$. By Lemma 26.4, $|f| = f(S, T)$. By Lemma 26.1, $f \\uparrow f'$ is a flow in $G$ with value $|f \\uparrow f'| = |f| + |f'|$. We will show that $|f| + |f'| = c(S, T)$ which, by the max-flow min-cut theorem, will prove that $f \\uparrow f'$ is a maximum flow in $G$.\n\n\nWe have\n\n\n\\begin{align}\n|f| + |f'|\n    & = f(S, T) + c_f(S, T) \\\\\n    & = \\Bigg( \\sum_{u \\in S} \\sum_{v \\in T} f(u, v) - \\sum_{u \\in S} \\sum_{v \\in T} f(v, u) \\Bigg) + \\sum_{u \\in S} \\sum_{v \\in T} c_f(u, v) \\\\\n    & = \\Bigg( \\sum_{u \\in S, v \\in T} f(u, v) - \\sum_{u \\in S, v \\in T} f(v, u) \\Bigg) + \\Bigg( \\sum_{u \\in S, v \\in T, (u, v) \\in E} c(u, v) - \\sum_{u \\in S, v \\in T, (u, v) \\in E} f(u, v) + \\sum_{u \\in S, v \\in T, (v, u) \\in E} f(v, u) \\Bigg). \n\\end{align}\n\n\nNoting that $(u, v) \\notin E$ implies $f(u, v) = 0$, we have that\n\n\n$$\\sum_{u \\in S, v \\in T} f(u, v) = \\sum_{u \\in S, v \\in T, (u, v) \\in E} f(u, v).$$\n\n\nSimilarly,\n\n\n$$\\sum_{u \\in S, v \\in T} f(v, u) = \\sum_{u \\in S, v \\in T, (v, u) \\in E} f(v, u).$$\n\n\nThus, the summations of $f(u, v)$ cancel each other out, as do the summations of $f(v, u)$. Therefore,\n\n\n\\begin{align}\n|f| + |f'|\n    & = \\sum_{u \\in S, v \\in T, (u, v) \\in E} c(u, v) \\\\\n    & = \\sum_{u \\in S} \\sum_{v \\in T} c(u, v) \\\\\n    & = c(S, T).\n\\end{align}\n\n\nBy this lemma, we see that the value of a maximum flow in $G$ is at most $2K|E|$ more than the value of the current flow $f$ in $G$. Every time the inner \nwhile\n loop finds an augmenting path of capacity at least $K$, the flow in $G$ increases by at least $K$. Since the flow cannot increase by more than $2K|E|$, the loop executes at most $(2K|E|) / K = 2|E|$ times.\n\n\nf.\n The time complexity is dominated by the \nwhile\n loop of lines 4\u20137. (The lines outside the loop take $O(E)$ time.) The outer \nwhile\n loop executes $O(\\lg C)$ times, since $K$ is initially $O(C)$ and is halved on each iteration, until $K < 1$. By part (e), the inner \nwhile\n loop executes $O(E)$ times for each value of $K$, and by part (b), each iteration takes $O(E)$ time. Thus, the total time is $O(E^2 \\lg C)$.",
            "title": "26-5 Maximum flow by scaling"
        },
        {
            "location": "/Chap26/Problems/26-6/",
            "text": "In this problem, we describe a faster algorithm, due to Hopcroft and Karp, for $p$ finding a maximum matching in a bipartite graph. The algorithm runs in $O(\\sqrt V E)$ time. Given an undirected, bipartite graph $G = (V, E)$, where $V = L \\cup R$ and all edges have exactly one endpoint in $L$, let $M$ be a matching in $G$. We say that a simple path $P$ in $G$ is an \naugmenting path\n with respect to $M$ if it starts at an unmatched vertex in $L$, ends at an unmatched vertex in $R$, and its edges belong alternately to $M$ and $E - M$. (This definition of an augmenting path is related to, but different from, an augmenting path in a flow network.) In this problem, we treat a path as a sequence of edges, rather than as a sequence of vertices. A shortest augmenting path with respect to a matching $M$ is an augmenting path with a minimum number of edges.\n\n\nGiven two sets $A$ and $B$, the \nsymmetric difference\n $A \\oplus B$ is defined as $(A - B) \\cup (B - A)$, that is, the elements that are in exactly one of the two sets.\n\n\na.\n Show that if $M$ is a matching and $P$ is an augmenting path with respect to $M$, then the symmetric difference $M \\oplus P$ is a matching and $M \\oplus P = |M| + 1$. Show that if $P_1, P_2, \\ldots, P_k$ are vertex-disjoint augmenting paths with respect to $M$, then the symmetric difference $M \\oplus (P_1 \\cup P_2 \\cup \\cdots \\cup P_k)$ is a matching with cardinality $|M| + k$.\n\n\nThe general structure of our algorithm is the following:\n\n\nHOPCROPFT\n-\nKARP\n(\nG\n)\n\n    \nM\n \n=\n \n\u2205\n\n    \nrepeat\n\n        \nlet\n \nP\n \n=\n \n{\nP1\n,\n \nP2\n,...,\nPk\n}\n \nbe\n \na\n \nmaximal\n \nset\n \nof\n \nvertex\n-\ndisjoint\n \nshortest\n \naugmenting\n \npaths\n \nwith\n \nrespect\n \nto\n \nM\n\n        \nM\n \n=\n \nM\n \n\u2a01\n \n(\nP1\n \n\u222a\n \nP2\n \n\u222a\n \n...\n \n\u222a\n \nPk\n)\n\n    \nuntil\n \nP\n \n==\n \n\u2205\n\n    \nreturn\n \nM\n\n\n\n\n\nThe remainder of this problem asks you to analyze the number of iterations in the algorithm (that is, the number of iterations in the repeat loop) and to describe an implementation of line 3.\n\n\nb.\n Given two matchings $M$ and $M^*$ in $G$, show that every vertex in the graph $G' = (V, M \\oplus M^*)$ has degree at most $2$. Conclude that $G'$ is a disjoint union of simple paths or cycles. Argue that edges in each such simple path or cycle belong alternately to $M$ or $M^*$. Prove that if $|M| \\le |M^*|$, then $M \\oplus M^*$ contains at least $|M^*| - |M|$ vertex-disjoint augmenting paths with respect to $M$.\n\n\nLet $l$ be the length of a shortest augmenting path with respect to a matching $M$, and let $P_1, P_2, \\ldots, P_k$ be a maximal set of vertex-disjoint augmenting paths of length $l$ with respect to $M$. Let $M' = M \\oplus (P_1 \\cup \\cdots \\cup P_k)$, and suppose that $P$ is a shortest augmenting path with respect to $M'$.\n\n\nc.\n Show that if $P$ is vertex-disjoint from $P_1, P_2, \\ldots, P_k$ , then $P$ has more than $l$ edges.\n\n\nd.\n Now suppose that $P$ is not vertex-disjoint from $P_1, P_2, \\ldots, P_k$ . Let $A$ be the set of edges $(M \\oplus M') \\oplus P$. Show that $A = (P_1 \\cup P_2 \\cup \\cdots \\cup P_k) \\oplus P$ and that $|A| \\ge (k + 1)l$. Conclude that $P$ has more than $l$ edges.\n\n\ne.\n Prove that if a shortest augmenting path with respect to $M$ has $l$ edges, the size of the maximum matching is at most $|M| + |V| / (l + 1)$.\n\n\nf.\n Show that the number of \nrepeat\n loop iterations in the algorithm is at most $2\\sqrt{|V|}$. ($\\textit{Hint:}$ By how much can $M$ grow after iteration number $\\sqrt{|V|}$?)\n\n\ng.\n Give an algorithm that runs in $O(E)$ time to find a maximal set of vertexdisjoint shortest augmenting paths $P_1, P_2, \\ldots, P_k$ for a given matching $M$. Conclude that the total running time of $\\text{HOPCROFT-KARP}$ is $O(\\sqrt V E)$.\n\n\n\n\na.\n Suppose $M$ is a matching and $P$ is an augmenting path with respect to $M$. Then $P$ consists of $k$ edges in $M$, and $k + 1$ edges not in $M$. This is because the first edge of $P$ touches an unmatched vertex in $L$, so it cannot be in $M$. Similarly, the last edge in $P$ touches an unmatched vertex in $R$, so the last edge cannot be in $M$. Since the edges alternate being in or not in $M$, there must be exactly one more edge not in $M$ than in $M$. This implies that\n\n\n$$|M \\oplus P| = |M| + |P| - 2k = |M| + 2k + 1 - 2k = |M| + 1,$$\n\n\nsince we must remove each edge of $M$ which is in $P$ from both $M$ and $P$. Now suppose $P_1, P_2, \\ldots, P_k$ are vertex-disjoint augmenting paths with respect to $M$. Let $k_i$ be the number of edges in $P_i$ which are in $M$, so that $|P_i| = 2k + i + 1$. Then we have\n\n\n$$M \\oplus (P_1 \\cup P_2 \\cup \\cdots \\cup P_k) = |M| + |P_1| + \\cdots + |P_k| - 2k_1 - 2k_2 - \\cdots - 2k_k = |M| + k.$$\n\n\nTo see that we in fact get a matching, suppose that there was some vertex $v$ which had at least $2$ incident edges $e$ and $e'$. They cannot both come from $M$, since $M$ is a matching. They cannot both come from $P$ since $P$ is simple and every other edge of $P$ is removed. Thus, $e \\in M$ and $e' \\in P \\backslash M$. However, if $e \\in M$ then $e \\in P$, so $e \\notin M \\oplus P$, a contradiction. A similar argument gives the case of $M \\oplus (P_1 \\cup \\cdots \\cup P_k)$.\n\n\nb.\n Suppose some vertex in $G'$ has degree at least $3$. Since the edges of $G'$ come from $M \\oplus M^*$, at least $2$ of these edges come from the same matching. However, a matching never contains two edges with the same endpoint, so this is impossible. Thus every vertex has degree at most $2$, so $G'$ is a disjoint union of simple paths and cycles. If edge $(u, v)$ is followed by edge $(z, w)$ in a simple path or cycle then we must have $v = z$. Since two edges with the same endpoint cannot appear in a matching, they must belong alternately to $M$ and $M^*$. Since edges alternate, every cycle has the same number of edges in each matching and every path has at most one more edge in one matching than in the other. Thus, if $|M| \\le |M^*|$ there must be at least $|M^*| - |M|$ vertex-disjoint augmenting paths with respect to $M$.\n\n\nc.\n Every vertex matched by $M$ must be incident with some edge in $M'$. Since $P$ is augmenting with respect to $M$\u2032, the left endpoint of the first edge of $P$ isn't incident to a vertex touched by an edge in $M'$. In particular, $P$ starts at a vertex in $L$ which is unmatched by $M$ since every vertex of $M$ is incident with an edge in $M'$. Since $P$ is vertex disjoint from $P_1, P_2, \\ldots, P_k$, any edge of $P$ which is in $M'$ must in fact be in $M$ and any edge of $P$ which is not in $M'$ cannot be in $M$. Since $P$ has edges alternately in $M'$ and $E - M'$, $P$ must in fact have edges alternately in $M$ and $E - M$. Finally, the last edge of $P$ must be incident to a vertex in $R$ which is unmatched by $M'$. Any vertex unmatched by $M'$ is also unmatched by $M$, so $P$ is an augmenting path for $M$. $P$ must have length at least $l$ since $l$ is the length of the shortest augmenting path with respect to $M$. If $P$ had length exactly $l$, then this would contradict the fact that $P_1 \\cup \\cdots \\cup P_k$ is a maximal set of vertex disjoint paths of length $l$ because we could add $P$ to the set. Thus $P$ has more than $l$ edges.\n\n\nd.\n Any edge in $M \\oplus M'$ is in exactly one of $M$ or $M'$. Thus, the only possible contributing edges from $M'$ are from $P_1 \\cup \\cdots \\cup P_k$. An edge from $M$ can contribute if and only if it is not in exactly one of $M$ and $P_1 \\cup \\cdots \\cup P_k$, which means it must be in both. Thus, the edges from $M$ are redundant so $M \\oplus M' = (P_1 \\cup \\cdots \\cup P_k)$ which implies $A = (P_1 \\cup \\cdots \\cup P_k) \\oplus P$.\n\n\nNow we'll show that $P$ is edge disjoint from each $P_i$. Suppose that an edge $e$ of $P$ is also an edge of $P_i$ for some $i$. Since $P$ is an augmenting path with respect to $M'$ either $e \\in M'$ or $e \\in E - M'$. Suppose $e \\in M'$. Since $P$ is also augmenting with respect to $M$, we must have $e \\in M$. However, if $e$ is in $M$ and $M'$, then $e$ cannot be in any of the $P_i$'s by the definition of $M'$. Now suppose $e \\in E - M'$. Then $e \\in E - M$ since $P$ is augmenting with respect to $M$. Since $e$ is an edge of $P_i$, $e \\in E - M'$ implies that $e \\in M$, a contradiction.\n\n\nSince $P$ has edges alternately in $M'$ and $E - M'$ and is edge disjoint from $P_1 \\cup \\cdots \\cup P_k$, $P$ is also an augmenting path for $M$, which implies $|P| \\ge l$. Since every edge in $A$ is disjoint we conclude that $|A| \\ge (k + 1)l$.\n\n\ne.\n Suppose $M^*$ is a matching with strictly more than $|M| + |V| / (l + 1)$ edges. By part (b) there are strictly more than $|V| / (l + 1)$ vertex-disjoint augmenting paths with respect to $M$. Each one of these contains at least $l$ edges, so it is incident on $l + 1$ vertices. Since the paths are vertex disjoint, there are strictly more than $|V|(l + 1) / (l + 1)$ distinct vertices incident with these paths, a contradiction. Thus, the size of the maximum matching is at most $|M| + |V| / (l + 1)$.\n\n\nf.\n Consider what happens after iteration number $\\sqrt{|V|}$. Let $M^*$ be a maximal matching in $G$. Then $|M^*| \\ge |M|$ so by part (b), $M \\oplus M^*$ contains at least $|M^*| - |M|$ vertex disjoint augmenting paths with respect to $M$. By part (c), each of these is also a an augmenting path for $M$. Since each has length  $\\sqrt{|V|}$, there can be at most $\\sqrt{|V|}$ such paths, so $|M^*| - |M| \\le \\sqrt{|V|}$. Thus, only $\\sqrt{|V|}$ additional iterations of the repeat loop can occur, so there are at most $2\\sqrt{|V|}$ iterations in total.\n\n\ng.\n For each unmatched vertex in $L$ we can perform a modified $\\text{BFS}$ to find the length of the shortest path to an unmatched vertex in $R$. Modify the $\\text{BFS}$ to ensure that we only traverse an edge if it causes the path to alternate between an edge in $M$ and an edge in $E - M$. The first time an unmatched vertex in $R$ is reached we know the length $k$ of a shortest augmenting path.\n\n\nWe can use this to stop our search early if at any point we have traversed more than that number of edges. To find disjoint paths, start at the vertices of $R$ which were found at distance $k$ in the $\\text{BFS}$. Run a $\\text{DFS}$ backwards from these, which maintains the property that the next vertex we pick has distance one fewer, and the edges alternate between being in $M$ and $E - M$. As we build up a path, mark the vertices as used so that we never traverse them again. This takes $O(E)$, so by part (f) the total runtime is $O(\\sqrt VE)$.",
            "title": "26-6 The Hopcroft-Karp bipartite matching algorithm"
        }
    ]
}