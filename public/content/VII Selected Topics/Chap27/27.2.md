---
title: "27.2 Multithreaded matrix multiplication"
---

## 27.2-1

> Draw the computation dag for computing $\text{P-SQUARE-MATRIX-MULTIPLY}$ on $2 \times 2$ matrices, labeling how the vertices in your diagram correspond to strands in the execution of the algorithm. Use the convention that spawn and call edges point downward, continuation edges point horizontally to the right, and return edges point upward. Assuming that each strand takes unit time, analyze the work, span, and parallelism of this computation.

## 27.2-2

> Repeat Exercise 27.2-1 for $\text{P-MATRIX-MULTIPLY-RECURSIVE}$.

## 27.2-3

> Give pseudocode for a multithreaded algorithm that multiplies two $n \times n$ matrices with work $\Theta(n^3)$ but span only $\Theta(\lg n)$. Analyze your algorithm.

We perform a modification of the $\text{P-SQUARE-MATRIX-MULTIPLY}$ algorithm. Basically, as hinted in the text, we will parallelize the innermost for loop in such a way that there aren't any data races formed. To do this, we will just define a parallelized dot product procedure. This means that lines 5-7 can be replaced by a single call to this procedure. $\text{P-DOT-PRODUCT}$ computes the dot dot product of the two lists between the two bounds on indices.

Using this, we can use this to modify $\text{P-SQUARE-MATRIX-MULTIPLY}$
Since the runtime of the inner loop is $O(\lg n)$, which is the depth of the recursion. Since the paralel for loops also take $O(\lg n)$ time. So, since the runtimes are additive here, the total span of this procedure is $\Theta(\lg n)$. The total work is still just $O(n^3)$ Since all the spawning and recursing call be re- placed with the normal serial version once there aren't enough free processors to handle all of the spawned calls to $\text{P-DOT-PRODUCT}$.

## 27.2-4

> Give pseudocode for an efficient multithreaded algorithm that multiplies a $p \times q$ matrix by a $q \times r$ matrix. Your algorithm should be highly parallel even if any of $p$, $q$, and $r$ are 1. Analyze your algorithm.

$T\_\infty = \Theta(\min \\{ \lg p, \lg q, \lg r \\})$

## 27.2-5

> Give pseudocode for an efficient multithreaded algorithm that transposes an $n \times n$ matrix in place by using divide-and-conquer to divide the matrix recursively into four $n / 2 \times n / 2$ submatrices. Analyze your algorithm.

```cpp
P-MATRIX-TRANSPOSE-RECURSIVE(A, r, c, s)
    // Transpose the s × s submatrix starting at a[r][c].
    if s == 1
        return
    else s' = floor(s / 2)
        spawn P-MATRIX-TRANSPOSE-RECURSIVE(A, r, c, s')
        spawn P-MATRIX-TRANSPOSE-RECURSIVE(A, r + s', c + s', s - s')
        P-MATRIX-TRANSPOSE-SWAP(A, r, c + s', r + s', c, s', s - s')
        sync
```

```cpp
P-MATRIX-TRANSPOSE-SWAP(A, r1, c1, r2, c2, s1, s2)
    // Transpose the s1 × s2 submatrix starting at a[r1][c1] with the s2 × s1 submatrix
    // starting at a[r2][c2].
    if s1 < s2
        P-MATRIX-TRANSPOSE-SWAP(A, r2, c2, r1, c1, s2, s1)
    else if s1 == 1     // since s1 ≥ s2, must have that s2 equals 1
        exchange a[r1][c1] with a[r2][c2]
    else s' = floor(s1 / 2)
        spawn P-MATRIX-TRANSPOSE-SWAP(A, r2, c2, r1, c1, s2, s')
        P-MATRIX-TRANSPOSE-SWAP(A, r2, c2 + s', r1 + s', c1, s2, s1 - s')
        sync
```

In order to transpose an $n \times n$ matrix $A$, we call $\text{P-MATRIX-TRANSPOSE-RECURSIVE}(A, 1, 1, n)$.

Let us first calculate the work and span of $\text{P-MATRIX-TRANSPOSE-SWAP}$ so that we can plug in these values into the work and span calculations of $\text{P-MATRIX-TRANSPOSE-RECURSIVE}$. The work $T\_1^\prime(N)$ of $\text{P-MATRIX-TRANSPOSE-SWAP}$ on an $N$-element matrix is the running time of its serialization. We have the recurrence

<div>
$$
\begin{aligned}
T_1'(N) & = 2T_1'(N / 2) + \Theta(1) \\
        & = \Theta(N).
\end{aligned}
$$
</div>

The span $T\_\infty^\prime(N)$ is similarly described by the recurrence

<div>
$$
\begin{aligned}
T_\infty'(N) & = T_\infty'(N / 2) + \Theta(1) \\
             & = \Theta(\lg N).
\end{aligned}
$$
</div>

In order to calculate the work of $\text{P-MATRIX-TRANSPOSE-RECURSIVE}$, we calculate the running time of its serialization. Let $T\_1(N)$ be the work of the algorithm on an $N$-element matrix, where $N = n^2$, and assume for simplicity that n is an exact power of $2$. Because the procedure makes two recursive calls with square submatrices of sizes $n / 2 \times n / 2 = N / 4$ and because it does $\Theta(n^2) = \Theta(N)$ work to swap all the elements of the other two submatrices of size $n / 2 \times n / 2$, its work is given by the recurrence

<div>
$$
\begin{aligned}
T_1(N) & = 2T_1(N / 4) + \Theta(N) \\
       & = \Theta(N).
\end{aligned}
$$
</div>

The two parallel recursive calls in $\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ execute on matrices of size $n / 2 \times n / 2$. The span of the procedure is given by maximum of the span of one of these two recursive calls and the $\Theta(\lg N)$ span of $\text{P-MATRIX-TRANSPOSE-SWAP}$, plus $\Theta(1)$. Since the recurrence

<div>
$$
T_\infty(N) = T_\infty(N / 4) + \Theta(1)
$$
</div>

has the solution $T\_\infty(N) = \Theta(\lg N)$ by case 2 of Theorem 4.1, the span of the recursive call is asymptotically the same as the span of $\text{P-MATRIX-TRANSPOSE-SWAP}$, and hence the span of $\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ is $\Theta(\lg N)$.

Thus, $\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ has parallelism $\Theta(N / \lg N) = \Theta(n^2 / \lg n)$.

## 27.2-6

> Give pseudocode for an efficient multithreaded implementation of the Floyd-Warshall algorithm (see Section 25.2), which computes shortest paths between all pairs of vertices in an edge-weighted graph. Analyze your algorithm.

```cpp
P-FLOYD-WARSHALL(W)
    n = W.rows
    parallel for i = 1 to n
        parallel for j = 1 to n
            d[i][j] = w[i][j]
    for k = 1 to n
        parallel for i = 1 to n
            parallel for j = 1 to n
                d[i][j] = min(d[i][j], d[i][k] + d[k][j])
    return D
```

By Exercise 25.2-4, we can compute all the $d\_{ij}$ values in parallel.

The work of $\text{P-FLOYD-WARSHALL}$ is the same as the running time of its serialization, which we computed as $\Theta(n^3)$ in Section 25.2. The span of the doubly nested **parallel for** loops, which do constant work inside, is $\Theta(\lg n)$. Note, however, that the second set of doubly nested **parallel for** loops is executed within each of the $n$ iterations of the outermost serial **for** loop. Therefore, $\text{P-FLOYD-WARSHALL}$ has span $\Theta(n\lg n)$ and $\Theta(n^2 / \lg n)$ parallelism.